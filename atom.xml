<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2021-04-20T01:34:12.000Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer Analysis</title>
    <link href="https://leijiezhang001.github.io/Transformer-Analysis/"/>
    <id>https://leijiezhang001.github.io/Transformer-Analysis/</id>
    <published>2021-04-13T01:45:40.000Z</published>
    <updated>2021-04-20T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于 Attention 的 Transformer 已经从 NLP 领域逐渐要统治图像，点云领域，本文详述了其原理机制，以及在图像，点云中的一些应用。</p><h2 id="attention-to-transformer1">1. Attention to Transformer<a href="#1" id="1ref"><sup>[1]</sup></a></h2><h3 id="attention">1.1. Attention</h3><p><img src="/Transformer-Analysis/attention.png" width="80%" height="80%" title="图 1. Attention"></p><p>　　Attention 的机制在于能自动关注到输入序列中的重要部分。如图 1. 左图，设输入 query \(Q\in\mathbb{R} ^ {N\times d _ k}\)，key-value \(K\in\mathbb{R} ^ {N _ k\times d _ k}, V\in\mathbb{R} ^ {N _ k\times d _ v}\)，那么 Attention 映射方程 \(\mathcal{A}(\cdot)\) 将其映射为输出 \(\mathbb{R} ^ {N\times d _ k}\)。具体的： <span class="math display">\[\begin{align}\mathcal{A}(Q,K,V) &amp;= \mathrm{score}(Q,K)V \\&amp;= \mathrm{softmax}\left(\frac{QK ^ T}{\sqrt{d _ k}}\right)V\end{align} \tag{1}\]</span> 当 \(d _ k\) 较大时，\(QK ^ T\) 的幅值会较大，导致 softmax 的反传梯度会趋于很小，为了降低这种影响，引入尺度变换 \(\frac{1}{\sqrt{d _ k}}\)。以上矩阵运算的维度变化为： <span class="math display">\[\begin{align}\mathrm{score}(\cdot):&amp;\;\; \mathbb{R} ^ {N\times d _ q}, \mathbb{R} ^ {N _ k\times d _ k} \rightarrow \mathbb{R} ^ {N\times N _ k}，其中\; d _ k=d _ q=d _ m\\\mathcal{A}(Q,K,V):&amp;\;\; \mathbb{R} ^ {N\times d _ k}, \mathbb{R} ^ {N _ k\times d _ k}, \mathbb{R} ^ {N _ k\times d _ v} \rightarrow \mathbb{R} ^ {N\times d _ k}\end{align} \tag{2}\]</span> 　　如图 1. 右图，实践中，采用 MultiHead Attention 形式，在不同的子空间不同的位置编码中学习特征，然后再作整合： <span class="math display">\[ \mathrm{MultiHead(Q,K,V)} =\mathrm{Concat(head _ 1,..., head _ h)}W ^ o \tag{3}\]</span> 其中 \(\mathrm{head _ i} = \mathcal{A}(QW _ i ^ Q,KW _ i ^ K,VW _ i ^ V)\)，参数 \(W _ i ^ Q\in\mathbb{R} ^ {d _ m\times d _ k},W _ i ^ K\in\mathbb{R} ^ {d _ m\times d _ k},W _ i ^ V\in\mathbb{R} ^ {d _ m\times d _ v}\)；\(W ^ o\in\mathbb{R} ^ {hd _ v\times d _ m}\)，为了与 Attention 计算复杂度类似，令 \(d _ k=d _ v=d _ m/h\)。</p><h3 id="transformer">1.2. Transformer</h3><p><img src="/Transformer-Analysis/transformer.png" width="45%" height="45%" title="图 2. Transformer"></p><p>　　如图 2. 所示，Transformer 结构有 Encoder，Decoder 构成，基本层由 Multi-head Attention 和 point-wise Fully Connected Layer 构成，称之为 Multi-head Attention： <span class="math display">\[\begin{align}\mathcal{A} ^ {MH}(X,Y)=\mathrm{LayerNorm}(S + \mathrm{rFF}(S)) \\S = \mathrm{LayerNorm}(X+\mathrm{Multihead(X,Y,Y)})\end{align} \tag{4}\]</span> 其中 \(\mathrm{rFF}\) 表示 row-wise fead-forward network。N 个该基础层叠加组成 Encoder 网络，其维度变换为： <span class="math display">\[\mathcal{A} ^ {MH}:\; \mathbb{R} ^ {N\times d _ m}, \mathbb{R} ^ {N _ k\times d _ m} \rightarrow \mathbb{R} ^ {N\times d _ m} \tag{5}\]</span></p><h3 id="positional-encoding">1.3. Positional Encoding</h3><p>　　由于 Transformer 对输入没有提取序列信息的能力，所以需要对输入作位置编码。位置编码可以通过学习得到，也可以用函数编码： <span class="math display">\[\begin{align}PE _ {pos,2i} &amp;= sin(pos/10000 ^ {2i/d _ {model}}) \\PE _ {pos,2i+1} &amp;= cos(pos/10000 ^ {2i/d _ {model}})\end{align} \tag{6}\]</span></p><p>其中 pos 表示位置，i 表示特征维度。</p><h3 id="why-self-attention">1.4. Why Self-Attention</h3><p><img src="/Transformer-Analysis/complex.png" width="80%" height="80%" title="图 3. Complexity"></p><p>　　如图 3. 所示，Attention 相比 Recurrent，Convolution，其在计算复杂度，并行化，输入序列间的最大距离上有较好的优势。</p><h2 id="vision-transformer2">2. Vision Transformer<a href="#2" id="2ref"><sup>[2]</sup></a></h2><p><img src="/Transformer-Analysis/vision_transformer.png" width="80%" height="80%" title="图 4. Vision Transformer"> 　　如图 4. 所示，首先将图像 \(\mathbf{x}\in\mathbb{R} ^ {H\times W\times C}\) 变换成 patch 序列 \(\mathbf{x} _ p\in\mathbb{R} ^ {N\times (P ^ 2\cdot C)}\)，其中 \((P,P)\) 是 patch 尺寸，\(N=HW/P ^ 2\)。然后用线性变换 \(\mathbf{E}\in\mathbb{R} ^ {(P ^ 2\cdot C)\times D}\) 将每个 patch 的特征维度映射到 D 维。最终的 Transformer 数学形式为： <span class="math display">\[\begin{align}\mathbf{z} _ 0 &amp;=[\mathbf{x} _ {class};\;\mathbf{x} _ p^1\mathbf{E};\;\mathbf{x} _ p^2\mathbf{E};\;\cdots;\;\mathbf{x} _ p^N\mathbf{E}]+\mathbf{E} _ {pos}, \;\;&amp; \mathbf{E}\in\mathbb{R} ^ {(P^2\cdot C)\times D},\;\mathbf{E} _ {pos}\in\mathbb{R} ^ {(N+1)\times D}\\\mathbf{z} _ l&#39; &amp;= \mathrm{MultiHeadAttention(LN(}\mathbf{z} _ {l-1}))+\mathbf{z} _ {l-1}, \;\;&amp; l=1\cdots L\\\mathbf{z} _ l &amp;= \mathrm{MLP(LN(}\mathbf{z})) _ l&#39;+\mathbf{z} _ l&#39;, \;\;&amp; l=1\cdots L\\\mathbf{y} &amp;= \mathrm{LN}(\mathbf{z}) _ L ^0\\\end{align} \tag{7}\]</span> 这里的关键是，在输入序列中串联了分类结果 \(\mathbf{z} _ 0 ^ 0= \mathbf{x} _ {class}\)，经过 \(L\) 次查询迭代后，获得最终的分类结果 \(\mathbf{z} _ L ^ 0\)。</p><h2 id="detr3">3. DETR<a href="#3" id="3ref"><sup>[3]</sup></a></h2><p><img src="/Transformer-Analysis/detr.png" width="80%" height="80%" title="图 5. DETR"></p><p><img src="/Transformer-Analysis/detr2.png" width="80%" height="80%" title="图 6. DETR"> 　　如图 5.6. 所示，DETR 首先用 CNN 网络作图像特征提取，然后用 Transformer Encoder 作特征序列化融合，接着用 Transformer Decoder 作目标框查询，得到检测结果。</p><p><img src="/Transformer-Analysis/detr_transformer.png" width="80%" height="80%" title="图 7. DETR Transformer"></p><h2 id="point-transformer4">4. Point Transformer<a href="#4" id="4ref"><sup>[4]</sup></a></h2><h2 id="group-free-3d-object-detection5">5. Group-Free 3D Object Detection<a href="#5" id="5ref"><sup>[5]</sup></a></h2><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于 Attention 的 Transformer 已经从 NLP 领域逐渐要统治图像，点云领域，本文详述了其原理机制，以及在图像，点云中的一些应用。&lt;/p&gt;
&lt;h2 id=&quot;attention-to-transformer1&quot;&gt;1. Attention to Tra
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Transformer" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Transformer/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Detection" scheme="https://leijiezhang001.github.io/tags/Detection/"/>
    
      <category term="Transformer" scheme="https://leijiezhang001.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Offboard 3D Object Detection from Point Cloud Sequences</title>
    <link href="https://leijiezhang001.github.io/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/"/>
    <id>https://leijiezhang001.github.io/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/</id>
    <published>2021-03-12T01:29:53.000Z</published>
    <updated>2021-03-18T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　目前点云检测等任务基本集中在在线实时情况下的研究，然而在离线场景下，自动化/半自动化标注/高精地图语义信息提取/数据闭环中的教师模型等，这些任务也相当重要。相比在线模式，离线模式对点云的实时处理要求较低，而且能更容易提取时序信息。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种基于时序的离线点云检测方法，能极大提高目标真值标注的自动化程度。其性能几乎达到了人工标注的水平。</p><h2 id="problem-statement-framework">1. Problem Statement &amp; Framework</h2><p>　　对于一个 \(N\) 帧序列点云 \(\{\mathcal{P} _ i\in\mathbf{R} ^ {n _ i\times C}\},i=1,2,...,N\)，已知每帧点云传感器在世界坐标系下的位姿 \(\{\mathcal{M} _ i=[R _ i|t _ i]\in\mathbf{R} ^ {3\times 4}\},i=1,2,...,N\)，那么我们要得到每帧点云中的目标 3D 属性(包括中心点，尺寸，朝向)，类别，ID。<br><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　方法框架如图 1. 所示，输入序列点云，首先用目标检测器检测每一帧中的 3D 目标，然后用跟踪器将目标进行数据关联作 ID 标记。最后提取每个 ID 目标跟踪轨迹上的所有点云及目标框信息，作进一步的 3D 目标框精修预测。</p><h2 id="d-auto-labeling-pipeline">2. 3D Auto Labeling Pipeline</h2><h3 id="multi-frame-3d-object-detection">2.1. Multi-frame 3D Object Detection</h3><p>　　采用 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds">End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds</a> 中的 MVF 检测器，输入用多帧经过运动补偿的点云代替，每个点增加相对时间的偏移量。在测试阶段，采用 Test-time Augmentation，将点云绕 Z 轴进行不同角度的增广，最终的检测框进行权重融合。这在离线计算中可用不同计算单元并行化实现。 <img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/mvf.png" width="90%" height="90%" title="图 2. MVF++"> 　　MVF 基本思想可见 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds">End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds</a>。这里对 MVF++ 作更详细的描述。结构如图 2. 所示，网络输入的点云尺寸为 \(N\times C\)，经过 MLP 网络映射到高维特征，然后在 Birds Eye View 和 Perspective View 下体素化并作卷积提取特征的操作，最终融合得到点级别的三种特征类型。Loss 构成为： <span class="math display">\[L = L _ {cls} + w _ 1L _ {centerness} + w _ 2L _ {reg} + w _ 3L _ {seg}\tag{1}\]</span> 其中 \(L _ {seg}\) 是区分前景，背景的辅助分支。</p><h3 id="multi-object-tracking">2.2. Multi-object Tracking</h3><p>　　多目标跟踪方法采用<a href="#2" id="1ref">[2]</a>，在三维空间下作前后帧率检测跟踪的的数据关联，然后用卡尔曼作状态估计。</p><h3 id="object-track-data-extraction">2.3. Object Track Data Extraction</h3><p>　　经过多目标跟踪模块后，每个目标实例在时空内都作了 ID 标记。在世界坐标系下，可提取目标的 4D 时空信息，包括点云及 3D 属性框：第 \(j\) 个目标在其出现的帧 \(S _ j\) 下的点云 \(\{\mathcal{P} _ {j,k}\},k\in S _ j\)，以及对应的 3D 框 \(\{\mathcal{B} _ {j,k}\},k\in S _ j\)。</p><h3 id="object-centric-auto-labeling">2.4. Object-centric Auto Labeling</h3><p>　　有了目标检测，跟踪算法流程后。接下来将目标自动标注分为目标动静状态分析，静态目标自动标注以及动态目标自动标注。<br>　　目标动静状态分析模块将每个目标实例提取 4D 特征，然后加入线性分类器来预测。特征包括目标框中心点的时空方差，以及目标从始至终的中心点偏移距离。真值标记时，将静态目标从始至终的距离阈值设定为 1.0m，最大速度不超过 1m/s。这种方式动静预测准确率高达 99%。此外，将行人都归为动态目标。</p><h4 id="static-object">2.4.1. Static Object</h4><p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/static.png" width="70%" height="70%" title="图 3. Static Object"> 　　如图 3. 所示，首先挑选 score 最高的目标框作为 initial box，将点云从世界坐标系转到该目标框坐标系。类似 Cascade-RCNN，作连续的前景分割-目标框回归的网络预测。Loss 项由中心点回归，朝向分类回归，尺寸分类回归三部分构成： <span class="math display">\[\begin{align}L &amp;= L _ {seg} + w\sum _ i ^ 2 L _ {box _ i} \\&amp;= L _ {seg} +  w\sum _ i ^ 2\left(L _ {center-reg _ i} + w _ 1L _ {size-cls _ i} + w _ 2L _ {size-reg _ i} + w _ 3L _ {heading-cls _ i} + w _ 4L _ {heading-reg _ i}\right)\end{align}\tag{2}\]</span></p><h4 id="dynamic-object">2.4.2. Dynamic Object</h4><p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/dynamic.png" width="70%" height="70%" title="图 4. Dynamic Object"> 　　如图 4. 所示，对于动态目标，也可以将目标点云累积到某一时刻的目标中心坐标系中，但是累积很难对齐(目测可以用迭代的思想，定位-累积来精修)。所以本文采用从轨迹中直接提取特征，从而预测目标尺寸的方法。对于 \(T\) 时刻前后的目标点云 \(\{\mathcal{P} _ {j,k}\} _ {k=T-r} ^ {T+r}\) 以及目标框 \(\{\mathcal{B} _ {j,k}\} _ {k=T-s} ^ {T+s}\)，特征由两部分构成：</p><ul><li>Point 分支。将点云增加时间信息后，转换到当前目标框 \(\mathcal{B} _ {j,T}\) 的中心位置坐标系下，用 PointNet 网络作前后景的分割，得到前景目标的全局特征量。</li><li>Box 分支。同样转换到当前目标框中心坐标系下。需要注意的是，取的 Box 帧数大多比 Point 长(Point 只取 5 帧)。目标框特征维度为 8(x,y,z,l,h,w,ry,time)，同样通过 PointNet 网络提取全局特征。</li></ul><p>将这两个特征作融合，然后预测当前 \(T\) 时刻的目标尺寸及朝向。<br>　　为了增强两个分支各自对预测目标的学习，分别对两个分支用真值目标作监督学习。最终的 Loss 为： <span class="math display">\[L = L _ {seg}+v _ 1L _ {box-traj} + v _ 2L _ {box-obj-pc}+v _ 3L _ {box-joint} \tag{3}\]</span></p><h2 id="experiments">3. Experiments</h2><h3 id="comparing-with-sota-detectors">3.1. Comparing with SOTA Detectors</h3><p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/sota.png" width="90%" height="90%" title="图 5. sota"> 　　如图 5. 所示，毫无疑问，效果拔群。</p><h3 id="comparing-with-human-labels">3.2. Comparing with Human Labels</h3><p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/human.png" width="65%" height="65%" title="图 6. Human labels"> 　　如图 6. 所示，挑选 3 个经验丰富的标注员对数据进行标注，以此与本文方法进行比较，可见本方法只在高度估计上无法与人类标注员媲美，定位及其它尺寸上，几乎能达到类似水平。 <img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/human2.png" width="65%" height="65%" title="图 7. Consistency between human lablers"> 　　进一步思考，人类不同的标注员，他们的标注结果一致性如何？如图 7. 所示，GT 由多个标注员交叉验证得到，以此为基准，发现人类标注员也很难达到与真值相同水平的程度，标注质量几乎与本方法差不多，当距离较远时，点数会较少，人类标注员反而标注质量会下降。</p><h3 id="applications-to-semi-supervised-learning">3.3. Applications to Semi-supervised Learning</h3><p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/semi-supervised.png" width="65%" height="65%" title="图 8. Semi-supervised"> 　　用本文的方法去标注未标注的数据，作为实时目标检测模型的训练数据，能极大提升其性能。也从另一方面论证了本文方法能比拟人工标注水平。</p><h3 id="analysis-of-the-multi-frame-detector">3.4. Analysis of the Multi-frame Detector</h3><p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/mvf-exp.png" width="65%" height="65%" title="图 9. MVF Experiments"> 　　如图 9. 所示，MVF 检测器性能实验，当输入帧数大于 5 帧时，帧数增多已经无法提升检测性能。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Qi, Charles R., et al. "Offboard 3D Object Detection from Point Cloud Sequences." arXiv preprint arXiv:2103.05073 (2021).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　目前点云检测等任务基本集中在在线实时情况下的研究，然而在离线场景下，自动化/半自动化标注/高精地图语义信息提取/数据闭环中的教师模型等，这些任务也相当重要。相比在线模式，离线模式对点云的实时处理要求较低，而且能更容易提取时序信息。本文&lt;a href=&quot;#1&quot; id=&quot;1
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
      <category term="Offline" scheme="https://leijiezhang001.github.io/categories/3D-Detection/Offline/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>A Comprehensive Survey on Point Cloud Registration</title>
    <link href="https://leijiezhang001.github.io/A-Comprehensive-Survey-on-Point-Cloud-Registration/"/>
    <id>https://leijiezhang001.github.io/A-Comprehensive-Survey-on-Point-Cloud-Registration/</id>
    <published>2021-03-10T01:24:45.000Z</published>
    <updated>2021-03-25T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　点云注册是是点云数据处理中非常重要的一个方向。<a href="/Object-Registration-with-Point-Cloud/" title="Object Registration with Point Cloud">Object Registration with Point Cloud</a> 中描述了基于点云的目标注册方法，主要阐述了传统 ICP 原理以及基于深度学习进行目标注册(相对位姿估计)的方法。本文<a href="#1" id="1ref"><sup>[1]</sup></a>则详细介绍整个点云注册方法的类别与细节。</p><h2 id="problem-definition">1. Problem Definition</h2><p>　　假设两个点云集 \(X\in\mathbb{R} ^ {M\times 3}, Y\in\mathbb{R} ^ {N\times 3}\)，其中每个点表示为 \(\mathbf{x} _ i(i\in [1,M])\)，\(\mathbf{y} _ i(y\in [1,N])\)。两个点云集合中有 \(K\) 个匹配点对，点云注册问题就是找到参数 \(g\)，即旋转矩阵 \(R\in\mathcal{SO}(3)\) 和位移矩阵 \(t\in\mathbb{R} ^ 3\)，使得： <span class="math display">\[\mathop{\arg\min}\limits _ {R\in\mathcal{SO}(3),t\in\mathbb{R} ^ 3}\Vert d(X,g(Y))\Vert ^ 2 _ 2 \tag{1}\]</span> 其中 \(d(X,g(Y))=d(X,RY+t)=\sum _ {k=1} ^ K \Vert \mathbf{x} _ k-(R\mathbf{y} _ k+t)\Vert _ 2\)。这个问题是典型的鸡生蛋蛋生鸡问题，如果匹配点对已知，那么变换矩阵可以求解；如果变换矩阵已知，那么就能得到匹配点对。</p><h2 id="challenges">2. Challenges</h2><p>　　根据数据源类型，点云注册可分为 same-source 以及 cross-source 两类。其挑战分别有：</p><ul><li>Same-source<ol type="1"><li>Noise and Outliers<br></li><li>Partial overlap<br></li></ol></li><li>Cross-source<ol type="1"><li>Noise and Outliers<br></li><li>Partial overlap<br></li><li>Density difference<br>不同传感器获得的数据源，点云密度可能不一样。</li><li>Scale variation<br>不同传感器获得的数据源，点云的空间尺度可能不一样。</li></ol></li></ul><h2 id="categories">3. Categories</h2><p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/taxonomy.png" width="90%" height="90%" title="图 1. Taxonomy"></p><ul><li>Optimisation-based<br></li><li>Feature learning<br></li><li>End-to-end learning-based</li><li>Cross-source registration</li></ul><h2 id="optimisation-based">4. Optimisation-based</h2><p>　　大多数优化方法都包含两个步骤：匹配点对搜索，以及转换矩阵估计。匹配点对可通过计算 point-point 距离或特征相似度得到。这种方法的好处是有严谨的数学解，能保证收敛，不需要训练数据；缺点是需要复杂的策略来解决噪音，离群点，遮挡等问题。<br><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/optimization-based.png" width="90%" height="90%" title="图 2. Optimization-based"> 　　对于已搜索到匹配点对后，可用非线性问题求解方法来优化计算转换矩阵。根据优化策略不同，可分为如下几种方法。</p><h3 id="icp-based">4.1. ICP-based</h3><p>　　首先匹配点中距离度量方式分为三种：</p><ul><li><p>Point-Point<br>就是式 (1) 下方的传统方式，计算两个点的欧式距离。</p></li><li><p>Point-Plane<br>表示点与对应平面之间的距离： <span class="math display">\[\mathop{\arg\min}\limits _ {R\in\mathcal{SO}(3),t\in\mathbb{R} ^ 3}\left\{\sum _ {k=1} ^ K w _ k\left\Vert \mathrm{n} _ k * (\mathrm{x} _ k-(R\mathrm{y} _ k+t))\right\Vert ^ 2\right\} \tag{2}\]</span> 其中 \(w _ k\) 是匹配对权重，\(\mathrm{n _ k}\) 是面的法向量。</p></li><li><p>Plane-Plane<br>表示面与对应平面之间的距离： <span class="math display">\[\mathop{\arg\min}\limits _ {R\in\mathcal{SO}(3),t\in\mathbb{R} ^ 3}\left\{\sum _ {k=1} ^ K \left\Vert \mathrm{nx} _ k-(R\mathrm{ny} _ k+t)\right\Vert ^ 2\right\} \tag{3}\]</span> 其中 \(\mathbf{nx,ny}\) 是对应的法向量。</p></li><li><p>Generalized ICP<br><span class="math display">\[\mathop{\arg\min}\limits _ {T}\left\{\sum _ {k=1} ^ K \left\Vert d ^T(C _ k ^ Y+\mathbf{T} C _ k ^ X\mathbf{T} ^ T) ^ {-1}\right\Vert ^ 2\right\} \tag{4}\]</span> 其中 \(\{C _ k ^ X\}\)，\(\{C _ k ^ Y\}\) 为点云 \(X,Y\) 之间的协方差矩阵。当 \(\{C _ k ^ X=0\}\)，\(\{C _ k ^ Y=I\}\) 时，就是标准的 point-point ICP；\(\{C _ k ^ X = 0\}\)，\(\{C _ k ^ Y = P _ k ^ {-1}\}\) 时就是 pont-plane ICP，其中 \(P _ k ^ {-1}\) 为法向量。<br>　　根据匹配点度量方式获得匹配点后，即可优化求解位姿矩阵，有三种方法：</p></li><li><p>SVD-based<br>用奇异值分解的方式求解。</p></li><li><p>Lucas-Kanade<br>包括 Levenberg-Marquardt 方法，用雅克比矩阵及近似高斯牛顿法优化求解。</p></li><li><p>Procrustes analysis<br>将位姿估计转换为线性最小二乘问题。位姿闭式解为 \(P=(X _ 2 ^ HX _ 1)^ { -1 }X _ 2^Hx _ 1\)。</p></li></ul><h3 id="graph-based">4.2. Graph-based</h3><p>　　将点云建模为非参图模型，包括边与顶点。GM 方法目的就是通过边与顶点去寻找两个图中的匹配点，GM 可分为 second-order 与 high-order 方法，前者只考虑边与边，顶点与顶点的相似性，后者则会考虑多于两个点的相似性，比如三角对相似性。</p><h3 id="gmm-based">4.3. GMM-based</h3><p>　　高斯混合模型法核心是将点云注册问题建模为最大化似然的过程。求解后，可得到位姿和混合高斯参数。</p><h3 id="semi-definite-registration">4.4. Semi-definite Registration</h3><p>　　将问题近似为其它问题求解。</p><h2 id="feature-learning">5. Feature-learning</h2><p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/feature-learning.png" width="90%" height="90%" title="图 3. Feature-learning"> 　　基于特征学习的方法，是提取点云的点级别特征，然后作一次性精准匹配，最后直接用 SVD 等后端优化方法得到，无需进行多次迭代。<a href="/Object-Registration-with-Point-Cloud/" title="Object Registration with Point Cloud">Object Registration with Point Cloud</a> 中介绍的也属于这种方法。<br>　　对于点云特征提取的方法，Learning on volumetric data 以及 Learning on point cloud 都介绍的已经非常多了，这里不作展开。</p><h2 id="end-to-end-learning-based">6. End-to-end Learning-based</h2><p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/end-end.png" width="90%" height="90%" title="图 4. End-to-end"> 　　如图 4. 所示，端到端的方法主要分为 Registration by regression 和 Registration by optimization and neural network 方法。</p><h2 id="cross-source">7. Cross-source</h2><p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/cross-source.png" width="90%" height="90%" title="图 5. cross-source"> 　　如图 5. 所示，多源点云数据的注册，方法也分为 Optimization-based 和 Learning-based，思路也差不多，这里不作展开。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Huang, Xiaoshui, et al. "A comprehensive survey on point cloud registration." arXiv preprint arXiv:2103.02690 (2021).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　点云注册是是点云数据处理中非常重要的一个方向。&lt;a href=&quot;/Object-Registration-with-Point-Cloud/&quot; title=&quot;Object Registration with Point Cloud&quot;&gt;Object Registratio
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Review" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Review/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Point Cloud Registration" scheme="https://leijiezhang001.github.io/tags/Point-Cloud-Registration/"/>
    
  </entry>
  
  <entry>
    <title>4D Panoptic LiDAR Segmentation</title>
    <link href="https://leijiezhang001.github.io/4D-Panoptic-LiDAR-Segmentation/"/>
    <id>https://leijiezhang001.github.io/4D-Panoptic-LiDAR-Segmentation/</id>
    <published>2021-03-02T01:51:00.000Z</published>
    <updated>2021-03-05T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　4D 激光点云的全景分割任务是在 3D 基础上，引入时间维度，同时在时间和空间下作点云的语义分割以及实例分割，输出的是每个点的语义类别信息，以及时序实例 ID。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种简单 4D 全景分割框架。</p><h2 id="framework">1. Framework</h2><p><img src="/4D-Panoptic-LiDAR-Segmentation/framework.png" width="70%" height="70%" title="图 1. Framework"> 　　如图 1. 所示，将时序点云作累积，然后用 Encoder-Decoder 网络，输出量有：</p><ul><li>Semantic Map，每个点的类别；</li><li>Objectness Map，目标中心点，或者靠近中心的某点；</li><li>Point Embeddings，每个点的特征；</li><li>Point Variance Map，每个点的位置方差；</li></ul><p>有了这些信息后，就可以聚类出目标实例，方法类似 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>，将 variance map 拓展到 Embedding 任意空间，应该是目前比较先进的策略了。<br>　　假设目标实例的点云符合高斯分布，那么在已知中心点，或者属于该实例的某点的情况下(不用必须是中心点，这样能处理遮挡等情况)，就可以判断其它点属于该实例的概率。具体的，已知实例中心点 \(\mathbf{p} _ i\)，以及对应的 Embedding 特征 \(e _ i\)，可以计算其它点 \(p _ j\) 是否属于该实例的概率： <span class="math display">\[\hat{p} _ {ij}=\frac{1}{(2\pi) ^ {D/2}|\Sigma _ i| ^ {\frac{1}{2}}}\mathrm{exp}\left(-\frac{1}{2}(e _ i-e _ j)^T\Sigma _ i^ {-1}(e _ i-e _ j)\right) \tag{1}\]</span> 其中 \(\Sigma _ i\) 是通过点 \(p _ i\) 预测的 \(\sigma _ i\) 所构建的对角矩阵。值得注意的是，Embedding 特征串联了空间和时序量，这样更有利于聚类，同时预测对应的 variance map。<br>　　ID 则是通过时序下作实例的数据关联得到的。</p><h2 id="loss">2. Loss</h2><p>　　网络输出都是点级别的。总的 Loss 为： <span class="math display">\[L=L _ {class}+L _ {obj}+L _ {ins}+L _ {var} \tag{2}\]</span> 具体的：</p><ul><li>Semantic Segmentation<br>用 Cross-entropy 分类损失函数，并且通过采样的方法来解决类别不平衡问题。</li><li>Point Centerness<br>计算每个点与其实例中心点，或者实例点云重心点的距离，归一化后作为中心点得分损失函数： <span class="math display">\[L _ {obj} = \sum _ {i=1} ^ N(\hat{o} _ i-o _ i) ^ 2,\;\;\;\hat{o} _ i,o _ i\in[0,1]\tag{3}\]</span></li><li>Instance Probability<br>计算每个实例中每个点属于该实例的概率，回归其值到 1: <span class="math display">\[L _ {ins}=\sum _ {j=1} ^ K\sum _ {i=1} ^ N(\hat{p} _ {ij}-p _ {ij}) ^ 2,\;\;\;p _ {ij} = 1,\;\mathrm{if}\; p _ i\in I _ j,\;\mathrm{else}\; 0\tag{4}\]</span></li><li>Variance Smooth<br>类似 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>，作实例内每个点 Variance 的一致性约束： <span class="math display">\[L _ {var} = \frac{1}{|I _ j|}\sum _ {i\in I _ j}\Vert \sigma _ i-\sigma _ j\Vert ^ 2\tag{5}\]</span></li></ul><h2 id="measuring-performance">3. Measuring Performance</h2><p>To be continued...</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Aygün, Mehmet, et al. "4D Panoptic LiDAR Segmentation." arXiv preprint arXiv:2102.12472 (2021).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　4D 激光点云的全景分割任务是在 3D 基础上，引入时间维度，同时在时间和空间下作点云的语义分割以及实例分割，输出的是每个点的语义类别信息，以及时序实例 ID。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; 提出了一种简单 4D 
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/Instance-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/tags/Instance-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>SPVConv &amp; 3D-NAS</title>
    <link href="https://leijiezhang001.github.io/SPVConv-3D-NAS/"/>
    <id>https://leijiezhang001.github.io/SPVConv-3D-NAS/</id>
    <published>2021-02-19T01:19:47.000Z</published>
    <updated>2021-02-28T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于点云的神经网络学习方法在 <a href="/Deep-Learning-for-3D-Point-Clouds/" title="Deep Learning for 3D Point Clouds">Deep Learning for 3D Point Clouds</a> 中已经有较为详细的描述，从网络结构上看，可分为 Multi-view-based(Projection-based)，Volumetric-based，Point-based 等三种方法。其中 Point-based 方法对点云的信息提取分辨率最高，但是前两种将点云空间离散化的方法更容易学到特征信息。所以如何结合这两种网络结构来更有效得学习点云特征就显得非常重要。<br>　　此外，<a href="/Rethinking-of-Sparse-3D-Convolution/" title="Rethinking of Sparse 3D Convolution">Rethinking of Sparse 3D Convolution</a> 中阐述了 Sparse Convolution 相比传统卷积在点云特征学习中的优势，其能更有效学习点云特征信息。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种 Sparse Point-Voxel 的基本网络结构，并采用 NAS 网络搜索方法来搜索最优的卷积通道数量及深度。</p><h2 id="sparse-point-voxel-convolutionspvconv">1. Sparse Point-Voxel Convolution(SPVConv)</h2><p>　　Voxel-based 特征提取会损失信息源，但是提取效率较高；Point-based 能保留信息分辨率，但是提取效率较低。本文设计的 SPVConv 则融合了二者的优势。 <img src="/SPVConv-3D-NAS/spvconv.png" width="90%" height="90%" title="图 1. SPVconv"> 　　如图 1. 所示，网络模块由两个分支组成，Point-Based 分支对点云进行点级别的 MLP 运算；Sparse Voxel-Based 分支将点云进行体素化，然后用稀疏卷积进行特征提取，然后反体素化得到点级别的特征。最后将二者提取的特征融合到一起。<br>　　数学描述上，首先是数据表示：</p><ul><li>稀疏体素化张量(Sparse Voxelized Tensor)表示为 \(\mathbf{S}=\{(\mathbf{p} _ m ^ s,\mathbf{f} _ m ^ s),v\}\)，其中 \(\mathbf{p} _ m ^ s=(\mathbf{x} _ m ^ s,\mathbf{y} _ m ^ s, \mathbf{z} _ m ^ s)\) 表示第 \(m\) 个体素的 3D 坐标，\(\mathbf{f} _ m ^ s\) 表示其特征向量；\(v\) 是该层级下的体素尺寸；</li><li>点云张量(Point Cloud Tensor)表示为 \(\mathbf{T} = \{(\mathbf{p} _ k ^ t,\mathbf{f} _ k ^ t)\}\)，其中 \(\mathbf{p} _ k=(\mathbf{x} _ k, \mathbf{y} _ k,\mathbf{z} _ k)\) 表示第 \(k\) 个点的 3D 坐标，\(\mathbf{f} _ k\) 为其特征向量。</li></ul><p>然后将点云进行稀疏体素化映射，计算每个体素的特征向量：</p><p><span class="math display">\[\hat{\mathbf{p}} _ k ^ t=(\hat{\mathbf{x}} _ k ^ t,\hat{\mathbf{y}} _ k ^ t,\hat{\mathbf{z}} _ k ^ t) = \left(\mathrm{floor}(\mathbf{x} _ k ^ t/v),\mathrm{floor}(\mathbf{y} _ k ^ t/v),\mathrm{floor}(\mathbf{z} _ k ^ t/v)\right) \tag{1}\]</span> <span class="math display">\[\mathbf{f} _ m ^ s=\frac{1}{N _ m}\sum _ {k=1} ^ n\mathbb{1}[\hat{\mathbf{x}} _ k ^ t=\mathbf{x} _ m ^ s,\hat{\mathbf{y}} _ k ^ t=\mathbf{y} _ m ^ s,\hat{\mathbf{z}} _ k ^ t=\mathbf{z} _ m ^ s]\cdot\mathbf{f} _ k ^ t\tag{2}\]</span> 直接计算的复杂度为 \(\mathcal{O}(mn)\)，为了实时计算，需要对体素化和反体素化进行哈希索引。哈希表的 key 为 3D 坐标，value 为稀疏张量的 index，所以最终的建立哈希表和查询复杂度为 \(\mathcal{O}(m+n)\)。<br>　　对于反体素化，采用 trilinear interpolation，在稀疏体素中采样得到点级别的特征。该特征与点级别 MLP 得到的特征作特征维度的串联操作，即得到提取的点特征向量。</p><h2 id="d-nas">2. 3D-NAS</h2><p>　　以 SPVConv 模块为基础结构，搜索最优的网络 channel 数，以及网络深度。卷积核固定为 3x3。<br>　　搜索策略为：首先训练一个最大的网络，那么搜索的子网络权重可以从大网络中继承，具体搜索策略可参考代码。</p><p><img src="/SPVConv-3D-NAS/3dnas.png" width="90%" height="90%" title="图 2. 3D-NAS"></p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Tang, Haotian, et al. "Searching efficient 3d architectures with sparse point-voxel convolution." European Conference on Computer Vision. Springer, Cham, 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于点云的神经网络学习方法在 &lt;a href=&quot;/Deep-Learning-for-3D-Point-Clouds/&quot; title=&quot;Deep Learning for 3D Point Clouds&quot;&gt;Deep Learning for 3D Point Cloud
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Depth Prediction/Completion</title>
    <link href="https://leijiezhang001.github.io/Depth-Prediction-Completion/"/>
    <id>https://leijiezhang001.github.io/Depth-Prediction-Completion/</id>
    <published>2021-02-03T01:28:15.000Z</published>
    <updated>2021-02-09T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　准确的深度信息获取在自动驾驶中非常重要，所以激光点云以其稳定精确的深度测量优势在自动驾驶传感器配置中不可或缺。但是激光雷达昂贵而且获取的点云较为稀疏，由此可考虑两种替代方案：基于单目、双目的深度预测；以及基于单目，激光雷达的深度补全。<br>　　深度预测与深度补全都是为了获得稠密的图像像素级别深度信息，所以在自监督的损失函数上，非常相似。本文就提取归纳一些共同的优化点，并对各自的优化方向作阐述。</p><h2 id="self-supervised-depth-estimation">1. Self-supervised Depth Estimation</h2><p>　　室外场景下，图像像素级别的深度信息真值很难获得，在位姿准确的情况下可累积激光点云的测量信息来填充大部分像素区域，以此作为真值；也可以借助 SLAM 建图定位技术，重建场景稠密的深度信息，从而使像素均能索引到真实的深度。但是以上两种方法都只能获得静态场景的深度信息。所以采用自监督的深度估计技术，就显得尤为重要。<br>　　本文介绍两篇文章：<a href="#1" id="1ref">[1]</a> 是基于图像的深度估计，<a href="#2" id="2ref">[2]</a> 是基于图像和激光雷达的深度补全。二者的网络结构都比较简单，主要是无监督/半监督的 Loss 设计。</p><h3 id="digging-into-self-supervised-monocular-depth-estimation">1.1. Digging Into Self-Supervised Monocular Depth Estimation</h3><p><img src="/Depth-Prediction-Completion/digging.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，深度估计的网络输入为单帧，经过 Encoder-Decoder 输出像素级别的深度。Pose 估计的网络输入前后帧，输出前后帧的本体位姿变换。无监督 Loss 通过当前帧与前后帧的像素匹配计算，在遮挡或是非连续的情况下，当前帧的某些像素区域在前后帧中是不可见的(如图 2. 所示)，所以会产生较大的 Loss，本文提出了取最小化 Loss 的方法，有效解决该问题。此外，计算不同尺度下的 Loss。 <img src="/Depth-Prediction-Completion/min-reproj.png" width="60%" height="60%" title="图 2. Per-pixel Minimum Reprojection"></p><h3 id="self-supervised-sparse-to-dense-self-supervised-depth-completion-from-lidar-and-monocular-camera">1.2. Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera</h3><p><img src="/Depth-Prediction-Completion/complete.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 3. 所示，类似的，该方法输入为 RGB+Sparse LiDAR 数据，通过 Depth Loss 半监督，通过 Photometric Loss 无监督学习深度信息。其中 Pose 通过提取前后帧角点，匹配，然后求解 PnP 的方式计算得到。</p><h2 id="loss">2. Loss</h2><p>　　无监督/半监督学习深度信息，Loss 的设计就很关键，以下列举几种常用的 Loss 设计方法。</p><h3 id="sparse-depth-loss">2.1. Sparse Depth Loss</h3><p>　　对于有对应的 LiDAR 点云的情况，可对点云打到的图像像素区域作有监督学习： <span class="math display">\[\mathcal{L} _ {depth} = \Vert \mathbb{1} _ {\{d &gt; 0\}}\cdot (\mathbf{pred-d})\Vert _ 2 ^ 2\tag{1}\]</span></p><h3 id="smoothness-loss">2.2. Smoothness Loss</h3><p>　　为了使得估计的深度信息在空间上较为平滑，<a href="#2" id="2ref">[2]</a> 采用简单的像素空间二次导数最小化的方法： <span class="math display">\[\mathcal{L} _ {smooth} = \Vert\nabla ^ 2\mathbf{pred}\Vert _ 1\tag{2}\]</span> <a href="#1" id="1ref">[1]</a> 则采用像素空间 Edge-aware 的平滑 Loss，这样在物理空间上深度信息没有拖影的现象： <span class="math display">\[\mathcal{L} _ {smooth} = \vert\partial _ xd _ t ^ * \vert e ^ {-\vert \partial _ xI _ t\vert}+\vert\partial _ yd _ t ^ * \vert e ^ {-\vert \partial _ yI _ t\vert}\tag{3}\]</span></p><h3 id="photometric-reprojection-loss">2.3. Photometric Reprojection Loss</h3><p>　　设当前图像帧预测的深度信息为 \(\mathbf{pred} _ 0\)，相机内参矩阵为 \(\mathcal{K}\)，那么对于相机相对位姿为 \(T _ {0\rightarrow 1}\) 的图像 \(\mathbf{RGB} _ 1\)，其像素坐标系关系为：\(p _ 1=\mathcal{K}T _ {0\rightarrow 1}\mathbf{pred} _ 0(p _ 0)\mathcal{K} ^ {-1}p _ 0\)，由此可得到图像从 \(0\rightarrow 1\) 的变换： <span class="math display">\[\mathbf{warped _ 1(p _ 0)}=\mathrm{bilinear}(\mathbf{RGB} _ 1(\mathcal{K}T _ {0\rightarrow 1}\mathbf{pred} _ 0(p _ 0)\mathcal{K} ^ {-1}p _ 0)) \tag{4}\]</span> <a href="#2" id="2ref">[2]</a> 采用多尺度的 L1 Loss，并只在无激光点云的像素区域作无监督学习： <span class="math display">\[\mathcal{L} _ {photometric}(\mathbf{warped _ 1,RGB _ 1})=\sum _ {s\in S} \frac{1}{s}\left\Vert\mathbb{1} ^ {(s)} _ {d==0}\cdot(\mathbf{warped _ 1} ^ {(s)}-\mathbf{RGB _ 1} ^ {(s)})\right\Vert _ 1 \tag{5}\]</span> 　　<a href="#1" id="1ref">[1]</a> 采用 L1 与 SSIM<a href="#3" id="3ref"><sup>[3]</sup></a> 的方式来构造 Photometric 损失函数： <span class="math display">\[\mathcal{L} _ {photometric}(\mathbf{warped _ 1,RGB _ 1}) = \frac{\alpha}{2}(1-\mathrm{SSIM}(\mathbf{warped _ 1,RGB _ 1}))+(1-\alpha)\Vert\mathbf{warped _ 1-RGB _ 1}\Vert _ 1\tag{6}\]</span> 其中 SSIM 是描述图像结构信息相似度的函数。此外 <a href="#1" id="1ref">[1]</a> 除了多尺度训练外，还作了两点改进：</p><ol type="1"><li><p>Per-Pixel Minimum Reprojection<br>如图 2. 所示，在遮挡以及图像边缘情况，\(\mathbf{warped} _ 1\) 可能在 \(\mathbf{RGB} _ 1\) 上找不到对应的像素点，导致损失函数失真变大，所以引入 \(\mathbf{RGB} _ {-1}\)，同时考虑 \(\mathbf{warped} _ {-1}\) 与其匹配: <span class="math display">\[\mathbf{warped _ {-1}(p _ 0)}=\mathrm{bilinear}(\mathbf{RGB} _ {-1}(\mathcal{K}T _ {0\rightarrow -1}\mathbf{pred} _ 0(p _ 0)\mathcal{K} ^ {-1}p _ 0)) \tag{7}\]</span> 取二者最小的误差作为投影误差： <span class="math display">\[\mathcal{L} _ {photometric} = \sum _ p \mathop{\min}\limits \{pe _ {-1}, pe _ {1}\} \tag{8}\]</span> 其中 \(pe\) 是像素级别的 SSIM 与 L1 误差。</p></li><li><p>Auto-Masking Stationary Pixels<br><img src="/Depth-Prediction-Completion/mask.png" width="70%" height="70%" title="图 4. Auto-Masking"> 对于静态场景以及运动物体，投影误差来描述深度估计都是不准确的，所以用一个像素级别的 mask 来计算最终的损失函数，判断一个像素是否计入损失函数的条件为 \(\mathbf{warped}\) 像素值与目标像素值的误差是否小于原始像素值与目标像素值的误差： <span class="math display">\[\mu = \left[ \mathop{\min}\limits \{pe _ {-1} ^ {warped}, pe _ {1} ^ {warped}\} &lt;\mathop{\min}\limits \{pe _ {-1} ^ {unwarped}, pe _ {1} ^ {unwarped}\} \right] \tag{9}\]</span> 其中 \([\cdot]\) 使得 \(\mu\in \{0,1\}\)。这样就能在静态场景以及运动物体与本车速度相似的情况下，不计入该区域的投影损失误差值。</p></li></ol><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Godard, Clément, et al. "Digging into self-supervised monocular depth estimation." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.<br><a id="2" href="#2ref">[2]</a> Ma, Fangchang, Guilherme Venturelli Cavalheiro, and Sertac Karaman. "Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera." 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.<br><a id="3" href="#3ref">[3]</a> Wang, Zhou, et al. "Image quality assessment: from error visibility to structural similarity." IEEE transactions on image processing 13.4 (2004): 600-612.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　准确的深度信息获取在自动驾驶中非常重要，所以激光点云以其稳定精确的深度测量优势在自动驾驶传感器配置中不可或缺。但是激光雷达昂贵而且获取的点云较为稀疏，由此可考虑两种替代方案：基于单目、双目的深度预测；以及基于单目，激光雷达的深度补全。&lt;br&gt;
　　深度预测与深度补全都是
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Depth Completion" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Depth-Completion/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Depth Prediction" scheme="https://leijiezhang001.github.io/tags/Depth-Prediction/"/>
    
      <category term="Depth Completion" scheme="https://leijiezhang001.github.io/tags/Depth-Completion/"/>
    
  </entry>
  
  <entry>
    <title>A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving</title>
    <link href="https://leijiezhang001.github.io/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/</id>
    <published>2020-12-24T01:40:41.000Z</published>
    <updated>2021-01-21T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　概率目标检测是将不确定估计应用于目标检测任务中，不确定性估计之前已经描述很多了，包括 Epistemic Uncertainty，Aleatoric Uncertainty，以及 Uncertainty Calibration 相关技术。本文<a href="#1" id="1ref"><sup>[1]</sup></a>则详细阐述概率目标检测的进展。<br>　　在自动驾驶领域，不管是什么传感器，在极端天气环境或不熟悉的场景下，以及远距离或高遮挡情况下，基于深度学习的目标检测模型失效概率会比较大，或者说预测的不确定性会比较大。人类驾驶员在这方面比较擅长，比如在雨夜看不太清的场景，会先降低速度，增加观察时间，以获得观测的高确定性。所以对网络而言，不确定的估计才能为后续增加观测的确定性作准备。</p><h2 id="uncertainty-estimation-in-deep-learning">1. Uncertainty Estimation in Deep Learning</h2><p>　　<a href="/Perception-Uncertainty-in-Deep-Learning/" title="Perception Uncertainty in Deep Learning">Perception Uncertainty in Deep Learning</a> 中已经较为详细得阐述了 Uncertainty 的来龙去脉。贝叶斯神经网络框架下，不确定性可分解为认知不确定性(Epistemic Uncertainty)以及偶然不确定性(Aleatoric Uncertainty)。 <img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/uncert-cate.png" width="60%" height="60%" title="图 1. Uncertainty Categorization"> 　　从感知数据流角度，如图 1. 所示，会引入很多不确定性，从最原始的传感器不确定性，到标注不确定性，再到模型训练测试的不确定性，所有这些不确定性构成了最终模型输出结果的不确定性。<br>　　数学上，对于训练数据集 \(\mathscr{D}\) 贝叶斯神经网络的输出分布表示为: <span class="math display">\[p(\mathbf{y|x}, \mathscr{D}) = \int p(\mathbf{y|x}, \mathbf{W})p(\mathbf{W}| \mathscr{D}) \mathrm{d}\mathbf{W}\tag{1}\]</span> 其中 \(p(\mathbf{y|x,W})\) 表示观测似然，包含了偶然不确定性；\(p(\mathbf{W}|\mathscr{D})\) 表示模型后验分布，包含认知不确定性。</p><h3 id="practical-methods-for-uncertainty-estimation">1.1. Practical Methods for Uncertainty Estimation</h3><p>　　实际不确定性的估计需要在效率上考虑到其可行性。可分为用于估计模型认知不确定性的 MC-Dropout，Deep Ensembles 方法；用于估计偶然不确定性的 Direct Modeling 方法；以及用于估计模型认知及偶然不确定性的 Error Propagation 方法，以下作详细描述：</p><ul><li><p>Monte-Carlo Dropout<br>对网络进行 \(T\) 次前向 Inference，\(\mathbf{W} _ t\) 为网络经过 dropout 后的权重，那么网络预测的概率分布为： <span class="math display">\[p(\mathbf{y|x},\mathscr{D})\approx \frac{1}{T}\sum _ {t=1} ^ T p(\mathbf{y|x,W _ t}) \tag{2}\]</span> 对于回归问题，由此可计算其回归量的均值和方差。</p></li><li><p>Deep Ensembles<br>用网络模型集合来估计输出的概率分布，本质上 Monte-Carlo Dropout 方法也是一种网络模型集合的方法。设 \(M\) 个网络权重为 \(\{\mathbf{W} _ m\} _ {m=1} ^ M\)，那么输出概率分布为： <span class="math display">\[p(\mathbf{y|x},\mathscr{D})\approx \frac{1}{M}\sum _ {m=1} ^ M p(\mathbf{y|x,W _ m}) \tag{3}\]</span></p></li><li><p>Direct Modeling<br>该方法假设模型的回归输出符合多模态混合高斯分布，即 \(p(y|\mathbf{x,W}) = \mathcal{N}(y|\hat{\mu}(\mathbf{x,W}),\hat{\sigma} ^ 2(\mathbf{x,W}))\)，其中 \(\hat{\mu}(\mathbf{x,W})\) 为网络输出，\(\hat{\sigma} ^ 2(\mathbf{x,W})\) 为输出值的方差。最小化负对数似然函数，即可预测输出量的均值和方差： <span class="math display">\[L(\mathbf{x,W})=-\mathrm{log}(p(\mathbf{y|x,W})) \approx \frac{(y-\hat{\mu}(\mathbf{x,W}))^2}{2\hat{\sigma} ^ 2(\mathbf{x,W})}+\frac{\mathrm{log}\hat{\sigma} ^ 2(\mathbf{x,W})}{2}\tag{4}\]</span> 对于分类问题，假设 softmax 预测中的每个 logits 元素符合独立高斯分布，类似回归问题，网络同时预测均值 logits 以及方差 logits。训练的时候，用重采样的方法在每个高斯分布中采样出每个 logits，最后再用标准的分类误差函数计算其损失函数。<br>直接模型求解只需要增加额外的模型分支即可，引入的计算量并不大，但是会产生预测的方差不准的问题，需要进一步标定。</p></li><li><p>Error Propagation<br>误差传递方法计算效率最高，直接将数据源的误差通过每个操作层进行传递，比如 <a href="/A-General-Framework-for-Uncertainty-Estimation-in-deep-learning/" title="A General Framework for Uncertainty Estimation in Deep Learning">A General Framework for Uncertainty Estimation in Deep Learning</a>。</p></li></ul><h3 id="evaluation-and-benchmarking">1.2. Evaluation and Benchmarking</h3><p>　　评估不确定性估计的指标有：</p><ul><li><p>Shannon Entropy<br>用来描述分类任务的不确定性： <span class="math display">\[\mathcal{H}(y|\mathbf{x},\mathscr{D}) = -\sum _ {c=1} ^ C p(y=c|\mathbf{x},\mathscr{D})\mathrm{log}\left(p(y=c|\mathbf{x},\mathscr{D})\right) \tag{5}\]</span> 当 \(p(y=c|\mathbf{x},\mathscr{D}) = 0 \mathrm{or} 1\) 时，不确定性最小。个人认为，该指标只能描述不确定性的大小，无法描述不确定估计的准确性。</p></li><li><p>Mutual Information<br>与 SE 类似，也是描述分类任务的不确定性，但是引入了模型不确定性： <span class="math display">\[\mathcal{I}(y,\mathbf{W|x},\mathscr{D})=\mathcal{H}(y|\mathbf{x},\mathscr{D})-\mathbb{E} _ {p(\mathbf{W}|\mathscr{D})}\left[\mathcal{H}(y|\mathbf{x,W})\right] \tag{6}\]</span> 其中 conditional Shannon Entropy 通过采样模型权重(MC-Dropout)计算得到： <span class="math display">\[\mathcal{H}(y|\mathbf{x,W}) = -\sum _ {c=1} ^ C p(y=c|\mathbf{x,W})\mathrm{log}\left(p(y=c|\mathbf{x,W})\right) \tag{7}\]</span></p></li><li><p>Calibration Plot<br><a href="/Uncertainty-Calibration/" title="Uncertainty Calibration">Uncertainty Calibration</a> 中已经较为详细的阐述了 Calibration Plot 的作用，这里不作展开。由此可用 ECE Score 来评估不确定的准确度： <span class="math display">\[\mathrm{ECE}=\sum _ {t=1} ^ T\frac{N _ t}{N _ {eval}}|p _ t-\hat{p} _ t| \tag{8}\]</span></p></li><li><p>Negative Log Likelihood(NLL)<br>NLL 也是 Direct Modeling 方法中不确定性的预测的 Loss，其描述了预测的概率分布与真值分布的相似性，所以能评估不确定性预测的准确度： <span class="math display">\[\mathrm{NLL}=-\sum _ {n=1} ^ {N _ {test}}\mathrm{log}\left(p(\mathbf{y _ n|x _ n}, \mathscr{D})\right) \tag{9}\]</span></p></li><li><p>Brier Score<br>用于评估分类概率的准确性: <span class="math display">\[\mathrm{BS} = \frac{1}{N _ {test}}\sum _ {n=1} ^ {N _ {test}}\sum _ {c=1} ^ C (\hat{s} _ {n,c} - y _ {n,c}) ^ 2 \tag{10}\]</span> 其中 \(\hat{s} _ {n,c}\) 表示 softmax score, \(y _ {n,c}\in\{0, 1\}\) 表示真值。越小表示估计的不确定性越好。</p></li><li><p>Error Curve<br>不确定性越大，表示与真值的误差越大。所以逐步去掉不确定性较大的数据，剩下数据的误差会逐渐减少。</p></li><li><p>Total Variance(TV)<br>在回归任务中，计算 Covariance matrix 的 trace，作为回归任务的总的方差。</p></li></ul><h2 id="probabilistic-object-detection">2. Probabilistic Object Detection</h2><p>　　传统的目标检测方法都是确定性的，而 POD 目的是在目标检测的基础上，在分类及回归任务中，进一步估计可靠的不确定性。<br><img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/det.png" width="90%" height="90%" title="图 2. POD Pipeline"> 　　我们要估计的不确定性包括 Epistemic Uncertainty 以及 Aleatoric Uncertainty。认知不确定性用 MC-Dropout 来估计。偶然不确定性中的回归问题在 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中已经较为详细的描述，基本就是用 NLL LOSS 来优化；对于分类问题，本文采用重采样高斯分布的 softmax logits 的方法，具体的，预测输出与分类 softmax logits 长度一样的 logits variance，然后对每个高斯分布的 logit 作重采样，作为最终的输出以作 Loss 优化。<br>　　综上，POD 估计不确定性可归纳为： <span class="math display">\[\left\{\begin{array}{l}\hat{\mu}(\mathbf{x}) = \frac{1}{T}\sum _ {t=1} ^ T(\mathbf{x,W} _ t) \\\hat{\sigma} ^ 2(\mathbf{x}) = \hat{\sigma} _ e ^ 2(\mathbf{x}) + \hat{\sigma} _ a ^ 2(\mathbf{x}) \\\hat{\sigma} _ e ^ 2(\mathbf{x}) = \frac{1}{T}\sum _ {t=1} ^ T\left(\hat{\mu}(\mathbf{x,W} _ t)\right) ^ 2 - \left(\hat{\mu}(\mathbf{x})\right) ^ 2\\\hat{\sigma} _ a ^ 2(\mathbf{x}) = \frac{1}{T} \sum _ {t=1} ^ T \hat{\sigma} ^ 2(\mathbf{x,W} _ t)\end{array}\tag{11}\right.\]</span> 通过 T 次蒙特卡洛采样，得到最终总的不确定性。</p><h2 id="comparative-study">3. Comparative Study</h2><p><img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/pod.png" width="90%" height="90%" title="图 3. Methods"> 　　用 Droupout 估计 Epistemic Uncertainty，对于 Aleatoric Uncertainty 的估计，实现了三种方法：</p><ul><li><p>Loss Attenuation<br>网络输出分类 logits 以及回归量的均值和方差，用 Direct Modeling 中的权重 Loss 来优化；</p></li><li><p>BayesOD<br>与 Loss Attenuation 不同的是，后处理用 Data Association + Bayesian Fusion 来代替标准的 NMS 算法；</p></li><li><p>Output Redundancy<br>网络输出不作改变，后处理采用 Data Association + Sample Statistics 来估计不确定性；</p></li></ul><p><img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/res.png" width="90%" height="90%" title="图 4. Evaluation Result"> 　　这几种方法对比如图 4. 所示，不确定性估计对目标检测性能有所提升，并且能提供预测的不确定性。具体方法的实现细节可见<a href="#2" id="2ref">[2]</a>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving." arXiv preprint arXiv:2011.10671 (2020).<br><a id="2" href="#2ref">[2]</a> https://github.com/asharakeh/pod_compare</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　概率目标检测是将不确定估计应用于目标检测任务中，不确定性估计之前已经描述很多了，包括 Epistemic Uncertainty，Aleatoric Uncertainty，以及 Uncertainty Calibration 相关技术。本文&lt;a href=&quot;#1&quot; i
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Review" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Review/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Multi-modal Fusion" scheme="https://leijiezhang001.github.io/tags/Multi-modal-Fusion/"/>
    
  </entry>
  
  <entry>
    <title>Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving - Datasets, Methods, and Challenges</title>
    <link href="https://leijiezhang001.github.io/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/"/>
    <id>https://leijiezhang001.github.io/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/</id>
    <published>2020-12-17T01:28:40.000Z</published>
    <updated>2020-11-23T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　由于相机，激光雷达，毫米波雷达等传感器各有优劣，所以深度多模态数据融合在自动驾驶感知中非常重要。本文<a href="#1" id="1ref"><sup>[1]</sup></a>以目标检测及语义分割为例，详细阐述了深度多模态数据融合的发展及挑战。<br>　　多模传感器融合的目标检测及语义分割任务，可分解为三大问题：What to Fuse，When to Fuse，How to Fuse。以下就从这三个方面进行分析归纳。</p><h2 id="what-to-fuse">1. What to Fuse</h2><p>　　自动驾驶中用于全范围感知的有激光雷达，毫米波雷达，相机。<a href="/paper-reading-RadarNet/" title="RadarNet">RadarNet</a> 中比较详细得介绍了激光雷达与毫米波雷达的优劣，并融合二者作目标检测跟踪；<a href="/paper-reading-CenterFusion/" title="CenterFusion">CenterFusion</a> 则融合毫米波雷达与相机二者的优势，作目标检测与速度测量；激光雷达与相机的融合，研究已经较多，这里不作举例。同时融合三个传感器的算法暂时没看到。<br>　　激光点云的处理方法主要有三种: 1. 将点云物理空间 3D Voxel 化处理；2. 直接在点云连续空间内进行点级别的学习；3. 将点云投影到 2D 空间，如 Bird-View，Apherical-View，Cylinder-View 等，然后作 2D 卷积处理。<br>　　毫米波雷达数据 \(x,y,v\) 可表示为 2D 特征图，然后用 2D 卷积来处理；也可表示为点云的形式，然后用点云的操作来处理。</p><h2 id="how-to-fuse">2. How to Fuse</h2><p>　　考虑两个不同的传感器数据源 \(M _ i, M _ j\)，对应的第 \(l\) 层网络特征 \(f _ l ^ {M _ i}, f _ l ^ {M _ j}\)，以及操作 \( G _ l(\cdot)\)。融合方式有以下几种：</p><ul><li>Addition or Average Mean:<br>将两个特征图相加或者取平均，\(f _ l=G _ {l-1}\left(f _ {l-1} ^ {M _ i}+f _ {l-1} ^ {M _ j}\right)\)。</li><li>Concatenation:<br>将两个特征图在深度维度进行串联，\(f _ l=G _ {l-1}\left(f _ {l-1} ^ {M _ i}\frown f _ {l-1} ^ {M _ j}\right)\)。</li><li>Ensemble:<br>在目标检测任务中，对 ROI 内的特征进行整合，\(f _ l=G _ {l-1}\left(f _ {l-1} ^ {M _ i}\right)\cup G _ {l-1}\left( f _ {l-1} ^ {M _ j}\right)\)。</li><li>Mixture of Experts:<br>用 experts 网络预测带融合特征的权重，然后作权重融合，\(f _ l=G _ {l}\left(w ^ {M _ i}\cdot f _ {l-1} ^ {M _ i}+w ^ {M _ j}\cdot f _ {l-1} ^ {M _ j}\right)\)，其中 \(w ^ {M _ i}+ w ^ {M _ j} = 1\)。</li></ul><h2 id="when-to-fuse">3. When to Fuse</h2><p><img src="/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/fusion-methods.png" width="90%" height="90%" title="图 1. Fusion Methods"> 　　如图 1 所示，融合的时间点可分为 early，middle，late 三种，<strong>本文归纳发现并没有哪一种融合是最优的，这与传感器类型，数据，网络结构等相关</strong>。设融合操作为 \(f _ l = f _ {l-1} ^ {M _ i}\oplus f _ {l-1} ^ {M _ j}\)，那么各融合方式可归纳为：</p><ul><li><p>Early Fusion<br>在传感器原始数据阶段进行数据融合: <span class="math display">\[f _ L = G _ L\left(G _ {L-1}\left(\dots G _ l\left(\dots G _ 2\left(G _ 1\left(f _ 0 ^ {M _ i}\oplus f _ 0 ^ {M _ j}\right)\right)\right)\right)\right)\tag{1}\]</span> 前融合的优势是深度整合传感器数据信息，理论上能挖掘最全的特征信息，以及计算量较小；劣势是模型灵活性较差，以及对多模态数据的空间对齐准确度非常敏感，其空间对齐的精度受传感器之间参数标定，采样频率，传感器缺陷等因素影响。</p></li><li><p>Late Fusion<br>在网络输出后进行融合： <span class="math display">\[f _ L=G _ L ^ {M _ i}\left(G _ {L-1} ^ {M _ i}\left(\dots G _ 1 ^ {M _ i}(f _ 0 ^ {M _ i})\right)\right) \oplus G _ L ^ {M _ j}\left(G _ {L-1} ^ {M _ j}\left(\dots G _ 1 ^ {M _ j}(f _ 0 ^ {M _ j})\right)\right)\tag{2}\]</span> 后融合是模块化的，所以有很强的灵活性；但是需要较多的计算资源，以及没有在特征层面对数据进行融合，可能丧失一定的信息量。</p></li><li><p>Middle Fusion<br>中融合变种非常多，如图 1. 所示，可以是 deep fusion 模式，也可以是 short-cut fusion 模式。网络结构上，还是比较难断定哪种结构是最优的。</p></li></ul><p><img src="/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/fusion-arch.png" width="100%" height="100%" title="图 2. Fusion Archtectures"> 　　对于目标检测任务来说，two-stage 方法基本都是在 ROI 内作特征融合，经典的方法如图 2. 所示，这里不做展开。</p><h2 id="datasets-methodology">4. Datasets &amp; Methodology</h2><p><img src="/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/challenges.png" width="100%" height="100%" title="图 3. Challenges and Open Questions"> 　　如图 3. 所示，目前基于多传感器融合的感知主要挑战有：</p><ul><li><p>Multi-modal data preparation<br>公开数据量及数据的多样性还较少，数据中多传感器的标定，标注准确性存疑。</p></li><li><p>Fusion Methodology<br>"What to fuse" 中融合的传感器数据还较少，还可以融合超声波雷达，V2X 信息，物理模型，先验模型等；"How to fuse" 中目前都是简单的融合，或者说整合，缺少对信息源不确定性的估计(Uncertainty)，可以采用 BNN 对不确定性进行估计；"When to fuse" 中目前基本凭经验去寻找最优的网络融合结构，缺少理论指导。</p></li><li><p>Others<br>评估指标上，还需进一步体现模型的鲁棒性；网络结构上，目前缺少时序融合。</p></li></ul><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges." IEEE Transactions on Intelligent Transportation Systems (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　由于相机，激光雷达，毫米波雷达等传感器各有优劣，所以深度多模态数据融合在自动驾驶感知中非常重要。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;以目标检测及语义分割为例，详细阐述了深度多模态数据融合的发展及挑战。&lt;br&gt;
　　多模传
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Review" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Review/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Multi-modal Fusion" scheme="https://leijiezhang001.github.io/tags/Multi-modal-Fusion/"/>
    
  </entry>
  
  <entry>
    <title>Uncertainty Calibration</title>
    <link href="https://leijiezhang001.github.io/Uncertainty-Calibration/"/>
    <id>https://leijiezhang001.github.io/Uncertainty-Calibration/</id>
    <published>2020-12-09T01:43:03.000Z</published>
    <updated>2020-12-16T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　If you don’t know the measurement uncertainty, don’t make the measurement at all!<a href="#1" id="1ref"><sup>[1]</sup></a><br>　　Uncertainty 在自动驾驶测量中的重要性在之前的文章，如 <a href="/Perception-Uncertainty-in-Deep-Learning/" title="Perception Uncertainty in Deep Learning">Perception Uncertainty in Deep Learning</a> 中已经有较详细的阐述，这里不做赘述。但更重要的是，如何确保 Uncertainty 估计的准确性。如图 1. 所示，本文讨论如何评估 Uncertainty 估计的准确性，以及通过 Uncertainty Calibration 来修正其估计误差。<br><img src="/Uncertainty-Calibration/framework.png" width="100%" height="100%" title="图 1. Framework"></p><h2 id="uncertainty-estimation-for-object-detection">1. Uncertainty Estimation for Object Detection</h2><p>　　以目标检测任务为例，深度学习中的 Uncertainty 估计可分为两大类方法：Ensemble Approach，以及 Direct-modeling Approach。本文以 Direct-modeling 方法为例，假设网络输出符合多多变量高斯分布。对于 Anchor-Free 的 3D 目标检测，分类的预测量为目标类别分数 \(p(y _ c=1|) = s _ {\mathbf{x}}\)，回归预测量为： <span class="math display">\[\mathbf{u _ x} = [\mathrm{cos}(\theta), \mathrm{sin}(\theta), dx,dy,\mathrm{log}(l),\mathrm{log}(w)] \tag{1}\]</span> 假设回归输出量符合多变量独立高斯分布 \(p(\mathbf{y _ r}| \mathbf{x})=(\mathbf{u _ x,\Sigma _ x})\)，那么其协方差矩阵为对角矩阵： <span class="math display">\[\mathbf{\sigma _ x} ^ 2=[\sigma ^2 _ {\mathrm{cos}(\theta)}, \sigma ^2 _ {\mathrm{sin}(\theta)}, \sigma ^ 2 _ {dx}, \sigma ^ 2 _ {dy}, \sigma ^ 2 _ {\mathrm{log}(l)}, \sigma ^ 2 _ {\mathrm{log}(w)}] _ {\mathbf{x}} \tag{2}\]</span> 由此加入预测的 Uncertainty 分支，Loss 项为： <span class="math display">\[L _ {reg}=\frac{1}{2}(\mathbf{y _ r-u _ x})\mathrm{diag}(\frac{1}{\mathbf{\sigma ^ 2 _ x}})(\mathbf{y _ r-u _ x}) ^ T+\frac{1}{2}\mathrm{log}(\mathbf{\sigma ^ 2 _ x})\mathbf{1} ^ T \tag{3}\]</span></p><h2 id="uncertainty-evaluation">2. Uncertainty Evaluation</h2><p>　　对于数据集 \(\{(\mathbf{x} ^ n,y _ c ^ n,\mathbf{y} _ r ^ n)\} _ {n=1} ^ N\)，\(\mathbf{X}\) 表示输入数据，\(\mathbf{Y} _ c\) 表示分类标签，\(\mathbf{Y _ r}\) 表示回归标签。概率描述为：\(\mathbf{X, Y} _ c \sim \mathbb{P} _ c\)，以及 \(\mathbf{X,Y _ r}\sim\mathbb{P} _ r\)。对于分类问题，，softmax score 预测了目标分类的概率分布，即 \(\mathbf{F} _ c ^ n(y _ c=1)=p(y _ c=1|\mathbf{x} ^ n)=s _ {\mathbf{x} ^ n}\)。对于回归问题，网络预测了概率密度函数 PDF：\(p(\mathbf{y _ r} ^ n | \mathbf{x} ^ n) = \mathcal{N}(\mathbf{u _ {x ^ n},\Sigma _ {x ^ n}})\)，其累积概率分布函数 CDF 定义为 \(\mathbf{F} _ r ^ n(\mathbf{y} _ r)\)，反函数为 \(\mathbf{F} _ r ^ {n ^ {-1}}(p)\)。<br>　　准确的不确定性预测意味着，预测的概率近似等于统计的频率。具体的：</p><ul><li><p>分类问题<br>0.9 的分数意味着 90% 的物体是被分类准确的。对于 \(\forall p\in[0,1]\)，数学形式为： <span class="math display">\[\mathbb{P} _ c(\mathbf{Y} _ c=1|\mathbf{F} _ c(\mathbf{Y} _ c=1)=p)\approx \frac{\sum _ {n=1} ^ N\mathbb{1}(y _ c ^ n=1,F _ c ^ n(y _ c=1)=p)}{\sum _ {n=1} ^ N\mathbb{1}(F _ c ^ n(y _ c=1)=p)} \tag{4}\]</span></p></li><li><p>回归问题<br>对于预测物体，其 90% 置信空间内，90% 的真值物体应该在置信空间内。对于 \(\forall p\in[0,1]\)，数学形式为： <span class="math display">\[\mathbb{P} _ r(\mathbf{Y} _ r\leq\mathbf{F} _ r ^ {-1}(p))\approx\frac{\sum _ {n=1} ^ N\mathbb{1}(y _ r ^ n\leq F _ r ^{n ^ {-1}}(p))}{N} \tag{5}\]</span></p></li></ul><p><img src="/Uncertainty-Calibration/cali-plot.png" width="60%" height="60%" title="图 2. Calibration Plot"> 　　由此可用 calibration plot 来刻画不确定性估计的准确性。如图 2. 所示，横坐标表示预测的概率，纵坐标表示统计的概率，将概率值划分为 \(0 &lt; p _ c ^ 1 &lt; ... &lt; p _ c ^ m &lt; ... &lt; 1\) 个置信区域，理想的 Calibration Plot 是对角线。计算该对角线与实际曲线的 Expected Calibration Error(ECE) 即可作为评估 Uncertainty 估计的准确性： <span class="math display">\[\mathrm{ECE} =\sum _ {m=1} ^ M\frac{N _ m}{N}\vert p ^ m-\hat{p} ^ m\vert\tag{6}\]</span></p><h2 id="uncertainty-recalibration">3. Uncertainty Recalibration</h2><p>　　为了使得 Calibration Plot 能完美贴合对角线，需要对 Uncertainty 进行标定。</p><h3 id="isotonic-regression">3.1. Isotonic Regression</h3><p>　　对于预测的累积概率分布函数 \(p=\mathbf{F} _ r(y _ r)\)，预测一个额外模型 \(g(p)\) 使得满足式 (5) 条件，该映射模型函数是非参单调递增的。额外模型通过 validation 数据集训练得到。</p><h3 id="temperature-scaling">3.2. Temperature Scaling</h3><p>　　对式 (2) 中的各方差作 \(T &gt; 0\) 的尺度变换：\(\hat{\sigma}\leftarrow\sigma ^ 2/T \)。最优的 \(T\) 通过最大化 Negative Log Likelihood(NLL) 实现，等价于最小化式 (3) 的 Loss 项。</p><h3 id="calibration-loss">3.3. Calibration Loss</h3><p>　　由式 (3) 可知，Variance 的预测是通过无监督的形式隐式来预测的，所以本质上就无法保证 Variance 的绝对正确性，所以可加入监督项来保证其正确性。因为一个准确的 Uncertainty 意味着预测的 Variance 与预测量和真值量的 Variance 是一致的，所以设计 calibration loss： <span class="math display">\[ L _ {calib} =\Vert \mathbf{\sigma _ x} ^ 2-(\mathbf{y _ r-u _ x})\odot (\mathbf{y _ r-u _ x})\Vert\tag{7}\]</span> 最终的 Loss 为： <span class="math display">\[L _ {total} = L _ {reg}+\lambda L _ {calib}\tag{8}\]</span></p><h2 id="recalibration-results">4. Recalibration Results</h2><p><img src="/Uncertainty-Calibration/calib.png" width="60%" height="60%" title="图 3. Calibration Plot After Recalibration"> 　　如图 3. 所示，经过标定后，预测的概率分布接近于实际统计分布。</p><p><img src="/Uncertainty-Calibration/pred.png" width="100%" height="100%" title="图 4. Predictions with Recalibration Uncertainties"> 　　图 4. 可视化了标定前后 Uncertainty 的准确程度，可见标定后，越是遮挡的目标，Uncertainty 越大，符合预期。此外，标定后目标检测的精度也有较大的提升。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "Can we trust you? on calibration of a probabilistic object detector for autonomous driving." arXiv preprint arXiv:1909.12358 (2019).<br><a id="2" href="#2ref">[2]</a> Kuleshov, Volodymyr, Nathan Fenner, and Stefano Ermon. "Accurate uncertainties for deep learning using calibrated regression." arXiv preprint arXiv:1807.00263 (2018).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　If you don’t know the measurement uncertainty, don’t make the measurement at all!&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;&lt;br&gt;
　　Uncert
      
    
    </summary>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/categories/Uncertainty/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Panoptic Segmentation via Dynamic Shifting Network&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/</id>
    <published>2020-12-03T02:00:27.000Z</published>
    <updated>2020-12-09T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　实例分割一般与语义分割同时进行，其难点是后处理如何准确的聚类出实例目标。目标聚类可归纳出的策略或方法有：</p><ul><li><strong>提高语义分割的准确率</strong>，在聚类的时候加入语义约束，能改善不同类别的欠分割，以及同类别的过分割；</li><li><strong>Spatial Offset &amp; Embedding Offset</strong>，提升聚类空间下的目标的聚集性，如 <a href="/paper-reading-OccuSeg/" title="OccuSeg">OccuSeg</a>，<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a>，<a href="/paper-reading-JSNet-JSIS3D/" title="JSNet, JSIS3D">JSNet, JSIS3D</a>；</li><li><strong>将聚类问题转换为每个点属于对应实例的概率问题</strong>，网络下 End-to-End 来优化聚类效果，如 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>；</li><li><strong>引入聚类时的 Bandwidth</strong>，不同尺寸的实例容忍不同程度的 Offset，如 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>；</li></ul><p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>属于引入聚类时的 Bandwidth 策略，对于大目标，聚类时自适应选择更大的 offset 容忍度，同时可调节点经过 offset 迭代至中心点的次数来提高聚类准确率。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，DS-Net 由 Cylinder-Convolution Backbone，Semantic Branch，Instance Branch 构成。<a href="/paper-reading-Cylinder3D/" title="Cylinder3D">Cylinder3D</a> 以及 <a href="/paper-reading-Pillar-based-Object-Detection/" title="[paper_reading]-" pillar-based object detection"">[paper_reading]-"Pillar-based Object Detection"</a> 已经较为详细得介绍了在 Cylinder 视野下的卷积过程，其相比 Spherical 和 Bird-eye 视野有一定的优势。Instance Branch 则由 Dynamic Shifting 以及 Consensus-driven Fusion 构成，以聚类实例目标。</p><h2 id="instance-branch">2. Instance Branch</h2><p>　　Instance Branch 预测每个实例目标的点 \(P\in\mathbb{R} ^ {M\times 3}\) 到实例中心 \(C _ {gt}\in\mathbb{R} ^ {M\times 3}\) 的 offset \(O\in\mathbb{R} ^ {M\times 3}\)。其 Loss 的基本形式为： <span class="math display">\[ L _ {ins} = \frac{1}{M}\sum _ {i=0} ^ M\Vert O[i]-(C _ {gt}[i]-P[i])\Vert \tag{1}\]</span> 其中 \(M\) 是实例目标的点个数，预测的回归中心点 \(O+P\) 可用于实例目标的聚类，一般通过 Heuristic Clustering 方法，本文则提出 Dynamic Shifting 方法。<br>　　自底向上的 Heuristic Clustering 方法有 Breadth First Search(BFS)，DBSCAN，HDBSCAN，Mean Shift 等。<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a> 采用了 BFS 方法，对于点云这种密度不一样的数据形式，固定的搜索半径是不太合理的，小的搜索半径容易过分割，大的搜索半径则容易欠分割。DBSCAN/HDBSCAN 是 density-based 方法，所以与 BFS 一样，对密度不一致的点云聚类效果不好。Mean Shift 对密度不一致的点云聚类更加友好，但是固定的 bandwidth 也不是一个好的选择。</p><h3 id="dynamic-shifting">2.1. Dynamic Shifting</h3><p>　　对于待聚类的点云集 \(X\in\mathbb{R} ^ {M\times 3}\)，预测的实例中心由回归的 offset \(S\in\mathbf{R} ^ {M\times 3}\) 与点云坐标计算得到： <span class="math display">\[X\leftarrow X + \eta S\tag{2}\]</span> offset \(S\) 的计算可定义为 \(S=f(X)-X\)，其中 \(f(\cdot)\) 为核函数。一种简单的平面核函数为： <span class="math display">\[f(X)=D ^ {-1} KX \tag{3}\]</span> 其中 \(K=(XX ^ T\leq \delta)\) 表示点周围 bandwidth \(\delta\) 区域的点集；\(D=diag(K\mathbf{1})\) 表示 \(\delta\) 内点集个数。<br>　　为了自适应不同的 bandwidth，设计 \(l\) 个候选 bandwidth \(L=\{\delta _ 1,\delta _ 2,...,\delta _ l\}\)。对候选 bandwidth 作权重化处理，权重通过 MLP，Softmax 得到 \(\sum _ {j=1} ^ l W[:,j] = \mathbf{1}\)。最终的核函数为： <span class="math display">\[\hat{f}(X) = \sum _ {j=1} ^ l W[:,j]\odot(D _ j ^ {-1}K _ j X) \tag{4}\]</span> 其中 \(K _ j=(XX ^ T\leq\delta _ j)\), \(D _ j=diag(K _ j\mathbf{1})\)。 <img src="/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/DSM.png" width="60%" height="60%" title="图 2. Dynamic Shifting Module"> 　　DSM 算法流程如图 2. 所示，种子点通过 offset 预测实例中心点迭代 \(I\) 次，第 \(i\) 次回归的 Loss 为： <span class="math display">\[l _ i=\frac{1}{M &#39;}\sum _ {x=1} ^ {M &#39;}\Vert X _ i[x]-C &#39; _ {gt}[x]\Vert _ 1 \tag{5}\]</span> 总的 Loss 为： <span class="math display">\[L _ {ds} = \sum _ {i=1} ^ I w _ i l _ i \tag{6}\]</span> 其中 \(w _ i\) 为权重，设为 1。</p><h3 id="consensus-driven-fusion">2.2. Consensus-driven Fusion</h3><p>　　最终实例的类别由点集合投票决定。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hong, Fangzhou, et al. "LiDAR-based Panoptic Segmentation via Dynamic Shifting Network." arXiv preprint arXiv:2011.11964 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　实例分割一般与语义分割同时进行，其难点是后处理如何准确的聚类出实例目标。目标聚类可归纳出的策略或方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;提高语义分割的准确率&lt;/strong&gt;，在聚类的时候加入语义约束，能改善不同类别的欠分割，以及同类别的过分割；&lt;/li&gt;

      
    
    </summary>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/Instance-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/tags/Instance-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;CenterFusion&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-CenterFusion/"/>
    <id>https://leijiezhang001.github.io/paper-reading-CenterFusion/</id>
    <published>2020-11-30T01:20:39.000Z</published>
    <updated>2020-12-02T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　自动驾驶领域多传感器融合对感知及定位都非常重要，对于感知而言，融合可分为数据前融合，特征级融合，目标状态后融合等类型。前融合对外参标定要求较高，后融合没有深度融合各传感器特征，特征级融合是比较折中的方法。<a href="/MOT-Fusion/" title="MOT Multimodal Fusion">MOT Multimodal Fusion</a> 中介绍了基于 BCM 来实现目标状态后融合的方法，本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出一种特征级融合相机以及毫米波雷达数据的目标检测方法。<br>　　<a href="/paper-reading-RadarNet/" title="RadarNet">RadarNet</a> 中详细介绍了毫米波雷达与激光雷达的优劣势，并提出了一种特征级融合方法，能更准确的测量目标的速度。此外，相机与激光雷达一样对恶劣天气环境比较敏感，所以毫米波与相机的结合，能有效利用毫米波能测量目标径向速度，应对恶劣环境以及测量范围较远等优势，并且保留相机高分辨率捕捉环境视觉信息的特性。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-CenterFusion/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，CenterFusion 网络首先用 CenterNet 作图像的 3D 目标检测，然后通过 Frustum Association Module 提取并融合对应的图像特征以及毫米波雷达特征，最后通过网络进一步准确估计目标的 3D 属性。<br>　　<a href="/Anchor-Free-Detection/" title="Anchor Free Detection">Anchor Free Detection</a> 以及 <a href="/CenterTrack/" title="CenTrack">CenTrack</a> 已经较为详细的介绍了 CenterNet 的网络结构和 Loss 形式，其真值 Heatmap 生成方式与 <a href="/paper-reading-AFDet/" title="AFDet">AFDet</a> 也类似，这里不做展开。<br><img src="/paper-reading-CenterFusion/vel.png" width="60%" height="60%" title="图 2. Radial Velocity"> 　　目前广泛使用的 3D 毫米波雷达测量的量有 \(x,y,v\)，如图 2. 所示，其中 \(v\) 是径向速度，为目标实际速度在径向的投影。为了准确估计目标的实际速度，需要估计目标的运动方向。</p><h2 id="association-and-feature-fusion">2. Association and Feature Fusion</h2><p>　　图像经过 CenterNet 得到目标的 2D size, Center Offset 等 2D 属性，以及 dimensions，depth, rotation 等 3D 属性。接下来要将 CenterNet 得到的目标与毫米波雷达的测量量进行数据关联，以便作进一步的特征融合与属性估计。</p><h3 id="frustum-association-mechanism">2.1. Frustum Association Mechanism</h3><p>　　基于图像的目标检测结果与毫米波雷达关联最简单的方法是将毫米波的测量点投影到图像中，看其是否处于图像 2D 框内。但是毫米波雷达测测量量没有 \(z\) 信息，所以这种方式不准确。<br><img src="/paper-reading-CenterFusion/asso.png" width="90%" height="90%" title="图 3. Frustum Association"> 　　本文提出一种锥形关联方法，如图 3. 所示，在俯视三维坐标下的锥形中进行关联，其中 \(\sigma\) 用来控制感兴趣锥形的尺寸，因为基于图像的目标 depth 估计准确度较差，所以 \(\sigma\) 可用来调节 depth 范围。图像目标只关联距离坐标原点最近的毫米波测量量。</p><h3 id="radar-feature-fusion">2.2. Radar Feature Fusion</h3><p>　　 当图像 2D 目标框与毫米波雷达测量量关联上后，将毫米波雷达的测量信息融合到 2D 框中。具体的，在 2D 目标框提取出的图像特征上，concate 毫米波雷达测量的 heatmap。heatmap 尺寸与 2D 目标框尺寸相关，heatmaps 定义为： <span class="math display">\[ F ^ j _ {x,y,i} = \frac{1}{M _ i}\left\{\begin{array}{l}f _ i \;\;\vert x - c _ x ^ j\vert \leq \alpha w ^j \;\mathrm{and}\;\vert y- c _ y ^ i\vert\leq\alpha h ^ j\\0 \;\;\mathrm{otherwise}\end{array}\tag{1}\right.\]</span> 其中 \(\alpha\) 是 heatmap 尺寸比例；\(i\in 1,2,3\) 是 heatmaps 特征维度；\(M _ i\) 是归一化系数；\(f _ i\in d, v _ x, v _ y\) 为毫米波雷达的测量量。<br>　　有了目标框内融合的图像及毫米波雷达特征后，可进一步精确估计目标的位置，朝向，速度，尺寸等 3D 属性。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Nabati, Ramin, and Hairong Qi. "CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection." arXiv preprint arXiv:2011.04841 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　自动驾驶领域多传感器融合对感知及定位都非常重要，对于感知而言，融合可分为数据前融合，特征级融合，目标状态后融合等类型。前融合对外参标定要求较高，后融合没有深度融合各传感器特征，特征级融合是比较折中的方法。&lt;a href=&quot;/MOT-Fusion/&quot; title=&quot;MOT
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
      <category term="Fusion" scheme="https://leijiezhang001.github.io/categories/3D-Detection/Fusion/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Radar" scheme="https://leijiezhang001.github.io/tags/Radar/"/>
    
      <category term="Fusion" scheme="https://leijiezhang001.github.io/tags/Fusion/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/</id>
    <published>2020-11-16T01:11:51.000Z</published>
    <updated>2020-10-19T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于点云的 Instance Segmentation 方法之前已经介绍过几种，其中将点云在 Bird-View 进行 Instance Segmentation 的思路基本与图像 Instance Segmentation 相似。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种图像 Instance Segmentation 方法，其能处理各种尺寸的目标，以及不用做聚类后处理，可直接得到目标实例。由于能处理较大尺寸的目标，所以也能应用于车道线检测领域，其思路值得借鉴。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，网络输出 Seed Branch 以及 Instance Branch。Seed Branch 中分数较高的表示每个类别每个实例的中心点，中心点具体坐标由该像素坐标以及对应的 offset 预测值决定。<br>　　Instance Branch 输出 offset vectors 以及 sigma maps。offset vectors 表示该像素点指向的对应实例的中心位置；sigma maps 表示 offset 指向中心的宽松度，是本方法能预测较大尺寸目标的关键。</p><h2 id="loss">2. Loss</h2><p>　　Instance Segmentation 的目标是将一堆二维像素点 \(\mathcal{X}=\{x _ 0, x _ 1, ..., x _ N\}\)，聚类成实例 \(\mathcal{S} = \{S _ 0, S _ 1, ..., S _ K\}\)。</p><h3 id="instance-branch">2.1. Instance Branch</h3><p>　　传统的做法是将每个像素点 \(x _ i\) 回归其与对应实例中心 \(C _ k=\frac{1}{N}\sum _ {x\in S _ k} x\) 的 offset 向量 \(o _ i\)，得到的 \(e _ i=x _ i+o _ i\) 即为该像素点指向的实例中心点。Loss 设计为： <span class="math display">\[\mathcal{L} _ {regr} = \sum _ {i=1} ^ n\Vert o _ i-\hat{o} _ i\Vert\tag{1}\]</span> 其中 \(\hat{o} _ i=C _ k-x _ i\)。<br>　　因为 \(e _ i\) 很难正好指向实例中心点，所以引入 hinge loss，让其指向实例中心周围 \(\sigma\) 范围区域： <span class="math display">\[\mathcal{L} _ {hinge} = \sum _ {k=1} ^ K\sum _ {e _ i\in S _ k}\mathrm{max}(\Vert e _ i-C _ k\Vert-\sigma, 0) \tag{2}\]</span> 从而保证： <span class="math display">\[e _ i\in S _ k \iff \Vert e _ i-C _ k\Vert &lt; \sigma \tag{3}\]</span> 但是这种方法需要根据最小目标来选择 \(\sigma\) 值，对于大目标，选取的 \(\sigma\) 又不太合理。<br>　　为了选取的 \(\sigma\) 能处理不同尺寸的实例目标，本文设计网络输出 sigma maps，在以实例中心为高斯概率分布下，每个像素属于该实例的概率为： <span class="math display">\[\phi _ k(e _ i) = \mathrm{exp}\left(-\frac{\Vert e _ i-C _ k\Vert ^ 2}{2\sigma _ k ^ 2}\right)\tag{4}\]</span> 当 \(\phi _ k(e _ i) &gt; 0.5\) 时，表示该像素点属于该实例。即： <span class="math display">\[e _ i\in S _ k \iff \mathrm{exp}\left(-\frac{\Vert e _ i-\hat{C} _ k\Vert ^ 2}{2\hat{\sigma} _ k ^ 2}\right) &gt; 0.5 \tag{5}\]</span> 由此像素回归指向中心点的区域可由 \(\sigma\) 控制： <span class="math display">\[\mathrm{margin} = \sqrt{-2\sigma _ k ^ 2\mathrm{ln}0.5} \tag{6}\]</span> 进一步得，可将 \(\sigma\) 分解为两个方向的值，形成椭圆状的二维高斯分布，这样能适应狭长型的目标： <span class="math display">\[\phi _ k(e _ i) = \mathrm{exp}\left(-\frac{\Vert e _ {ix}-C _ {kx}\Vert ^ 2}{2\sigma _ {kx} ^ 2}-\frac{\Vert e _ {iy}-C _ {ky}\Vert ^ 2}{2\sigma _ {ky} ^ 2}\right)\tag{7}\]</span> 以及，可将实例中心点像素坐标向量用特征向量代替： <span class="math display">\[\phi _ k(e _ i) = \mathrm{exp}\left(-\frac{\Vert e _ i-\frac{1}{\vert S _ k\vert}\sum _ {e _ j\in S _ k}e _ j\Vert ^ 2}{2\sigma _ k ^ 2}\right)\tag{8}\]</span> \(\sigma _ k\) 定义为： <span class="math display">\[\sigma _ k=\frac{1}{\vert S _ k\vert}\sum _ {\sigma _ i\in S _ k}\sigma _ i\tag{9}\]</span> 　　采用 Lovase-hinge loss 作用于像素点属于对应实例的概率图，\(\sigma\) 通过概率图隐式地学到。以及为了保证式 (9) 的一致性，增加 \(\sigma\) 平滑 Loss： <span class="math display">\[\mathcal{L} _ {smooth}=\frac{1}{\vert S _ k\vert}\sum _ {\sigma _ i\in S _ k}\Vert\sigma _ i-\sigma _ k\Vert ^ 2\tag{10}\]</span></p><h3 id="seed-branch">2.2. Seed Branch</h3><p>　　实例中心点的预测采用回归方法，背景点标签为零，前景标签是以实例中心为原点的高斯分布。Loss 设计为： <span class="math display">\[\mathcal{L} _ {seed} = \frac{1}{N}\sum _ i ^ N\mathbb{1} _ {\{s _ i\in S _ k\}}\Vert s _ i-\phi _ k(e _ i)\Vert ^ 2+\mathbb{1} _ {\{s _ i\in\mathbf{bg}\}}\Vert s _ i-0\Vert ^ 2 \tag{11}\]</span> 此时 \(\phi _ k(e _ i)\) 不作梯度反传。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> even, Davy, et al. "Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于点云的 Instance Segmentation 方法之前已经介绍过几种，其中将点云在 Bird-View 进行 Instance Segmentation 的思路基本与图像 Instance Segmentation 相似。本文&lt;a href=&quot;#1&quot; id=&quot;
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/Instance-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/tags/Instance-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Perception Uncertainty in Deep Learning</title>
    <link href="https://leijiezhang001.github.io/Perception-Uncertainty-in-Deep-Learning/"/>
    <id>https://leijiezhang001.github.io/Perception-Uncertainty-in-Deep-Learning/</id>
    <published>2020-11-12T02:32:13.000Z</published>
    <updated>2020-11-23T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　原文发布于 <a href="https://mp.weixin.qq.com/s/cSWIk7i2UhxqdadMDH_lmQ" target="_blank" rel="noopener">Deeproute 招聘公众号</a>。<br>　　基于深度学习感知技术的发展极大推动了自动驾驶行业的落地，然而基于深度学习的感知技术还存在很多问题，比如针对经典的目标检测任务，算法无法保证在所有场景下做到 100% 的检测准确率。由此，针对 L3/L4 自动驾驶，产品落地只能寄期望于 ODD(operational design domain) 的精心设计。要实现真正意义上的 L4/L5 自动驾驶，精准的环境感知很重要，但更重要的是，算法能否给出当前传感器数据及模型感知的不确定性(uncertainty)。换句话说，我们期望算法模型知道什么，但更期望算法模型不知道什么。<br>　　另一方面，毫米波雷达，相机，激光雷达等传感器各有优势，比如毫米波雷达能应对下雨等恶劣环境，相机能捕捉更丰富的语义信息，激光雷达能精确测量物理信息。所以多传感器融合是 L4/L5 自动驾驶的基础，而传感器及模型的不确定性估计，则又是多传感器后融合的基础。以下以多目标状态估计任务为例，来描述不确定的作用及估计方式。</p><h2 id="基于多传感器的目标状态估计">1. 基于多传感器的目标状态估计</h2><p>　　这里考虑基于毫米波雷达，相机，激光雷达等三种传感器的目标状态估计后融合方案。假设 3D 毫米波雷达测量的目标属性为 \((x, y, v)\)；基于深度学习的相机与激光雷达测量的目标属性均为 \((x, y, z, l, w, h, \theta)\)。经过前后多目标的数据关联后，这些量通过 KF 或 UKF 进行融合，最终得到目标状态的鲁棒估计。以最简单的 KF 为例，融合的过程需要测量量以及测量的方差。这里的方差就是我们要讨论的不确定性，即假设测量满足高斯分布下对应的方差。<br>　　传统的做法是将测量方差设为经验固定值，但是在多传感器融合框架里，这样无法区分 1) 单传感器在不同场景下算法模型的性能；2) 多传感器在同一场景下算法模型的性能。由此，基于深度学习的感知测量输出需要同时估计测量值的不确定性，以此作更鲁棒的状态估计。</p><h2 id="不确定性概述">2. 不确定性概述</h2><p>　　不确定性由传感器及模型产生。传感器方面，比如激光雷达的点云测量存在厘米级别的误差。模型方面，在贝叶斯框架下，可以建模两大类不确定性:</p><ul><li>认知不确定性(Epistemic Uncertainty)，描述模型因为缺少训练数据而存在的未知，可通过增加训练数据解决；</li><li>偶然不确定性(Aleatoric Uncertainty)，描述数据不能解释的信息，可通过提高数据的精度来消除，与传感器相关；<ul><li>数据依赖/异方差不确定性(Data-dependent/Heteroscedastic Uncertainty)，与模型输入数据有关，可作为模型预测输出；</li><li>任务依赖/同方差不确定性(Task-dependent/Homoscedastic Uncertainty)，与模型输入数据无关，且不是模型的预测输出，不同任务有不同的值；</li></ul></li></ul><p>　　认知不确定性多应用于 Active Learning，数据可标注性等领域，能有效挖掘提升模型感知能力的数据。同方差不确定性多应用于神经网络的多任务学习，能根据各个任务对应的数据方差学习其损失函数权重，提升所有任务的整体性能。异方差不确定性是反映在线网络预测量是否可信的主要不确定性。</p><h2 id="贝叶斯深度学习中不确定性的数学描述">3. 贝叶斯深度学习中不确定性的数学描述</h2><p>　　针对一批训练数据集\(\{\mathbf{X,Y}\}\)，训练模型 \(\mathbf{y=f^W(x)}\)，在贝叶斯框架下，预测量的后验分布为： <span class="math display">\[p\left(\mathbf{y\vert x,X,Y}\right) = \int p\left(\mathbf{y\,|\,f^W(x)}\right) p\left(\mathbf{W\,|\,X,Y}\right)d\mathbf{W} \tag{1}\]</span> 其中 \(p(\mathbf{W\,|\,X,Y})\) 为模型参数的后验分布，描述了模型的不确定性，即认知不确定性；\(p\left(\mathbf{y\,|\,f^W(x)}\right)\) 为观测似然，描述了观测不确定性，即偶然不确定性。</p><h3 id="认知不确定性">3.1. 认知不确定性</h3><p>　　认知不确定性表征的是模型的不确定性，对于训练集 \(\mathrm{D}=\{\mathbf{X},\mathbf{Y}\}\)，认知不确定性即为权重参数的分布 \(p(\mathbf{\omega} | \mathbf{X},\mathbf{Y})\)。可采用 Monte-Carlo 采样方法来近似估计模型权重分布: <span class="math display">\[p(\omega|\mathbf{X},\mathbf{Y})\approx q(\mathbf{\omega};\mathbf{\Phi})=Bern(\mathbf{\omega};\mathbf{\Phi}) \tag{2}\]</span> 其中 \(\mathbf{\Phi}\) 是 Bernolli Rates，具体的采样通过 Dropout 实现。在训练阶段，Dropout 等价于优化网络权重 \(W\) 的 Bernoulli 分布；在测试阶段，使用 Dropout 对样本进行多次测试，能得到模型权重的后验分布，由此模型的不确定性即为 T 次采样的方差： <span class="math display">\[\mathbf{Var} _ {p(\mathbf{y}|\mathbf{x})} ^ {model}(\mathbf{y})=\sigma _ {model} = \frac{1}{T}\sum _ {t=1} ^ T(\mathbf{y} _ t-\bar{\mathbf{y}}) ^ 2\tag{3}\]</span> 其中 \(\{\mathbf{y} _ t\} _ {t=1} ^ T\) 是不同权重 \(\omega ^ t\sim q(\omega;\mathbf{\Phi})\) 采样下的输出。</p><p>　　这种模型不确定性的计算方式，直观的理解为：当模型对某些数据预测比较好，误差比较小的时候，那么模型对这些数据的冗余度肯定是较高的，所以去掉模型的一部分网络，模型对这些数据的预测与原模型应该会有较高的一致性，即不确定性会较小。</p><h3 id="偶然不确定性">3.2. 偶然不确定性</h3><p>　　偶然不确定性估计是最大化高斯似然过程。对于回归任务，定义模型输出为高斯分布： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \mathcal{N}\left(\mathbf{f^W(x)}, \sigma ^2\right) \tag{4}\]</span> 其中 \(\sigma\) 为观测噪声方差，描述了模型输出中含有多大的噪声。对于分类任务，玻尔兹曼分布下的模型输出概率分布为： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)},\sigma\right) = \mathrm{Softmax}\left(\frac{1}{\sigma ^2}\mathbf{f^W(x)}\right) \tag{5}\]</span> 由此对于多任务，模型输出的联合概率分布为： <span class="math display">\[p\left(\mathbf{y}_1,\dots,\mathbf{y}_K\vert\mathbf{f^W(x)}\right) = p\left(\mathbf{y}_1\vert\mathbf{f^W(x)}\right) \dots p\left(\mathbf{y}_K\vert\mathbf{f^W(x)}\right) \tag{6}\]</span></p><p>　　对于回归任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) \propto -\frac{1}{2\sigma ^2} \Vert \mathbf{y-f^W(x)} \Vert ^2 - \mathrm{log}\sigma \tag{7}\]</span> 对于分类任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}=c\vert\mathbf{f^W(x)}, \sigma\right) = \frac{1}{2\sigma ^2}f_c^{\mathbf{W}}(\mathbf{x})- \mathrm{log}\sum_{c&#39;} \mathrm{exp}\left(\frac{1}{\sigma^2}f^{\mathbf{W}}_{c&#39;}(\mathbf{x}) \right) \tag{8}\]</span></p><p>　　最大化高斯似然，等价于最小化其负对数似然函数。现同时考虑回归与分类任务，则多任务的联合 \(Loss\)： <span class="math display">\[\begin{align}\mathcal{L}(\mathbf{W}, \sigma _1, \sigma _2) &amp;= -\mathrm{log}p\left(\mathrm{y_1,y_2}=c\vert\mathbf{f^W(x)} \right) \\&amp;= -\mathrm{log}\mathcal{N}\left(\mathbf{y_1};\mathbf{f^W(x)}, \sigma_1^2\right) \cdot \mathrm{Softmax}\left(\mathbf{y_2}=c;\mathbf{f^W(x)},\sigma_2\right) \\&amp;= \frac{1}{2\sigma_1^2}\Vert \mathbf{y}_1-\mathbf{f^W(x)}\Vert ^2 + \mathrm{log}\sigma_1 - \mathrm{log}p\left(\mathbf{y}_2=c\vert\mathbf{f^W(x)},\sigma_2\right) \\&amp;= \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\frac{\sum_{c&#39;}\mathrm{exp}\left(\frac{1}{\sigma_2^2}f_{c&#39;}^{\mathbf{W}}(x)\right)}{\left(\sum_{c&#39;}\mathrm{exp}\left(f_{c&#39;}^{\mathbf{W}}(x) \right) \right)^{\frac{1}{\sigma_2^2}}} \\&amp;\approx \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\sigma_2 \tag{9}\end{align}\]</span></p><p>由此可见，分类及回归的偶然不确定性估计，可通过额外预测对应的方差，并将方差通过上式作用于损失函数实现。实际应用中，为了数值稳定，可令 \(s:=\mathrm{log}\sigma^2\)。</p><h2 id="不确定性估计方法">4. 不确定性估计方法</h2><h3 id="统计法">4.1. 统计法</h3><p>　　观测量的方差(不确定性)与目标的属性有关，如距离，遮挡，类别等。可以按照不同属性，统计不同的方差。这种统计出来的方差实际上就是在特定传感器精度下，标注的不确定性，比如随着距离越远点云越稀少，标注误差也会越大。这样统计出来的方差与实际网络输出的不确定性不是等价的，但是模型训练好后，模型预测的分布是与训练集分布是相似的，所以用训练集的方差来直接代替模型预测的方差也合理。<br>　　但是更准确的来说，不确定性对每个目标都应该是不同的，这里只统计了特定属性以及标注误差所产生的不确定性，而实际上遮挡大的目标，是更难学习的(目标学习有难易之分，即预测分布与训练集分布会有偏差)，即预测结果会有额外的不确定性，所以这种离线统计方法也有很大的局限性。</p><h3 id="网络分支预测法">4.2. 网络分支预测法</h3><p>　　假设网络输出符合多变量混合高斯分布，将偶然不确定性设计为网络的输出， <span class="math display">\[\left\{\begin{array}{l}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \sum_k \alpha_k \mathcal{N}\left(\mathbf{f^W(x)}_{(k)}, \Sigma(\mathbf{x})_{(k)} \right)\\\sum_k \alpha_k = 1\end{array}\tag{10}\right.\]</span> 　　对于 3D Detection 问题，网络输出的 3D 框参数为 \(\mathbf{y}=(x,y,z,l,h,w,\theta)\)，当输出满足 \(K\) 个混合高斯分布时，网络的输出量有：</p><ul><li>\(K\) 组目标框参数预测量 \(\{\mathbf{y}_k\}\)；</li><li>\(K\) 个对数方差 \(\{s_k\}\)；</li><li>\(K\) 个混合高斯模型权重参数 \(\{\alpha_k\}\)；</li></ul><p>　　训练时，找出与真值分布最近的一组预测量，混合高斯模型权重用分类问题的 Loss 求解，找到最相似的分布后，将该分布的方差用式 (9) 作用于回归的 Loss 项；测试时，找到混合高斯模型最大的权重项，对应的高斯分布，即作为最终的输出分布。这里只考虑了输出 3D 框的一个整体的方差，也可以输出定位方差+尺寸方差+角度方差，只要将该方差作用于对应的 Loss 项即可。当 \(K=1\) 时，就是多变量单高斯模型。</p><h3 id="assumed-density-filteringadf-估计法">4.3. Assumed Density Filtering(ADF) 估计法</h3><p>　　假设传感器得到的数据符合噪音水平 \(\mathbf{v}\) 的高斯分布，那么输入网络的数据 \(\mathbf{z}\) 与其真实数据 \(\mathbf{x}\) 的关系为： <span class="math display">\[q(\mathbf{z}|\mathbf{x})\sim \mathcal{N}(\mathbf{z};\mathbf{x},\mathbf{v})\tag{11}\]</span> 为了计算网络预测量的不确定性，通过 Assumed Density Filtering(ADF) 来传递输入数据的噪声，从而计算模型的偶然不确定性。网络的联合概率分布为： <span class="math display">\[p(\mathbf{z}^{(0:l)})=p(\mathbf{z}^{(0)})\prod _ {i=1} ^ l p(\mathbf{z}^{(i)}|\mathbf{z} ^ {(i-1)}) \tag{12}\]</span> 其中： <span class="math display">\[p(\mathbf{z}^{(i)}|\mathbf{z}^{(i-1)})=\sigma[\mathbf{z} ^ {(i)}-\mathbf{f} ^ {(i)}(\mathbf{z}^{(i-1)})]\tag{13}\]</span> ADF 将其近似为： <span class="math display">\[p(\mathbf{z}^{(0:l)})\approx q(\mathbf{z}^{(0:l)})=q(\mathbf{z}^{(0)})\prod _ {i=1} ^ l q(\mathbf{z}^{(i)}) \tag{14}\]</span> 其中 \(q(\mathbf{z})\) 符合独立高斯分布： <span class="math display">\[q(\mathbf{z}^{(i)})\sim \mathcal{N}\left(\mathbf{z}^{(i)};\mathbf{\mu}^{(i)},\mathbf{v}^{(i)}\right)=\prod _ j\left(\mathbf{z} _ j ^ {(i)};\mathbf{\mu} _ j ^ {(i)}, \mathbf{v} _ j ^ {(i)}\right)\tag{15}\]</span> 特征 \(\mathbf{z}^{(i-1)}\) 通过第 \(i\) 层映射方程 \(\mathbf{f} ^{(i)}\)，得到： <span class="math display">\[\hat{p}(\mathbf{z}^{(0:i)})=p(\mathbf{z}^{(i)}|\mathbf{z}^{(i-1)})q(\mathbf{z}^{(0:i-1)}) \tag{16}\]</span> ADF 的目标是找到 \(\hat{p}(\mathbf{z}^{(0:i)})\) 的近似分布 \(q(\mathbf{z}^{(0:i)})\)，比如 KL divergence： <span class="math display">\[q(\mathbf{z}^{(0:i)})=\mathop{\arg\min}\limits _ {\hat{q}(\mathbf{z}^{(0:i)})}\mathbf{KL}\left(\hat{q}(\mathbf{z}^{(0:i)})\;\Vert\;\hat{p}(\mathbf{z}^{(0:i)})\right)\tag{17}\]</span> 基于高斯分布的假设，误差通过每层的映射方程 \(\mathbf{f} ^{(i)}\) 进行独立传播，故以上解为： <span class="math display">\[\begin{align}\mathbf{\mu}^{(i)}=\mathbb{E} _ {q(\mathbf{z}^{(i-1)})}[\mathbf{f}^{(i)}(z^{(i-1)})]\\\mathbf{v}^{(i)}=\mathbb{V} _ {q(\mathbf{z}^{(i-1)})}[\mathbf{f}^{(i)}(z^{(i-1)})]\\\end{align}\tag{18}\]</span> 以映射方程 \(\mathbf{f} ^{(i)}\) 为卷积层为例，均值传递就是正常的卷积操作，方差传递则需要将卷积权重平方，然后作卷积操作。其它神经网络层都可推导出对应的方差传递方程。<br>　　结合蒙特卡洛采样计算认知不确定性与 ADF 方法计算偶然不确定性，网络预测的结果与对应的总的不确定性可计算为： <span class="math display">\[\left\{\begin{array}{l}\mu = \frac{1}{T}\sum _ {t=1} ^ T \mathbf{\mu} _ t ^ {(l)}\\\sigma _ {tot} = \frac{1}{T}\sum _ {t=1} ^ {T} \mathbf{v} _ t ^ {(l)} + \frac{1}{T}\sum _ {t=1} ^ T\left(\mathbf{\mu} _ t ^ {(l)}-\bar{\mathbf{\mu}}\right) ^ 2\end{array}\tag{19}\right.\]</span> 其中 \(\{\mathbf{\mu} _ t ^ {(l)},\mathbf{v} _ t ^ {(l)}\} _ {t=1} ^ T\) 是 ADF 网络 \(T\) 次蒙特卡洛采样结果。<strong>由此可见，不同于以往将认知不确定性和偶然不确定性完全作独立假设的方式，本方法是将二者联合来估计的。这也比较好理解，如果数据噪音很大，那么模型就很难训，其模型不确定性也会很大，所以二者不可能是完全独立的</strong>。该方法可归纳为：</p><ol start="0" type="1"><li>以正常方式训练网络；</li><li>将现有的网络转换为 ADF 网络形式，增加每层的方差传递函数；</li><li>计算 \(T\) 次蒙特卡洛采样的网络输出；</li><li>计算网络预测的均值和方差。</li></ol><h2 id="总结">5. 总结</h2><p>　　本文以多传感器目标检测后融合任务出发，介绍了基于深度学习的感知不确定性估计方法。需要注意的是，一般情况下，同一传感器同一模型的不同预测量之间不确定性的相对大小才有意义。所以进行多传感器融合时，需要对不同模型估计的不确定性进行幅值标定，这里不作展开。</p><h2 id="参考文献">6. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Kendall, Alex, and Yarin Gal. "What uncertainties do we need in bayesian deep learning for computer vision?." Advances in neural information processing systems. 2017.<br><a id="2" href="#2ref">[2]</a> Gal, Yarin. Uncertainty in deep learning. Diss. PhD thesis, University of Cambridge, 2016.<br><a id="3" href="#1ref">[3]</a> Loquercio, Antonio , Segù, Mattia, and D. Scaramuzza . "A General Framework for Uncertainty Estimation in Deep Learning." (2019).<br><a id="4" href="#1ref">[4]</a> Kendall, Alex, Yarin Gal, and Roberto Cipolla. "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　原文发布于 &lt;a href=&quot;https://mp.weixin.qq.com/s/cSWIk7i2UhxqdadMDH_lmQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Deeproute 招聘公众号&lt;/a&gt;。&lt;br&gt;
　　基于深度学习感知技术的
      
    
    </summary>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/categories/Uncertainty/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning for 3D Point Clouds</title>
    <link href="https://leijiezhang001.github.io/Deep-Learning-for-3D-Point-Clouds/"/>
    <id>https://leijiezhang001.github.io/Deep-Learning-for-3D-Point-Clouds/</id>
    <published>2020-11-04T02:17:58.000Z</published>
    <updated>2020-11-27T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文介绍三篇点云相关的综述文章，并作归纳分析。<a href="#1" id="1ref">[1]</a> 的目录结构为：</p><ul><li>3D Shape Classification<ul><li>Multi-view based Methods</li><li>Volumetric-based Methods</li><li>Point-based Methods<ul><li>Pointwise MLP Methods</li><li>Convolution-based Methods</li><li>Graph-based Methods</li><li>Hierarchical Data Structure-based Methods</li><li>Other Methods</li></ul></li></ul></li><li>3D Object Detection and Tracking<ul><li>3D Object Detection<ul><li>Region Proposal-based Methods</li><li>Single Shot Methods</li></ul></li><li>3D Object Tracking</li><li>3D Scene Flow Estimation</li></ul></li><li>3D Point Cloud Segmentation<ul><li>3D Semantic Segmentation<ul><li>Projection-based Methods</li><li>Discretization-based Methods</li><li>Hybrid Methods</li><li>Point-based Methods</li></ul></li><li>Instance Segmentation<ul><li>Proposal-based Methods</li><li>Proposal-free Methods</li></ul></li><li>Part Segmentation</li></ul></li></ul><p><a href="#2" id="2ref">[2]</a> 的目录结构为：</p><ul><li>Classification<ul><li>Projection-based Methods<ul><li>Multi-view Representation</li><li>Volumetric Representation</li><li>Basis Point Set</li></ul></li><li>Point-based Methods<ul><li>MLP Networks</li><li>Convolutional Networks</li><li>Graph Networks</li><li>Other Networks</li></ul></li></ul></li><li>Segmentation<ul><li>Semantic Segmentation<ul><li>Projection-based Methods</li><li>Point-based Methods</li></ul></li><li>Instance Segmentation<ul><li>Proposal-based Methods</li><li>Proposal-free Methods</li></ul></li></ul></li><li>Detection, Tracking and Flow Estimation<ul><li>Object Detection<ul><li>Projection-based Methods</li><li>Point-based Methods</li><li>Multi-view Methods</li></ul></li><li>Object Tracking</li><li>Scene Flow Estimation</li></ul></li><li>Registration<ul><li>Traditional Methods</li><li>Learning-based Methods</li></ul></li><li>Augmentation and Completion<ul><li>Discriminative Methods</li><li>Generative Methods</li></ul></li></ul><p><a href="#3" id="3ref">[3]</a> 只分析了 Segmentation 和 Detection 任务，每个任务都从 Point-based，Voxel-based，View-based 三种方法来阐述。<br>　　本文结合这三篇综述，对不同任务的不同方法作详尽的归纳。</p><h2 id="datasets">1. Datasets</h2><p><img src="/Deep-Learning-for-3D-Point-Clouds/datasets.png" width="90%" height="90%" title="图 1. Datasets"></p><h2 id="metrics">2. Metrics</h2><p><img src="/Deep-Learning-for-3D-Point-Clouds/metrics.png" width="90%" height="90%" title="图 2. Metrics"> 　　不同任务的主要度量指标如图 2. 所示，此外还有：</p><ul><li>Average Precision(AP)<br>用于 3D 目标检测，计算的是 precision-recall 曲线下的面积：\(AP=\frac{1}{11}\sum _ {r\in\{0,0.1,...,1\}}\mathop{\max}\limits _ {\tilde{r} : \tilde{r}\geq r} p(\tilde{r})\)，其中 \(r\) 表示 recall，\(p(r)\) 表示对应的 precision。</li><li>Average Orientation Similarity(AOS)<br>类似的，\(AOS=\frac{1}{11}\sum _ {r\in\{0,0.1,...,1\}}\mathop{\max}\limits _ {\tilde{r} : \tilde{r}\geq r} s(\tilde{r})\)。</li><li>Panoptic Quality(PQ)<a href="#4" id="4ref"><sup>[4]</sup></a><br>用于全景分割，可分解为 Segmentation Quality(SQ) 以及 Recognition Quality(RQ)，即同时评估语义分割及 Instance 分割，定义为： <span class="math display">\[\begin{align}PQ &amp;= \frac{\sum _ {(p,g)\in TP}IoU(p,g)}{\vert TP\vert+\frac{1}{2}\vert FP\vert+\frac{1}{2}\vert FN\vert} \\&amp;= \frac{\sum _ {(p,g)\in TP}IoU(p,g)}{\vert TP\vert} \times\frac{\vert TP\vert}{\vert TP\vert+\frac{1}{2}\vert FP\vert+\frac{1}{2}\vert FN\vert} \\&amp;= SQ \times RQ\tag{1}\end{align}\]</span> \(RQ\) 就相当于 F1-Score，只不过这里的 TP 可能是根据点集交并比来判断的。</li></ul><h2 id="classification">3. Classification</h2><p><img src="/Deep-Learning-for-3D-Point-Clouds/classification.png" width="100%" height="100%" title="图 3. Classification"> 　　分类是最基础的任务，创新的网络结构基本首先在分类任务中进行应用，网络结构的创新目的本质上又是更有效的特征提取。<a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a> 中已经较详细的描述了基于点的点云特征提取的相关进展，如图 3. 所示，所有高层任务所用到的网络结构基本可以在这找到对应的基础网络。</p><h3 id="multi-view-based-methods">3.1. Multi-view based Methods</h3><p>　　这种方法将点云投影到特定的几种视角平面上，提取特征后再将各个视角的特征进行整合，最后得到全局特征，来作分类。一般选取 Bird-eye View 与 Front View 两个视角，BV 的好处是目标尺寸的一致性，FV 的好处是对狭长型目标较为友好。代表方法由 MVCNN，MHBN，View-GCN 等。</p><h3 id="volumetric-based-methods">3.2. Volumetric-based Methods</h3><p>　　这种方法将点云量化为 3D 体素(Voxel)，然后采用 3D 卷积作特征提取。方法有 VoxNet，OctNet，O-CNN 等。</p><h3 id="point-based-methods">3.3. Point-based Methods</h3><p>　　此类方法玩法较多，因为直接在原始点云上进行特征提取，所以没有额外的信息损失。其研究的变种也较多。</p><h4 id="pointwise-mlp-methods">3.3.1. Pointwise MLP Methods</h4><p>　　典型代表为 PointNet，直接对每个点的特征维度进行 MLP 变换，然后用 Symmetric Operator 在点的维度进行 Reduction 得到全局特征。此外可以加入各种采样策略，作特征的级联采样并提取，其它方法有 DeepSets，PointNet++，Mo-Net，PATs，PointWeb，SRN，JustLookUp，PointASNL 等。</p><h4 id="convolution-based-methods">3.3.2. Convolution-based Methods</h4><p><img src="/Deep-Learning-for-3D-Point-Clouds/3D-conv.png" width="60%" height="60%" title="图 4. Continuous and Discrete Convolution"> 　　由于点云数据的离散性，传统的图像 2D/3D 卷积无法直接使用。如图 4. 所示，根据卷积核的定义方式，基于点的卷积可分为 Continuous Convolution 和 Discrete Convolution。</p><ul><li>Continuous Convotion<br>对于以某一中心点作卷积的点集，其卷积核权重是与点集相对该中心点的空间分布有关。权重，即空间分布的计算，一般通过 MLP 网络对每个点的欧式距离等特征编码得到。</li><li>Discrete Convotion<br>对于以某一中心点作卷积的点集，其卷积核权重是与点集相对该中心点的空间残差有关。首先对点集区域以中心点作一定形状的栅格化，对每个栅格内的点进行特征提取，然后再以栅格为单位，作类似传统的卷积操作。</li></ul><h4 id="graph-based-methods">3.3.3. Graph-based Methods</h4><p>　　该方法将点云中的点建模为图顶点，然后将相邻点进行有向连接得到有向图。点云的特征提取可在 Spatial 或 Spectral 空间内进行。<br>　　在 Spatial 空间，图顶点通常包含坐标值，反射率，颜色等初始特征，图边通常与连接边的两个顶点空间距离有关，一般通过 MLP 网络构建，<a href="/paper-reading-Point-GNN/" title="Point-GNN">Point-GNN</a> 中比较详细得描述了这一过程。<br>　　在 Spectral 空间，卷积定义为光谱滤波器，用 Graph Laplacian Matrix 的特征向量相乘实现。</p><h4 id="hierarchical-data-structure-based-methods">3.3.4. Hierarchical Data Structure-based Methods</h4><p>　　该类方法将点云构建成级联的类树状结构，特征学习通过树叶至树根传递。代表方法有 OctNet，Kd-Net 等。</p><h4 id="other-methods">3.3.5. Other Methods</h4><h2 id="detection-and-tracking">4. Detection and Tracking</h2><h3 id="d-object-detection">4.1. 3D Object Detection</h3><p><img src="/Deep-Learning-for-3D-Point-Clouds/detection.png" width="100%" height="100%" title="图 5. detection methods"></p><p><img src="/Deep-Learning-for-3D-Point-Clouds/det_comp.png" width="100%" height="100%" title="图 6. detection methods comparison"> 　　目标检测是非常重要的一个任务，不同类型的方法如图 5. 所示，其性能如图 6. 所示。</p><h4 id="region-proposal-based-methods">4.1.1. Region Proposal-based Methods</h4><p>　　这种方法首先找出候选区域框，然后作进一步的目标属性精修。根据候选框的产生方式，可分为 Multi-view based Methods，Segmentation-based Methods 以及 Frustum-based Methods。<br>　　Multi-view based Methods 一般速度比较慢，会融合不同模态的数据，算法也比较复杂。<br>　　Segmentation-based Methods 先通过语义分割的方法去除背景区域点，根据前景点生成高质量的候选框集合，由较高的召回率，且善于处理遮挡等较为复杂的场景。<br>　　Frustum-based Methods 通常通过图像生成候选目标框，然后根据视锥去提取激光点云或毫米波点云的目标测量，对于中后期的融合，该方法应用也较多。</p><h4 id="single-shot-methods">4.1.2. Single Shot Methods</h4><p>　　单阶段方法没有提取候选框这个步骤，网络直接预测目标的 3D 属性，可分为 BEV-based，Discretization-based Methods，以及 Point-based Methods。<br>　　BEV-based Methods，Discretization-based Methods 均通过离散化点云空间，然后作类似图像的 2D 卷积或 3D 卷积操作。Point-based Methods 则直接在点云级别作特征提取以及目标检测。这些方法之前讨论较多了，不做展开。</p><h3 id="d-object-tracking">4.2. 3D Object Tracking</h3><p>　　相比图图像的 2D 跟踪，3D 跟踪能比较容易的解决遮挡，光照，尺度等问题。本博客讨论过基于传统 ICP 的 <a href="/ADH-Tracker/" title="ADH-Tracker">ADH-Tracker</a> 方法，也介绍过基于深度学习的 <a href="/paper-reading-P2B/" title="P2B">P2B</a> 方法。总体上来讲，3D 目标跟踪套路较少，除非将检测，跟踪，预测联合来优化，比如 <a href="/paper-reading-PnPNet/" title="PnPNet">PnPNet</a>。</p><h3 id="d-scene-flow-estimation">4.3. 3D Scene Flow Estimation</h3><p>　　3D Scene Flow 问题定义为：给定 \(\mathcal{X,Y}\) 两个点云集，3D Scene Flow \(D=\{d _ i\} ^ N\) 描述了 \(\mathcal{X}\) 中的点 \(x _ i\) 与其在 \(\mathcal{Y}\) 中的最近对应点 \(x _ i '\) 的距离 \(x _ i '=x _ i+d _ i\)。<br>　　3D Scene Flow 是一个中低层任务，根据 3D Scene Flow 可以进一步作运动物体分割聚类，目标运动速度估计等高层任务。<a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 是较早采用深度学习进行 3D Scene Flow 估计的方法。类似图像中的光流估计，可采用无监督方法，对应的 Loss 有 EMD 等。</p><h2 id="segmentation">5. Segmentation</h2><h3 id="semantic-segmentation">5.1. Semantic Segmentation</h3><p><img src="/Deep-Learning-for-3D-Point-Clouds/semantic-seg.png" width="100%" height="100%" title="图 7. semantic segmentation methods"></p><p><img src="/Deep-Learning-for-3D-Point-Clouds/seg_comp.png" width="100%" height="100%" title="图 8. semantic segmentation comparison"> 　　语义分割方法如图 7. 所示，各方法的性能如图 8. 所示。</p><h4 id="projection-based-methods">5.1.1. Projection-based Methods</h4><p>　　点云语义分割需要尽可能保留点级别的特性信息，投影法基本上可分为 BirdView，Spherical，Cylinde 三种，代表方法有 <a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a>，<a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>，<a href="/paper-reading-Cylinder3D/" title="Cylinder3D">Cylinder3D</a> 等。</p><h4 id="discretization-based-methods">5.1.2. Discretization-based Methods</h4><p>　　这种方法是将点云空间体素化，然后用 3D 卷积的形式来提取特征。一般会丢失点级别的信息，所以需要点级别的信息提取方式来辅助。</p><h4 id="point-based-methods-1">5.1.3. Point-based Methods</h4><p>　　在点云语义分割中，点级别的特征提取是非常有必要的，一般采用点级别的 MLP，PointNet Convolution，RNN，Graph-based 等方法，本质上都是对每个点周围的点集作特征提取操作。<a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中有较详细的描述。</p><h3 id="instance-segmentation">5.2. Instance Segmentation</h3><p><img src="/Deep-Learning-for-3D-Point-Clouds/ins-seg.png" width="80%" height="80%" title="图 9. instance segmentation methods"> 　　实例分割不仅作语义分割，还得将同一目标的点云作聚类处理。其可分为 Proposal-based Methods，以及 Proposal-free Methods。Proposal-based 将实例分割分解为目标检测以及实例 mask 预测两个任务；Proposal-free 则是自下而上的求解该问题，一般通过预测语义及聚类辅助量，最后通过相关聚类策略实现。相关方法如图 9. 所示。</p><h2 id="registration">6. Registration</h2><p>　　点云注册主要有两大块应用，本车的位姿估计以及目标车的速度估计。对于前后帧点云的注册能估计出前后帧时间内本车的位姿变化；对于目标点云的注册能测量出目标的速度。<br>　　传统的点云注册方法通过 ICP 实现，详见 <a href="/Object-Registration-with-Point-Cloud/" title="Object Registration with Point Cloud">Object Registration with Point Cloud</a> 中的描述。对于两个点云集合，也可通过网络求解 \(R,t\)。</p><h2 id="augmentation-and-completion">7. Augmentation and Completion</h2><p>　　激光点云数据往往受噪声，离群点，未测量点影响，可以采用网络对点云进行噪声滤出处理，以及点云补全处理，这里不做展开。</p><h2 id="conclusion">8. Conclusion</h2><p>　　基于点云的每个任务对应的方法，其实都可分为 View-based，Voxel-based，Point-based 三大类，当然也可以结合来做。其中 View-based 主要由 BirdView，Spherical，Cylinde 三种；Voxel-based 则主要分为 2D/3D；Point-based 则玩法较多，不过本质上还是对点周围的小点集作特征提取。</p><h2 id="reference">9. Reference</h2><p><a id="1" href="#1ref">[1]</a> Guo, Yulan, et al. "Deep learning for 3d point clouds: A survey." IEEE transactions on pattern analysis and machine intelligence (2020).<br><a id="2" href="#2ref">[2]</a> Lu, Haoming, and Humphrey Shi. "Deep Learning for 3D Point Cloud Understanding: A Survey." arXiv preprint arXiv:2009.08920 (2020).<br><a id="3" href="#3ref">[3]</a> Li, Ying, et al. "Deep learning for LiDAR point clouds in autonomous driving: a review." IEEE Transactions on Neural Networks and Learning Systems (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文介绍三篇点云相关的综述文章，并作归纳分析。&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt; 的目录结构为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D Shape Classification
&lt;ul&gt;
&lt;li&gt;Multi-view based Methods&lt;/l
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
      <category term="Review" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/Review/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Learning Efficient Convolutional Networks through Network Slimming&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/</id>
    <published>2020-10-26T01:35:44.000Z</published>
    <updated>2020-11-02T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　剪枝是神经网络加速的重要手段之一，<a href="/pruning/" title="Pruning">Pruning</a> 中较详细的描述了剪枝的特性与基本方法，<a href="/Filter-Pruning/" title="Filter-Pruning">Filter-Pruning</a> 则描述了卷积核剪枝的基本方法。Filter Pruning 属于结构化剪枝(Structure Pruners)，能在不改变硬件计算单元的情况下，提升网络计算速度。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 就属于 Filter Pruning 方法，简单有效，较为经典。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，本文提出的方法非常简单：将各通道特征(对应各卷积核)乘以一个尺度系数，并用正则化方法来稀疏化尺度系数。训练后卷积核对应的较小的尺度，则认为其不重要，进行剪枝。最终 Fine-Tune 剩下的网络即可。<br><img src="/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/pipeline.png" width="70%" height="70%" title="图 2. Pipeline"> 　　其剪枝及 Fine-Tune 过程如图 2. 所示，可迭代进行，以获得最优的剪枝效果。<br>　　实际操作中用 BN 层中的 \(\gamma\) 系数来代替该尺度系数。BN 层计算过程为： <span class="math display">\[\hat{z}=\frac{z _ {in}-\mu _ {\mathcal{B}}}{\sqrt{\sigma _ {\mathcal{B}} ^ 2 + \epsilon}}; \; z _ {out}=\gamma \hat{z}+\beta\tag{1}\]</span> 其中 \(z _ {in}, z _ {out}\) 表示 BN 层的输入输出，\(\mathcal{B}\) 表示当前 mini-batch，\(\mu _ {\mathcal{B}},\sigma _ {\mathcal{B}}\) 表示 mini-batch 中 channel-wise 的均值和方差，\(\gamma,\beta\) 为可训练的 channel-wise 的尺度及偏移系数。<br>　　对 \(\gamma\) 进行 L1 正则化，较小的 \(\gamma\) 对应的卷积核即认为是不重要的，可裁剪掉。</p><h2 id="thinking">2. Thinking</h2><p>　　本方法非常简单，但是为什么仅凭较小的 \(\gamma\) 即可确定对应的卷积核不重要呢？我的思考如下：</p><ul><li>为什么不需要考虑卷积核的 L1 值？<br>因为 BN 的 \(\mu _ {\mathcal{B}},\sigma _ {\mathcal{B}}\) 将输出归一化了，所以卷积核的值幅度对之后没有影响，故其值幅度无法体现其重要性；</li><li>为什么不需要考虑 \(\beta\) 值？<br>因为 \(\beta\) 只会影响输出特征的均值，而不会影响输出特征的方差，神经网络的表达能力在于特征的方差，而不是均值，故 \(\gamma\) 才能体现卷积核的重要性，而 \(\beta\) 不能。</li></ul><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Liu, Zhuang, et al. "Learning efficient convolutional networks through network slimming." Proceedings of the IEEE International Conference on Computer Vision. 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　剪枝是神经网络加速的重要手段之一，&lt;a href=&quot;/pruning/&quot; title=&quot;Pruning&quot;&gt;Pruning&lt;/a&gt; 中较详细的描述了剪枝的特性与基本方法，&lt;a href=&quot;/Filter-Pruning/&quot; title=&quot;Filter-Pruning&quot;&gt;F
      
    
    </summary>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/categories/Model-Compression/"/>
    
      <category term="Pruning" scheme="https://leijiezhang001.github.io/categories/Model-Compression/Pruning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/tags/Model-Compression/"/>
    
      <category term="Pruning" scheme="https://leijiezhang001.github.io/tags/Pruning/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;OccuSeg&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-OccuSeg/"/>
    <id>https://leijiezhang001.github.io/paper-reading-OccuSeg/</id>
    <published>2020-10-16T09:36:29.000Z</published>
    <updated>2020-10-20T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　之前介绍了 <a href="/paper-reading-JSNet-JSIS3D/" title="JSNet, JSIS3D">JSNet, JSIS3D</a>，<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a> 等 Instance 分割方法，为了点云聚类成 Instance，网络输出基本分为每个点距离目标中心的坐标残差以及每个点的 Embedding 特征两种。对于目标中心的坐标残差，之后可以直接在几何空间内作基于距离的聚类；对于每个点的 Embedding 特征，由于训练时要求同一个 Instance 内的 Embedding 相近，不同的距离要远，所以也是通过高维空间的距离计算来作聚类。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>大致也是这个思路，此外还预测了每个 voxel 的 Occupancy Regression，表示对应 Instance 包含的 Voxel 数目。最后采用基于图的聚类方法得到 Instance。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-OccuSeg/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，网络输入为 RGBD 点云数据，通过 3D UNet Backbone 网络，输出基于 Voxel 的三种预测结果：</p><ul><li><strong>Semantic Segmentation</strong><br>语义分割结果 \(\mathbf{c} _ i\)。</li><li><strong>Spatial Embedding and Feature Embedding</strong><br>基于坐标系的残差预测 \(\mathbf{d} _ i\)，和基于特征空间的 Embedding 预测 \(\mathbf{s} _ i\)，以及对应的方差 \(\mathbf{b} _ i=(\sigma _ d ^ i,\sigma _ s ^ i)\)。</li><li><strong>Occupancy Regression</strong><br>预测该 Voxel 对应 Instance 所含有的 Voxel 数量 \(\mathbf{o} _ i\)。</li></ul><p>Loss 项目构成为： <span class="math display">\[\mathcal{L} _ {joint} = \mathcal{L} _ {c} + \mathcal{L} _ {e} + \mathcal{L} _ {o} \tag{1}\]</span></p><h3 id="spatial-and-feature-embedding">1.1. Spatial and Feature Embedding</h3><p>　　Embedding 的 Loss 项构成为： <span class="math display">\[\mathcal{L} _ e = \mathcal{L} _ {sp} + \mathcal{L} _ {se} + \mathcal{L} _ {cov} \tag{2}\]</span> 其中 Spatial Embedding 目的是回归每个 voxel 与目标中心点的残差： <span class="math display">\[\mathcal{sp}=\frac{1}{C}\sum _ {c=1} ^ C\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\Vert \mathbf{d} _ i+\mu _ i-\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\mu _ i\Vert \tag{3}\]</span> \(C\) 表示 Instance 数量，\(N _ c\) 表示第 \(c\) 个 Instance 包含的 Voxel 数量，\(\mu _ i\) 表示第 \(i\) 个 voxel 的坐标。<br>　　Feature Embedding 目的是相同 Instance 的 voxel 预测相似的特征，不同的则预测不同的特征，通过 Metric Learning 实现： <span class="math display">\[\begin{align}\mathcal{L} _ {se} &amp;=\mathcal{L} _ {var}+\mathcal{L} _ {dist} +\mathcal{L} _ {reg}\\&amp;= \frac{1}{C}\sum _ {c=1} ^ C\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\left[\Vert\mathbf{u} _ c-\mathbf{s} _ i\Vert -\delta _ v\right] _ + ^ 2 + \frac{1}{C(C-1)}\mathop{\sum _ {i=1} ^ C\sum _ {j=1} ^ C} \limits _ {i\neq j}\left[2\delta _ d-\Vert\mathbf{u} _ i-\mathbf{u} _ j\Vert \right] _ + ^ 2 + \frac{1}{C}\sum _ {c=1} ^ C\Vert\mathbf{u} _ c\Vert\tag{4}\end{align}\]</span> 其中 \(\mathbf{u} _ c=\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c} \mathbf{s} _ i\) 表示第 \(c\) 个 Instance 的平均 Embedding 特征。以上和 <a href="/paper-reading-JSNet-JSIS3D/" title="JSNet, JSIS3D">JSNet, JSIS3D</a> 基本一致。<br>　　此外本文还预测了 Covariance 项。设预测的 Feature 和 Spatio Covariance 为 \(\mathbf{b} _ i=(\sigma _ s ^ i, \sigma _ d ^ i)\)，对 Instance 内的 voxel covariance 融合可得到该 Instance 的 Covariance \((\sigma _ s ^ c,\sigma _ d ^ c)\) (<strong>这里需要注意的是，Inference 的时候，即作基于图分割算法的聚类时候，见1,3，是只需要作 super-voxel 内的 Covariance 融合；而训练的时候，是由 Instance 标签的，所以能通过 Instance 的 Covariance 融合，以重构 \(p _ i\)</strong>)。由此可得到第 \(i\) 个 voxel 属于第 \(c\) 个 Instance 的概率： <span class="math display">\[p _ i = \mathrm{exp}\left(-\left(\frac{\Vert\mathbf{s} _ i-\mathbf{u} _ c\Vert}{\sigma _ s ^ c}\right)^2-\left(\frac{\Vert \mu _ i+\mathbf{d} _ i-\mathbf{e} _ c\Vert}{\sigma _ d ^ c}\right)^2\right)\tag{5}\]</span> 其中 \(\mathbf{e} _ c=\frac{1}{N _ c}\sum _ {k=1} ^ {N _ c}(\mu _ k+\mathbf{d} _ k)\) 表示预测的目标中心点。当 \(p _ i\) 大于 0.5 时，就表示该 voxel 属于该 Instance，所以用 binary cross-entropy loss： <span class="math display">\[\mathcal{L} _ {cov} = -\frac{1}{C}\sum _ {c=1}^C\frac{1}{N}\sum _ {i=1} ^ N[y _ i\mathrm{log}(p _ i)+(1-y _ i)\mathrm{log}(1-p _ i)]\tag{6}\]</span> 其中 \(y _ i\) 为标签，1 表示该 voxel 属于该 Instance，0 表示不属于。</p><h3 id="occupancy-regression">1.2. Occupancy Regression</h3><p>　　每个 Voxel 预测其对应的 Instance 包含的 Voxel 数目 \(o _ i\)，为了预测的鲁棒性，设计为回归其 log 值： <span class="math display">\[\mathcal{L} _ o = \frac{1}{C}\sum _ {c=1} ^ C\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\Vert o _ i-\mathrm{log}(N _ c)\Vert \tag{7}\]</span> 其中 \(N _ c\) 是第 \(c\) 个 Instance 包含的 Voxel 数量。</p><h3 id="instance-clustering">1.3. Instance Clustering</h3><p>　　基于每个 Voxel 的预测：Semantic Label，Spatial and Feature Embedding，Occupancy Regression，本文采用自底向上的图分割算法。<br>　　设 \(\Omega _ i\) 为上层 super-voxel \(v _ i\) 包含的 Voxel 数量。\(v _ i\) 对应的 Spatial Embedding，Feature Embedding，Occupancy Regression 为： <span class="math display">\[\left\{\begin{array}{l}\mathbf{D} _ i =\frac{1}{\vert\Omega _ i\vert}\sum _ {k\in\Omega _ i}(\mathbf{d} _ k+\mu _ k)\\\mathbf{S} _ i =\frac{1}{\vert\Omega _ i\vert}\sum _ {k\in\Omega _ i}(\mathbf{s} _ k)\\\mathbf{O} _ i =\frac{1}{\vert\Omega _ i\vert}\sum _ {k\in\Omega _ i}(\mathrm{exp}(\mathbf{o} _ k))\\\end{array}\tag{8}\right.\]</span> 为了指导聚类过程，定义(文章中应该是写反了)： <span class="math display">\[r _ i=\frac{\vert\Omega _ i\vert}{O _ i} \tag{9}\]</span> \(r _ i&gt;1\) 表示该 Instance 聚类的 Voxel 太多了，即欠分割；反之即为过分割。<br>　　图分割算法定义图 \(G=(V,E,W)\)，其中 \(v _ i\in V\) 表示 super-voxel，\(e _ {i,j}\) 表示 \((v _ i, v _ j)\in E\) 通过全概率权重 \(w _ {i,j}\in W\) 连接。\(w _ {i,j}\) 可定义为两个 super-voxel 的相似度： <span class="math display">\[w _ {i,j}=\frac{\mathrm{exp}\left(-\left(\frac{\Vert\mathbf{S} _ i-\mathbf{S} _ j\Vert}{\sigma _ s}\right) ^ 2-\left(\frac{\Vert \mathbf{D} _ i-\mathbf{D} _ j\Vert}{\sigma _ d}\right) ^ 2\right)}{\mathrm{max}(x,0.5)} \tag{10}\]</span> 自底向上迭代聚类通过 \(w _ {i,j} &gt; T _ 0\) 实现，\(T _ 0\) 设定为 0.5，最后保留 \(0.3 &lt; r&lt;2\) 的 Instance，以减少 FP。</p><h2 id="reference">2. Reference</h2><p><a id="1" href="#1ref">[1]</a> Han, Lei , et al. "OccuSeg: Occupancy-aware 3D Instance Segmentation." (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　之前介绍了 &lt;a href=&quot;/paper-reading-JSNet-JSIS3D/&quot; title=&quot;JSNet, JSIS3D&quot;&gt;JSNet, JSIS3D&lt;/a&gt;，&lt;a href=&quot;/paper-reading-PointGroup/&quot; title=&quot;PointG
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/Instance-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/tags/Instance-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;P2B&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-P2B/"/>
    <id>https://leijiezhang001.github.io/paper-reading-P2B/</id>
    <published>2020-10-14T01:42:39.000Z</published>
    <updated>2020-10-16T09:29:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　3D 目标状态估计中，对目标的跟踪测量非常重要，所谓跟踪测量，指的是给定前后目标框或者目标点云，计算目标的 R,t 的过程，由此可得到目标位置及速度的观测。<strong>一般的测量量有：目标框中心点距离，目标点云重心距离，目标点云 ICP 结果</strong>。<strong>在目标点云观测较为完备的情况下，理论上类 ICP 方法的结果是最为准确的，但是在点云较少的情况下，通过基于深度学习脑补预测出来的目标框中心点可能会更靠谱</strong>。ICP 一个比较大的问题是速度较慢，<a href="/ADH-Tracker/" title="ADH-Tracker">ADH-Tracker</a> 中的测量量本质上类似 ICP，但是其通过退火算法将 T 的搜索空间进行压缩，获得了极大的效率提升，但是对旋转量的估计还是比较棘手。<br>　　将跟踪测量问题往外扩，就是图像领域常说的单目标跟踪：给定上一帧目标位置，找到当前帧目标的位置。点云场景中，一般不会作目标的单目标跟踪，而是直接采用目标运动模型预测当前帧位置以及目标检测当前帧位置，所以目标的定位精度基本完全靠目标检测结果以及卡尔曼平滑结果。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出一种基于深度学习的点云单目标跟踪方法。类似图像中的单目标跟踪，输入为上一帧目标或目标的模型，以及当前帧搜索区域，输出为当前帧目标的位置。进一步讲，<strong>套用目标框中心，点云重心以及类 ICP 计算后，该模块结果可作为另一种更准确（相比直接采用检测框）的跟踪测量量</strong>。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-P2B/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，P2B 首先将目标(目标模型)特征与搜索区域的特征作融合，用来预测潜在的目标中心点，然后作端到端的目标 proposal 以及 verification。其网络主要由两部分组成：1. Target-specific feature augmentation; 2. 3D target proposal and verification。</p><h3 id="target-specific-feature-augmentation">1.1. Target-specific feature augmentation</h3><p>　　目标点云为 \(P _ {tmp}\in\mathbb{R} ^ {N _ 1\times 3}\)，搜索区域的点云为 \(P _ {sea}\in\mathbb{R} ^ {N _ 2\times 3}\)。经过 PointNet++ 采样及提取特征后，得到目标点云的种子点集 \(Q=\{q _ i\} _ {i=1} ^ {M _ 1}\)，搜索区域的种子点集 \(R=\{r _ j\} _ {j=1} ^ {M _ 2}\)，每个点特征向量为 \([x;f]\in\mathbb{R} ^ {3+d _ 1}\)。计算 \(Q,R\) 的相似度： <span class="math display">\[Sim _ {j,i} =\frac{f _ {q _ i} ^ T\cdot f _ {r _ j}}{\Vert f _ {q _ i}\Vert _ 2\cdot\Vert f _ {r _ j} \Vert _ 2},\;\forall q _ i\in Q,\;r _ j\in R \tag{1}\]</span> <img src="/paper-reading-P2B/feat.png" width="90%" height="90%" title="图 2. Feature Aggregation"> 　　得到的相似性特征图 \(Sim\) 与目标种子点集 \(Q\) 的顺序有关，如图 2. 所示，Target-Specific Feature Augmentation 模块目的就是消除 \(Q\) 顺序的影响。其基本思想也是通过对称操作将 \(Q\) 所在的 \(M _ 1\) 维度进行压缩。具体的，将 \(Sim \in\mathbb{R} ^ {M _ 2\times M _ 1}\) 变换为 \(M _ 2\times (3+d _ 2)\) 维度的特征。图 2 采用了 \(Q\) 特征，也可以采用其它方式，实验表明这种方式最好。</p><h3 id="d-target-proposal-and-verification">1.2. 3D target proposal and verification</h3><p>　　有了搜索区域每个种子点的特征后，可以基于此作候选目标框的预测。接下来分为两个分支，一个分支输出 \(M _ 2\times 1\)，表示每个种子点作为目标中心的概率分数；另一个分支采用 VoteNet<a href="#2" id="2ref"><sup>[2]</sup></a> ，输出相同维度及尺寸的特征，表示种子点与中心点的<strong>坐标及特征残差</strong>。VoteNet 中种子点的坐标残差用目标中心点与该种子点的距离来监督，而特征残差没有监督 Loss。这与 Instance-Seg 里面的套路非常相似，特征残差其实可以加上类内 Pull Loss 项，详见 <a href="/paper-reading-JSNet-JSIS3D/" title="JSNet-JSIS3D">JSNet-JSIS3D</a>。<br>　　对每个种子点作残差补偿后，类似 Instance-Seg 中的套路，可采用 Ball-Query 作目标的点集聚类，然后对聚类后的点集归一化并用 PointNet 即可预测目标的分数以及目标的 3D 属性，这里不作展开，详见<a href="#2" id="2ref">[2]</a>。<br>　　Loss 项由残差回归，是否是 proposal 的概率，proposal 质量分数及目标 3D 属性回归等组成： <span class="math display">\[L = L _ {reg} +\gamma _ 1L _ {cla} + \gamma _ 2L _ {prop} + \gamma _ 3L _ {box} \tag{2}\]</span> 其中 \(\gamma\) 分别设计为 \(0.2, 1.5, 0.2\)。</p><h2 id="workflow">2. Workflow</h2><p><img src="/paper-reading-P2B/workflow.png" width="60%" height="60%" title="图 3. Workflow"> 　　P2B 的整个算法流程如图 3. 所示，需要注意的是，最后得到的 \(K\) 个 Proposal 只需要取分数最高的即可。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Qi, Haozhe, et al. "P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br><a id="2" href="#2ref">[2]</a> Qi, Charles R., et al. "Deep hough voting for 3d object detection in point clouds." Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　3D 目标状态估计中，对目标的跟踪测量非常重要，所谓跟踪测量，指的是给定前后目标框或者目标点云，计算目标的 R,t 的过程，由此可得到目标位置及速度的观测。&lt;strong&gt;一般的测量量有：目标框中心点距离，目标点云重心距离，目标点云 ICP 结果&lt;/strong&gt;。&lt;s
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="Tracking" scheme="https://leijiezhang001.github.io/tags/Tracking/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;JSNet, JSIS3D&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-JSNet-JSIS3D/"/>
    <id>https://leijiezhang001.github.io/paper-reading-JSNet-JSIS3D/</id>
    <published>2020-10-10T09:29:45.000Z</published>
    <updated>2020-10-15T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a> 通过预测每个点与对应 instance 重心的 offset，然后在三维物理坐标系下作 instance 聚类。<a href="#1" id="1ref">[1]</a> 也是这种方案。另一种思路，是通过 Metric Learning 技术，预测每个点的高维特征(Embedding Space)，然后作 instance-level 聚类。本文介绍的 JSNet<a href="#2" id="2ref"><sup>[2]</sup></a> 以及 JSIS3D<a href="#3" id="3ref"><sup>[3]</sup></a> 就是采用的这种方式。</p><h2 id="jsnet">1. JSNet</h2><p><img src="/paper-reading-JSNet-JSIS3D/JSNet.png" width="90%" height="90%" title="图 1. JSNet"> 　　如图 1. 所示，整个网络共享的 Backbone 只有点云特征的 Encode 阶段，两个分支分别作 Decode 并通过 PCFF 模块，最终输出用于 Instance-Seg 的特征 \(F _ {IS}\in\mathbb{R} ^ {N _ a\times 128}\)，以及用于 Semantic-Seg 的特征 \(F _ {SS}\in\mathbb{R} ^ {N _ a\times 128}\)。这一阶段完全可以用其它 Voxel 或 Point 网络代替。然后通过 JISS 模块进行两个分支的特征融合，最终输出点云类别，以及用于点云 Instance 聚类的特征 Embedding。最后采用 Mean-Shift 聚类方法即可根据 Embedding 作 Instance 聚类。</p><h3 id="pcff">1.1. PCFF</h3><p>　　PCFF 类似图像 2D 卷积中上采样特征融合模块，如图 1.a 所示，目的是为了融合不同尺度的点云特征。PCFF 及之前的网络均可用其它点云特征网络代替。</p><h3 id="jiss">1.2. JISS</h3><p>　　JISS 模块目的是将 Instance-Seg 和 Semantic-Seg 两个任务的特征作充分的融合。Semantic-Seg 一般比 Instance-Seg 更底层，所以相同深度的网络，理论上能学到更加抽象(高层)的特征，所以如图 1.c 所示，先将 \(F _ {SS}\) 特征融入 \(F _ {IS}\) 特征中，然后在 Instance-Seg 分支作进一步特征提取后，再将特征返回来与 \(F _ {SS}\) 特征作融合。此外，每个分支还引入了 Self-Attention 模块，通过 Sigmoid 操作实现。<br>　　最终输出的是每个点的类别分数 \(P _ {SSI}\in\mathbb{R} ^ {N _ a\times C}\)，以及用于 Instance 聚类的点云特征 \(E _ {ISS}\in\mathbb{R} ^ {N _ a\times K}\)。</p><h3 id="loss">1.3. Loss</h3><p>　　Loss 由 Semantic-Seg 以及 Instance-Seg 两个任务组成： <span class="math display">\[\mathcal{L}=\mathcal{L} _ {sem}+\mathcal{L} _ {ins}\tag{1}\]</span> 其中语义分割的 Loss 项 \(\mathcal{L} _ {sem}\) 为传统的分类 Loss。\(\mathcal{L} _ {ins}\) 则要求能区分不同 Instance 的点云 Embedding 特征，但是又要保证同一 Instance 的点云 Embedding 特征的相似性，设计为： <span class="math display">\[\begin{align}\mathcal{L} _ {ins} &amp;=\mathcal{L} _ {pull}+\mathcal{L} _ {push}\\&amp;= \frac{1}{M}\sum _ {m=1} ^ M\frac{1}{N _ m}\sum _ {n=1} ^ {N _ m}\left[\Vert\mu _ m-e _ n\Vert _ 1-\delta _ v\right] _ + ^ 2 + \frac{1}{M(M-1)}\mathop{\sum _ {i=1} ^ M\sum _ {j=1} ^ M} \limits _ {i\neq j}\left[2\delta _ d-\Vert\mu _ i-\mu _ j\Vert _ 1\right] _ + ^ 2\tag{2}\end{align}\]</span> 其中 \([x] _ +=\mathrm{max}(0,x)\)，\(|| \cdot || _ 1\) 为 L1 距离，\(\delta _ v,\delta _ d\) 分别为 \(\mathcal{L} _ {pull},\mathcal{L} _ {push}\) 的幅度。</p><h2 id="jsis3d">2. JSIS3D</h2><p><img src="/paper-reading-JSNet-JSIS3D/JSIS.png" width="90%" height="90%" title="图 2. JSIS"> 　　如图 2. 所示，JSIS3D 由 MT-PNet 网络和 MV-CRF 构成。MV-CRF 是基于 MT-PNet 网络预测的 Semantic Label 和 Embeddings 作基于条件随机场的 instance 聚类，效果比直接对 Embeddings 作聚类要好，这里只讨论 MT-PNet 网络。</p><h3 id="mt-pnet">2.1. MT-PNet</h3><p><img src="/paper-reading-JSNet-JSIS3D/MT-PNet.png" width="90%" height="90%" title="图 3. MT-PNet"> 　　如图 3. 所示，网络由基本的 PointNet 构成，最终预测的也是每个点的类别以及用于聚类的 Embedding。所以输出方案是与 JSNet 是一样的。Loss 项中的 Embedding(ins) 预测项加入了正则化： <span class="math display">\[\begin{align}\mathcal{L} _ {ins} &amp;=\alpha\mathcal{L} _ {pull}+\beta\mathcal{L} _ {push}+\gamma\mathcal{L} _ {reg}\\&amp;= \frac{\alpha}{M}\sum _ {m=1} ^ M\frac{1}{N _ m}\sum _ {n=1} ^ {N _ m}\left[\Vert\mu _ m-e _ n\Vert _ 2-\delta _ v\right] _ + ^ 2 + \frac{\beta}{M(M-1)}\mathop{\sum _ {i=1} ^ M\sum _ {j=1} ^ M} \limits _ {i\neq j}\left[2\delta _ d-\Vert\mu _ i-\mu _ j\Vert _ 2\right] _ + ^ 2 + \frac{\gamma}{M}\sum _ {m=1} ^ M \Vert \mu _ m\Vert _ 2\tag{3}\end{align}\]</span> 其中 \(M\) 为 instance 数量，\(N _ m\) 为对应 instance 内点的个数，\(e _ n\) 为点的 Embedding，\(\mu _ m\) 表示第 \(m\) 个 instance 内点的平均 Embedding。设计 \(\sigma _ d &gt; 2\sigma _ v,\alpha=\beta=1,\gamma=0.001\)，可以实现同一个 instance 内点的 Embedding 相近，不同 instance 的平均 Embedding 距离较远，并且正则化使得平均 Embedding 接近 0。</p><h3 id="experiments">2.2. Experiments</h3><p><img src="/paper-reading-JSNet-JSIS3D/res.png" width="90%" height="90%" title="图 4. Mean-Shift VS. MV-CRF"> 　　如图所示，用 MV-CRF 代替 Means-Shift 聚类，对于大物体，提升效果比较明显，但是小物体，精度会下降。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> F. Zhang, C. Guan, J. Fang, S. Bai, R. Yang, P. Torr, and V. Prisacariu, “Instance segmentation of lidar point clouds,” in ICRA, 2020<br><a id="2" href="#2ref">[2]</a> L. Zhao and W. Tao, “JSNet: Joint instance and semantic segmentation of 3D point clouds,” in AAAI, 2020.<br><a id="3" href="#3ref">[3]</a> Pham, Quang Hieu , et al. "JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds With Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields." 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) IEEE, 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/paper-reading-PointGroup/&quot; title=&quot;PointGroup&quot;&gt;PointGroup&lt;/a&gt; 通过预测每个点与对应 instance 重心的 offset，然后在三维物理坐标系下作 instance 聚类。&lt;a href=
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/Instance-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/tags/Instance-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PointGroup&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PointGroup/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PointGroup/</id>
    <published>2020-09-28T02:02:56.000Z</published>
    <updated>2020-09-29T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　之前一直提到，以 Semantic Segmentation 为基础作目标检测，可以有较高的召回率，而在最终出目标框或目标 Polygon 之前，还需要作 Instance Segmentation。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种 Instance Segmentation 方法。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-PointGroup/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，整个网络由三部分构成：Backbone，Clustering Part，ScoreNet。Backbone 我们已经很熟悉了，输入为点云以及点云的其它属性比如 rgb 信息，输出为每个点提取的局部-全局特征 \(\mathbf{F} = \{F _ i\}\in\mathbb{R} ^ {N\times K}\)，这里不作展开。然后用提取的特征 \(\mathbf{F}\) 通过两个分支分别作 Semantic Segmentation 以及预测每个点与该点对应的目标重心点的 Offset，得到每个点的类别 \(s _ i\) 以及 \(o _ i=(\Delta x _ i,\Delta y _ i,\Delta z _ i)\)。然后经过 Clustering Part 作 Instance 聚类。最后用 ScoreNet 预测 Instance 的分数，用于 NMS 去除重合的 Instance。</p><h2 id="backbone">2. Backbone</h2><p>　　对于 Semantic Segmentation Branch，在 \(\mathbf{F}\) 之后加入 MLP 网络输出语义类别分数 \(\mathbf{SC}=\{sc _ 1,...,sc _ N\}\in\mathbb{R}^{N\times N _ {class}}\)。最终的类别 \(s _ i = \mathrm{argmax}(sc _ i)\)。<br>　　对于 Offset Prediction Branch，输出 \(N\) 个点的 \(\mathbf{O}=\{o _ 1,...,o _ N\}\in\mathbb{R} ^ {N\times 3}\)。采用 L1 Loss： <span class="math display">\[ L _ {o_reg} = \frac{1}{\sum _ i m _ i}\sum _ i\Vert o _ i-(\hat{c} _ i-p _ i)\Vert\cdot m _ i\tag{1}\]</span> 其中 \(\mathbf{m} = \{m _ i,...,m _ N\}\) 是一个二进制 mask，\(m _ i=1\) 表示第 \(i\) 个点属于一个 Instance。\(\hat{c} _ i\) 为 Instance 的重心： <span class="math display">\[\hat{c} _ i=\frac{1}{N _ {g(i)} ^ I}\sum _ {j\in I _ {g(i)}}p _ j\tag{2}\]</span> 其中 \(N _ {g(i)} ^ I \) 表示 Instance \(I _ {g(i)}\) 中点的个数。此外，考虑到尺寸大的目标其边缘点的 offset 较难回归，所以加入方向约束的 loss： <span class="math display">\[L _ {o _ dir}=-\frac{1}{\sum _ i m _ i}\sum _ i\frac{o _ i}{\Vert o _ i\Vert _ 2}\cdot\frac{\hat{c} _ i-p _ i}{\Vert \hat{c} _ i-p _ i\Vert _ 2}\cdot m _ i\tag{3}\]</span></p><h2 id="clustering-part">3. Clustering Part</h2><p>　　有了点云的语义标签以及每个点相对目标物体重心的 offset 后，接下来将点云聚类成对应的 Instance。设点云原始坐标为 \(\mathbf{P}=\{p _ i\}\)，经过 offset 变换后坐标为 \(\mathbf{Q}=\{q _ i = p _ i+o _ i\in\mathbb{R} ^ 3\}\)。根据 \(\mathbf{Q}\) 来作聚类，能更容易的区分相邻的同类别的物体；但是对于目标的边缘点，offset 容易预测错误，所以再加上根据 \(\mathbf{P}\) 来作聚类。最终获得的聚类 Instance 为 \(\mathbf{C}=\mathbf{C} ^ p\cup\mathbf{C} ^ q=\{C _ 1 ^ p,...,C _ {M _ p}^p\}\cup\{C _ 1 ^ q,...,C _ {M _ q} ^ q\}\)。 <img src="/paper-reading-PointGroup/cluster.png" width="50%" height="50%" title="图 2. Clustering"> 　　如图 2. 所示，聚类算法就是一个基于点集的 BFS 搜索，这里需要设定 ball query 的半径 \(r\)。</p><h2 id="scorenet">4. ScoreNet</h2><p>　　经过基于坐标 \(\mathbf{P},\mathbf{Q}\) 聚类后，总共得到 \(\mathbf{C} = \{C _ 1,...,C _ M\}\)。因为这里面会有重叠的 Instance，所以 ScoreNet 用来评价这些 Instance 的质量，然后作 NMS 操作，从而达到综合两者聚类优势的效果。 <img src="/paper-reading-PointGroup/score.png" width="90%" height="90%" title="图 3. ScoreNet"> 　　如图 3. 所示，对于每个 Cluster，将其点特征加上点坐标作为点特征输入到网络。然后采用 Backbone 相似的结构，最终得到 Clustering 分数：\(\mathbf{S} _ c=\{s _ 1 ^ c,...,s _ M ^ c\}\)。<br>　　对于评价 cluster 质量的标签，可以直接用 0/1，但是本文使用了 soft 形式： <span class="math display">\[\hat{s} _ i ^ c=\left\{\begin{array}{l}0  &amp;\;iou _ i &lt; \theta _ l\\1  &amp;\;iou _ i &lt; \theta _ h\\\frac{1}{\theta _ h-\theta _ t}\cdot (iou _ i - \theta _ l) &amp;\;otherwise\end{array}\tag{4}\right.\]</span> 其中 \(\theta _ l,\theta _ h \) 分别设为 0.25，0.75。然后用 binary cross-entropy 作为 Loss： <span class="math display">\[L _ {c_score} = -\frac{1}{M}\sum _ {i=1} ^ M\left(\hat{s} _ i ^ clog(s _ i ^ c)+(1-\hat{s} _ i ^ c)log(1-s _ i ^ c)\right)\tag{5}\]</span></p><h2 id="experiments">5. Experiments</h2><p><img src="/paper-reading-PointGroup/ablation.png" width="90%" height="90%" title="图 4. Ablation"> 　　如图 4. 所示，用 \(\mathbf{P,Q}\) 作聚类，效果提升还是比较明显的，能同时综合二者的聚类优势。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Li.Jiang, PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　之前一直提到，以 Semantic Segmentation 为基础作目标检测，可以有较高的召回率，而在最终出目标框或目标 Polygon 之前，还需要作 Instance Segmentation。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/
      
    
    </summary>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/categories/Segmentation/Instance-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
      <category term="Instance Segmentation" scheme="https://leijiezhang001.github.io/tags/Instance-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PnPNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PnPNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PnPNet/</id>
    <published>2020-09-11T01:35:32.000Z</published>
    <updated>2020-09-16T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　自动驾驶的障碍物状态估计功能模块中，包含 perception/Detection，tracking，prediction 三个环节。传统的做法这三个环节是分步进行的，Detection 出目标框检测结果；Tracking 则作前后帧目标的数据关联然后用卡尔曼平滑并估计目标状态；Prediction 预测目标未来的运动轨迹。 <img src="/paper-reading-PnPNet/diff-pipe.png" width="60%" height="60%" title="图 1. Perception and Prediction"> 　　如图 1. 所示，(a) 代表传统的做法，每个步骤都是独立优化并出结果，这种方式将功能模块解耦，容易找到具体问题的位置，但是会降低算法找到最优解的概率；(b) 则将 Detection 与 Prediction 用同一个网络预测，然后用 Tracking 来平滑估计整个运动轨迹(代表方法是 <a href="/paperreading-Fast-and-Furious/" title="Fast and Furious">Fast and Furious</a>)，这种方法下 Tracking 中丰富的时序及空域特征信息没有作用于 Detection 和 Prediction；本文提出的 PnP<a href="#1" id="1ref"><sup>[1]</sup></a>方法则将三个环节作深度的特征再利用，即整个功能模块是 End-to-End 可训练的，更容易得到目标状态及预测的全局最优解，更容易处理遮挡等问题。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-PnPNet/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，PnP 网络包含 Detection、Tracking，Motion Forecasting 三个模块。网络输入为点云及 HD Map。检测模块包含一个任意的 3D 目标检测网络，以及一个存储历史 BEV 特征图的 Memory；Tracking 跟踪模块包含一个存储目标历史轨迹的 Memory，首先作 Track-Detection 的数据关联，然后优化目标历史轨迹并更新存储；Motion Forecasting 模块则根据历史轨迹作目标的运动预测。</p><h2 id="object-detection">2. Object Detection</h2><p>　　网络的输入为序列点云(本文采用 0.5s)及 HD Map，分别将点云在俯视图下体素化后在特征通道维度进行串联得到 \(\mathbf{x} ^ t\)，然后输入 Backbone 网络得到俯视图下特征图： <span class="math display">\[\mathcal{F} ^ t _ {bev}(\mathbf{x} ^ t) = \mathrm{CNN} _ {bev}(\mathbf{x} ^ t) \tag{1}\]</span> 最后加入 3D 目标检测头，得到 3D 目标框属性 \((u _ i ^ t, v _ i ^ t,w _ i,l _ i,\theta _ i ^ t)\) 的预测： <span class="math display">\[\mathcal{D} ^ t=\mathrm{CNN} _ {det}(\mathcal{F} ^ t _ {bev})\tag{2}\]</span></p><h2 id="discrete-continuous-tracking">3. Discrete-Continuous Tracking</h2><p>　　<strong>Tracking 模块包括离散的数据关联问题，以及连续的目标运动轨迹(状态)估计问题。</strong>目标运动轨迹的优化估计对之后的目标运动预测非常重要。</p><h3 id="trajectory-level-object-representation">3.1. Trajectory Level Object Representation</h3><p><img src="/paper-reading-PnPNet/trajectory.png" width="90%" height="90%" title="图 3. Trajectory Level Object Representation"> 　　Tracking 需要优化历史轨迹，Prediction 需要预测未来轨迹，所以轨迹级别的目标特征提取及表达非常重要。本文采用 LSTM 网络来表征。如图 3. 所示，对于轨迹 \(\mathcal{P} _ i ^ t=\mathcal{D} _ i ^ {t _ 0...t}\)，首先提取每个时刻目标的感知特征： <span class="math display">\[f _ i^{bev,t} = \mathrm{BilinearInterp}(\mathcal{F} _ {bev} ^ t,(u _ i ^ t, v _ i ^ t)) \tag{3}\]</span> 然后提取目标运动特征： <span class="math display">\[f _ i ^ {velocity,t}=(\dot{x} _ i ^ t,\dot{x} _ {ego} ^ t, \dot{\theta} _ {ego} ^ t)\tag{4}\]</span> 其中 \(\dot{x} _ i,\dot{x} _ {ego}\) 分别是第 \(i\) 个目标及本车的二维速度，通过位置差计算得到，对于新目标，将其设定为 0。由此得到第 \(i\) 个目标的特征： <span class="math display">\[f(\mathcal{D} _ i ^ t)=\mathrm{MLP} _ {merge}\left(f _ i^{bev,t},f _ i ^ {velocity,t}\right)\tag{5}\]</span> 最后通过 LSTM 网络来提取轨迹级别目标特征： <span class="math display">\[h(\mathcal{P} _ i ^ t)=\mathrm{LSTM}(f(\mathcal{D} _ i ^ {t _ 0...t}))\tag{6}\]</span></p><h3 id="data-association">3.2. Data Association</h3><p>　　当前时刻检测的目标数量为 \(N _ t\)，上一时刻目标轨迹数量为 \(M _ {t-1}\)，将二者关联匹配就是数据关联问题。这在有新目标出现以及目标出现遮挡的时候变得较为困难。类似传统方法，这里设计检测与跟踪轨迹的相似性矩阵 \(C\in\mathbb{R} ^ {N _ t\times (M _ {t-1}+N _ t)}\)(跟踪轨迹加入\(N _ t\)个目标是为了处理新出现目标的情况)： <span class="math display">\[C _ {i,j}=\left\{\begin{array}{l}\mathrm{MLP} _ {pair}\left(f(\mathcal{D} _ i ^ t),h(\mathcal{P} _ j ^ {t-1})\right) &amp;\;\; \mathrm{if}\; 1\leq j\leq M _ {t-1},\\\mathrm{MLP} _ {unary}\left(f(\mathcal{D} _ i ^ t)\right) &amp;\;\; \mathrm{if}\; j=  M _ {t-1} + i,\\-\mathrm{inf} &amp;\;\; \mathrm{otherwise}\end{array}\tag{7}\right.\]</span> 其中 \(\mathrm{MLP} _ {pair}\) 计算检测与跟踪轨迹的相似性分数，\(\mathrm{MLP} _ {unary}\) 计算目标是新出现的概率。有了该相似性矩阵，即可通过匈牙利算法求解最佳匹配对。<br>　　对于被遮挡的物体，跟踪轨迹在当前帧容易出现没有检测的情况，本文引入单目标跟踪的思想作跟踪搜索。设未匹配的跟踪轨迹为 \(\mathcal{P} _ j ^ {t-1}\)，那么根据上一帧该轨迹目标的位置 \(u _ j ^ {t-1}, v _ j ^ {t-1}\)，进行运动补偿后为 \(\tilde{u} _ j ^ {t}, \tilde{v} _ j ^ {t}\)，在其邻域 \(\Omega _ j\) 内寻找最优的检测(跟踪)结果 \(\tilde{\mathcal{D}} _ k ^ t\)： <span class="math display">\[k = \mathop{\arg\max}\limits _ {i\in\Omega _ j} \mathrm{MLP} _ {pair}\left(f(\tilde{\mathcal{D}} _ i ^ t),h(\mathcal{P} _ j ^ {t-1})\right)\tag{8}\]</span> 其中 \(\Omega _ j\) 设计为目标的最大假设速度，如 \(110 km/h\)。<br>　　最终可得到 \(N _ t+ K _ t\) 个目标轨迹，其中 \(K _ t\) 为未匹配的目标轨迹而通过单目标跟踪方法召回的轨迹数量。</p><h3 id="trajectory-estimation">3.3. Trajectory Estimation</h3><p>　　当前帧的观测加入到目标轨迹后，可进一步对目标轨迹作优化以减少 FP 以及提高轨迹定位精度。网络预测轨迹的置信度以及最近 \(T _ 0\) 时间内目标位置的残差： <span class="math display">\[\mathrm{score} _ i,\Delta u _ i ^ {t-T _ 0+1:t},\Delta v _ i ^ {t- T _ 0+1:t}=\mathrm{MLP} _ {refine}(h(\mathcal{P} _ i^t))\tag{9}\]</span> 其中 \(T _ 0\) 小于轨迹的总时间。最后用 NMS 去掉重叠的目标轨迹以消除 FP 与重叠项。</p><h2 id="motion-forecasting">4. Motion Forecasting</h2><p>　　根据优化后的目标轨迹，通过网络预测目标的未来轨迹： <span class="math display">\[\Delta u _ i^{t:t+\Delta T}, \Delta v _ i ^ {t:t+\Delta T}=\mathrm{MLP} _ {predict}(h(\mathcal{P} _ i^t))\tag{10}\]</span></p><h2 id="end-to-end-learning">5. End-to-End Learning</h2><p>　　整个网络多任务联合训练的 Loss 为： <span class="math display">\[\begin{align}\mathcal{L} &amp;= \mathcal{L} _ {detect} + \mathcal{L} _ {track} + \mathcal{L} _ {predict}\\&amp;= \mathcal{L} _ {detect} + \mathcal{L} _ {score} ^ {affinity} + \mathcal{L} _ {score} ^ {sot} + \mathcal{L} _ {socre} ^ {refine} + \mathcal{L} _ {reg} ^ {refine} + \mathcal{L} _ {predict}\end{align}\tag{11}\]</span> 其中 \(\mathcal{L} _ {score}\) 为 \(max-margin \;loss\): <span class="math display">\[\mathcal{L} _ {score} = \frac{1}{N _ {i,j}}\sum _ {i\in pos,j\in neg} \mathrm{max}(0,m-(a _ i-a _ j))\tag{12}\]</span> 对于 \(\mathcal{L} _ {score} ^ {affinity}\) 和 \(\mathcal{L} _ {score} ^ {sot}\)，计算正样本与所有负样本的 Loss；对于 \(\mathcal{L} _ {score} ^ {refine}\)，与真值框 IoU 较高的，则 score 较高，这样作 NMS 时可以该 score 为准则。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Liang, Ming, et al. "PnPNet: End-to-End Perception and Prediction with Tracking in the Loop." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　自动驾驶的障碍物状态估计功能模块中，包含 perception/Detection，tracking，prediction 三个环节。传统的做法这三个环节是分步进行的，Detection 出目标框检测结果；Tracking 则作前后帧目标的数据关联然后用卡尔曼平滑并估计
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="Prediction" scheme="https://leijiezhang001.github.io/tags/Prediction/"/>
    
  </entry>
  
</feed>
