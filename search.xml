<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>如何搭建一个 ADAS 产品</title>
    <url>/How-to-Build-An-ADAS/</url>
    <content><![CDATA[<div id="hbe-security">
  <div class="hbe-input-container">
  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了基于视觉的 ADAS 产品构建方法，全文 6K 字 12 图，请输入密码查看：" />
    <label for="pass">本文介绍了基于视觉的 ADAS 产品构建方法，全文 6K 字 12 图，请输入密码查看：</label>
    <div class="bottom-line"></div>
  </div>
</div>
<div id="decryptionError" style="display: none;">Incorrect Password!</div>
<div id="noContentError" style="display: none;">No content to display!</div>
<div id="encrypt-blog" style="display:none">
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
</div>
<script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Autonomous Driving System</category>
        <category>ADAS</category>
      </categories>
      <tags>
        <tag>autonomous driving</tag>
        <tag>ADAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer Analysis</title>
    <url>/Transformer-Analysis/</url>
    <content><![CDATA[<p>　　基于 Attention 的 Transformer 已经从 NLP 领域逐渐要统治图像，点云领域，本文详述了其原理机制，以及在图像，点云中的一些应用。</p>
<h2 id="attention-to-transformer1">1. Attention to Transformer<a href="#1" id="1ref"><sup>[1]</sup></a></h2>
<h3 id="attention">1.1. Attention</h3>
<p><img src="/Transformer-Analysis/attention.png" width="80%" height="80%" title="图 1. Attention"></p>
<p>　　Attention 的机制在于能自动关注到输入序列中的重要部分。如图 1. 左图，设输入 query \(Q\in\mathbb{R} ^ {N\times d _ k}\)，key-value \(K\in\mathbb{R} ^ {N _ k\times d _ k}, V\in\mathbb{R} ^ {N _ k\times d _ v}\)，那么 Attention 映射方程 \(\mathcal{A}(\cdot)\) 将其映射为输出 \(\mathbb{R} ^ {N\times d _ k}\)。具体的： <span class="math display">\[\begin{align}
\mathcal{A}(Q,K,V) &amp;= \mathrm{score}(Q,K)V \\
&amp;= \mathrm{softmax}\left(\frac{QK ^ T}{\sqrt{d _ k}}\right)V
\end{align} \tag{1}\]</span> 当 \(d _ k\) 较大时，\(QK ^ T\) 的幅值会较大，导致 softmax 的反传梯度会趋于很小，为了降低这种影响，引入尺度变换 \(\frac{1}{\sqrt{d _ k}}\)。以上矩阵运算的维度变化为： <span class="math display">\[\begin{align}
\mathrm{score}(\cdot):&amp;\;\; \mathbb{R} ^ {N\times d _ q}, \mathbb{R} ^ {N _ k\times d _ k} \rightarrow \mathbb{R} ^ {N\times N _ k}，其中\; d _ k=d _ q=d _ m\\
\mathcal{A}(Q,K,V):&amp;\;\; \mathbb{R} ^ {N\times d _ k}, \mathbb{R} ^ {N _ k\times d _ k}, \mathbb{R} ^ {N _ k\times d _ v} \rightarrow \mathbb{R} ^ {N\times d _ k}
\end{align} \tag{2}\]</span> 　　如图 1. 右图，实践中，采用 MultiHead Attention 形式，在不同的子空间不同的位置编码中学习特征，然后再作整合： <span class="math display">\[ \mathrm{MultiHead(Q,K,V)} =\mathrm{Concat(head _ 1,..., head _ h)}W ^ o \tag{3}\]</span> 其中 \(\mathrm{head _ i} = \mathcal{A}(QW _ i ^ Q,KW _ i ^ K,VW _ i ^ V)\)，参数 \(W _ i ^ Q\in\mathbb{R} ^ {d _ m\times d _ k},W _ i ^ K\in\mathbb{R} ^ {d _ m\times d _ k},W _ i ^ V\in\mathbb{R} ^ {d _ m\times d _ v}\)；\(W ^ o\in\mathbb{R} ^ {hd _ v\times d _ m}\)，为了与 Attention 计算复杂度类似，令 \(d _ k=d _ v=d _ m/h\)。</p>
<h3 id="transformer">1.2. Transformer</h3>
<p><img src="/Transformer-Analysis/transformer.png" width="45%" height="45%" title="图 2. Transformer"></p>
<p>　　如图 2. 所示，Transformer 结构有 Encoder，Decoder 构成，基本层由 Multi-head Attention 和 point-wise Fully Connected Layer 构成，称之为 Multi-head Attention： <span class="math display">\[\begin{align}
\mathcal{A} ^ {MH}(X,Y)=\mathrm{LayerNorm}(S + \mathrm{rFF}(S)) \\
S = \mathrm{LayerNorm}(X+\mathrm{Multihead(X,Y,Y)})
\end{align} \tag{4}\]</span> 其中 \(\mathrm{rFF}\) 表示 row-wise fead-forward network。N 个该基础层叠加组成 Encoder 网络，其维度变换为： <span class="math display">\[\mathcal{A} ^ {MH}:\; \mathbb{R} ^ {N\times d _ m}, \mathbb{R} ^ {N _ k\times d _ m} \rightarrow \mathbb{R} ^ {N\times d _ m} \tag{5}\]</span></p>
<h3 id="positional-encoding">1.3. Positional Encoding</h3>
<p>　　由于 Transformer 对输入没有提取序列信息的能力，所以需要对输入作位置编码。位置编码可以通过学习得到，也可以用函数编码： <span class="math display">\[\begin{align}
PE _ {pos,2i} &amp;= sin(pos/10000 ^ {2i/d _ {model}}) \\
PE _ {pos,2i+1} &amp;= cos(pos/10000 ^ {2i/d _ {model}})
\end{align} \tag{6}\]</span></p>
<p>其中 pos 表示位置，i 表示特征维度。</p>
<h3 id="why-self-attention">1.4. Why Self-Attention</h3>
<p><img src="/Transformer-Analysis/complex.png" width="80%" height="80%" title="图 3. Complexity"></p>
<p>　　如图 3. 所示，Attention 相比 Recurrent，Convolution，其在计算复杂度，并行化，输入序列间的最大距离上有较好的优势。</p>
<h2 id="vision-transformer2">2. Vision Transformer<a href="#2" id="2ref"><sup>[2]</sup></a></h2>
<p><img src="/Transformer-Analysis/vision_transformer.png" width="80%" height="80%" title="图 4. Vision Transformer"> 　　如图 4. 所示，首先将图像 \(\mathbf{x}\in\mathbb{R} ^ {H\times W\times C}\) 变换成 patch 序列 \(\mathbf{x} _ p\in\mathbb{R} ^ {N\times (P ^ 2\cdot C)}\)，其中 \((P,P)\) 是 patch 尺寸，\(N=HW/P ^ 2\)。然后用线性变换 \(\mathbf{E}\in\mathbb{R} ^ {(P ^ 2\cdot C)\times D}\) 将每个 patch 的特征维度映射到 D 维。最终的 Transformer 数学形式为： <span class="math display">\[\begin{align}
\mathbf{z} _ 0 &amp;=[\mathbf{x} _ {class};\;\mathbf{x} _ p^1\mathbf{E};\;\mathbf{x} _ p^2\mathbf{E};\;\cdots;\;\mathbf{x} _ p^N\mathbf{E}]+\mathbf{E} _ {pos}, \;\;&amp; \mathbf{E}\in\mathbb{R} ^ {(P^2\cdot C)\times D},\;\mathbf{E} _ {pos}\in\mathbb{R} ^ {(N+1)\times D}\\
\mathbf{z} _ l&#39; &amp;= \mathrm{MultiHeadAttention(LN(}\mathbf{z} _ {l-1}))+\mathbf{z} _ {l-1}, \;\;&amp; l=1\cdots L\\
\mathbf{z} _ l &amp;= \mathrm{MLP(LN(}\mathbf{z})) _ l&#39;+\mathbf{z} _ l&#39;, \;\;&amp; l=1\cdots L\\
\mathbf{y} &amp;= \mathrm{LN}(\mathbf{z}) _ L ^0\\
\end{align} \tag{7}\]</span> 这里的关键是，在输入序列中串联了分类结果 \(\mathbf{z} _ 0 ^ 0= \mathbf{x} _ {class}\)，经过 \(L\) 次查询迭代后，获得最终的分类结果 \(\mathbf{z} _ L ^ 0\)。</p>
<h2 id="detr3">3. DETR<a href="#3" id="3ref"><sup>[3]</sup></a></h2>
<p><img src="/Transformer-Analysis/detr.png" width="80%" height="80%" title="图 5. DETR"></p>
<p><img src="/Transformer-Analysis/detr2.png" width="80%" height="80%" title="图 6. DETR"> 　　如图 5.6. 所示，DETR 首先用 CNN 网络作图像特征提取，然后用 Transformer Encoder 作特征序列化融合，接着用 Transformer Decoder 作目标框查询，得到检测结果。</p>
<p><img src="/Transformer-Analysis/detr_transformer.png" width="80%" height="80%" title="图 7. DETR Transformer"></p>
<h2 id="point-transformer4">4. Point Transformer<a href="#4" id="4ref"><sup>[4]</sup></a></h2>
<h2 id="group-free-3d-object-detection5">5. Group-Free 3D Object Detection<a href="#5" id="5ref"><sup>[5]</sup></a></h2>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Detection</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Offboard 3D Object Detection from Point Cloud Sequences</title>
    <url>/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/</url>
    <content><![CDATA[<p>　　目前点云检测等任务基本集中在在线实时情况下的研究，然而在离线场景下，自动化/半自动化标注/高精地图语义信息提取/数据闭环中的教师模型等，这些任务也相当重要。相比在线模式，离线模式对点云的实时处理要求较低，而且能更容易提取时序信息。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种基于时序的离线点云检测方法，能极大提高目标真值标注的自动化程度。其性能几乎达到了人工标注的水平。</p>
<h2 id="problem-statement-framework">1. Problem Statement &amp; Framework</h2>
<p>　　对于一个 \(N\) 帧序列点云 \(\{\mathcal{P} _ i\in\mathbf{R} ^ {n _ i\times C}\},i=1,2,...,N\)，已知每帧点云传感器在世界坐标系下的位姿 \(\{\mathcal{M} _ i=[R _ i|t _ i]\in\mathbf{R} ^ {3\times 4}\},i=1,2,...,N\)，那么我们要得到每帧点云中的目标 3D 属性(包括中心点，尺寸，朝向)，类别，ID。<br>
<img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　方法框架如图 1. 所示，输入序列点云，首先用目标检测器检测每一帧中的 3D 目标，然后用跟踪器将目标进行数据关联作 ID 标记。最后提取每个 ID 目标跟踪轨迹上的所有点云及目标框信息，作进一步的 3D 目标框精修预测。</p>
<h2 id="d-auto-labeling-pipeline">2. 3D Auto Labeling Pipeline</h2>
<h3 id="multi-frame-3d-object-detection">2.1. Multi-frame 3D Object Detection</h3>
<p>　　采用 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds">End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds</a> 中的 MVF 检测器，输入用多帧经过运动补偿的点云代替，每个点增加相对时间的偏移量。在测试阶段，采用 Test-time Augmentation，将点云绕 Z 轴进行不同角度的增广，最终的检测框进行权重融合。这在离线计算中可用不同计算单元并行化实现。 <img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/mvf.png" width="90%" height="90%" title="图 2. MVF++"> 　　MVF 基本思想可见 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds">End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds</a>。这里对 MVF++ 作更详细的描述。结构如图 2. 所示，网络输入的点云尺寸为 \(N\times C\)，经过 MLP 网络映射到高维特征，然后在 Birds Eye View 和 Perspective View 下体素化并作卷积提取特征的操作，最终融合得到点级别的三种特征类型。Loss 构成为： <span class="math display">\[L = L _ {cls} + w _ 1L _ {centerness} + w _ 2L _ {reg} + w _ 3L _ {seg}\tag{1}\]</span> 其中 \(L _ {seg}\) 是区分前景，背景的辅助分支。</p>
<h3 id="multi-object-tracking">2.2. Multi-object Tracking</h3>
<p>　　多目标跟踪方法采用<a href="#2" id="1ref">[2]</a>，在三维空间下作前后帧率检测跟踪的的数据关联，然后用卡尔曼作状态估计。</p>
<h3 id="object-track-data-extraction">2.3. Object Track Data Extraction</h3>
<p>　　经过多目标跟踪模块后，每个目标实例在时空内都作了 ID 标记。在世界坐标系下，可提取目标的 4D 时空信息，包括点云及 3D 属性框：第 \(j\) 个目标在其出现的帧 \(S _ j\) 下的点云 \(\{\mathcal{P} _ {j,k}\},k\in S _ j\)，以及对应的 3D 框 \(\{\mathcal{B} _ {j,k}\},k\in S _ j\)。</p>
<h3 id="object-centric-auto-labeling">2.4. Object-centric Auto Labeling</h3>
<p>　　有了目标检测，跟踪算法流程后。接下来将目标自动标注分为目标动静状态分析，静态目标自动标注以及动态目标自动标注。<br>
　　目标动静状态分析模块将每个目标实例提取 4D 特征，然后加入线性分类器来预测。特征包括目标框中心点的时空方差，以及目标从始至终的中心点偏移距离。真值标记时，将静态目标从始至终的距离阈值设定为 1.0m，最大速度不超过 1m/s。这种方式动静预测准确率高达 99%。此外，将行人都归为动态目标。</p>
<h4 id="static-object">2.4.1. Static Object</h4>
<p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/static.png" width="70%" height="70%" title="图 3. Static Object"> 　　如图 3. 所示，首先挑选 score 最高的目标框作为 initial box，将点云从世界坐标系转到该目标框坐标系。类似 Cascade-RCNN，作连续的前景分割-目标框回归的网络预测。Loss 项由中心点回归，朝向分类回归，尺寸分类回归三部分构成： <span class="math display">\[\begin{align}
L &amp;= L _ {seg} + w\sum _ i ^ 2 L _ {box _ i} \\
&amp;= L _ {seg} +  w\sum _ i ^ 2\left(L _ {center-reg _ i} + w _ 1L _ {size-cls _ i} + w _ 2L _ {size-reg _ i} + w _ 3L _ {heading-cls _ i} + w _ 4L _ {heading-reg _ i}\right)
\end{align}\tag{2}\]</span></p>
<h4 id="dynamic-object">2.4.2. Dynamic Object</h4>
<p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/dynamic.png" width="70%" height="70%" title="图 4. Dynamic Object"> 　　如图 4. 所示，对于动态目标，也可以将目标点云累积到某一时刻的目标中心坐标系中，但是累积很难对齐(目测可以用迭代的思想，定位-累积来精修)。所以本文采用从轨迹中直接提取特征，从而预测目标尺寸的方法。对于 \(T\) 时刻前后的目标点云 \(\{\mathcal{P} _ {j,k}\} _ {k=T-r} ^ {T+r}\) 以及目标框 \(\{\mathcal{B} _ {j,k}\} _ {k=T-s} ^ {T+s}\)，特征由两部分构成：</p>
<ul>
<li>Point 分支。将点云增加时间信息后，转换到当前目标框 \(\mathcal{B} _ {j,T}\) 的中心位置坐标系下，用 PointNet 网络作前后景的分割，得到前景目标的全局特征量。</li>
<li>Box 分支。同样转换到当前目标框中心坐标系下。需要注意的是，取的 Box 帧数大多比 Point 长(Point 只取 5 帧)。目标框特征维度为 8(x,y,z,l,h,w,ry,time)，同样通过 PointNet 网络提取全局特征。</li>
</ul>
<p>将这两个特征作融合，然后预测当前 \(T\) 时刻的目标尺寸及朝向。<br>
　　为了增强两个分支各自对预测目标的学习，分别对两个分支用真值目标作监督学习。最终的 Loss 为： <span class="math display">\[L = L _ {seg}+v _ 1L _ {box-traj} + v _ 2L _ {box-obj-pc}+v _ 3L _ {box-joint} \tag{3}\]</span></p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="comparing-with-sota-detectors">3.1. Comparing with SOTA Detectors</h3>
<p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/sota.png" width="90%" height="90%" title="图 5. sota"> 　　如图 5. 所示，毫无疑问，效果拔群。</p>
<h3 id="comparing-with-human-labels">3.2. Comparing with Human Labels</h3>
<p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/human.png" width="65%" height="65%" title="图 6. Human labels"> 　　如图 6. 所示，挑选 3 个经验丰富的标注员对数据进行标注，以此与本文方法进行比较，可见本方法只在高度估计上无法与人类标注员媲美，定位及其它尺寸上，几乎能达到类似水平。 <img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/human2.png" width="65%" height="65%" title="图 7. Consistency between human lablers"> 　　进一步思考，人类不同的标注员，他们的标注结果一致性如何？如图 7. 所示，GT 由多个标注员交叉验证得到，以此为基准，发现人类标注员也很难达到与真值相同水平的程度，标注质量几乎与本方法差不多，当距离较远时，点数会较少，人类标注员反而标注质量会下降。</p>
<h3 id="applications-to-semi-supervised-learning">3.3. Applications to Semi-supervised Learning</h3>
<p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/semi-supervised.png" width="65%" height="65%" title="图 8. Semi-supervised"> 　　用本文的方法去标注未标注的数据，作为实时目标检测模型的训练数据，能极大提升其性能。也从另一方面论证了本文方法能比拟人工标注水平。</p>
<h3 id="analysis-of-the-multi-frame-detector">3.4. Analysis of the Multi-frame Detector</h3>
<p><img src="/Offboard-3D-Object-Detection-from-Point-Cloud-Sequences/mvf-exp.png" width="65%" height="65%" title="图 9. MVF Experiments"> 　　如图 9. 所示，MVF 检测器性能实验，当输入帧数大于 5 帧时，帧数增多已经无法提升检测性能。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Qi, Charles R., et al. "Offboard 3D Object Detection from Point Cloud Sequences." arXiv preprint arXiv:2103.05073 (2021).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
        <category>Offline</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>A Comprehensive Survey on Point Cloud Registration</title>
    <url>/A-Comprehensive-Survey-on-Point-Cloud-Registration/</url>
    <content><![CDATA[<p>　　点云注册是是点云数据处理中非常重要的一个方向。<a href="/Object-Registration-with-Point-Cloud/" title="Object Registration with Point Cloud">Object Registration with Point Cloud</a> 中描述了基于点云的目标注册方法，主要阐述了传统 ICP 原理以及基于深度学习进行目标注册(相对位姿估计)的方法。本文<a href="#1" id="1ref"><sup>[1]</sup></a>则详细介绍整个点云注册方法的类别与细节。</p>
<h2 id="problem-definition">1. Problem Definition</h2>
<p>　　假设两个点云集 \(X\in\mathbb{R} ^ {M\times 3}, Y\in\mathbb{R} ^ {N\times 3}\)，其中每个点表示为 \(\mathbf{x} _ i(i\in [1,M])\)，\(\mathbf{y} _ i(y\in [1,N])\)。两个点云集合中有 \(K\) 个匹配点对，点云注册问题就是找到参数 \(g\)，即旋转矩阵 \(R\in\mathcal{SO}(3)\) 和位移矩阵 \(t\in\mathbb{R} ^ 3\)，使得： <span class="math display">\[\mathop{\arg\min}\limits _ {R\in\mathcal{SO}(3),t\in\mathbb{R} ^ 3}\Vert d(X,g(Y))\Vert ^ 2 _ 2 \tag{1}\]</span> 其中 \(d(X,g(Y))=d(X,RY+t)=\sum _ {k=1} ^ K \Vert \mathbf{x} _ k-(R\mathbf{y} _ k+t)\Vert _ 2\)。这个问题是典型的鸡生蛋蛋生鸡问题，如果匹配点对已知，那么变换矩阵可以求解；如果变换矩阵已知，那么就能得到匹配点对。</p>
<h2 id="challenges">2. Challenges</h2>
<p>　　根据数据源类型，点云注册可分为 same-source 以及 cross-source 两类。其挑战分别有：</p>
<ul>
<li>Same-source
<ol type="1">
<li>Noise and Outliers<br>
</li>
<li>Partial overlap<br>
</li>
</ol></li>
<li>Cross-source
<ol type="1">
<li>Noise and Outliers<br>
</li>
<li>Partial overlap<br>
</li>
<li>Density difference<br>
不同传感器获得的数据源，点云密度可能不一样。</li>
<li>Scale variation<br>
不同传感器获得的数据源，点云的空间尺度可能不一样。</li>
</ol></li>
</ul>
<h2 id="categories">3. Categories</h2>
<p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/taxonomy.png" width="90%" height="90%" title="图 1. Taxonomy"></p>
<ul>
<li>Optimisation-based<br>
</li>
<li>Feature learning<br>
</li>
<li>End-to-end learning-based</li>
<li>Cross-source registration</li>
</ul>
<h2 id="optimisation-based">4. Optimisation-based</h2>
<p>　　大多数优化方法都包含两个步骤：匹配点对搜索，以及转换矩阵估计。匹配点对可通过计算 point-point 距离或特征相似度得到。这种方法的好处是有严谨的数学解，能保证收敛，不需要训练数据；缺点是需要复杂的策略来解决噪音，离群点，遮挡等问题。<br>
<img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/optimization-based.png" width="90%" height="90%" title="图 2. Optimization-based"> 　　对于已搜索到匹配点对后，可用非线性问题求解方法来优化计算转换矩阵。根据优化策略不同，可分为如下几种方法。</p>
<h3 id="icp-based">4.1. ICP-based</h3>
<p>　　首先匹配点中距离度量方式分为三种：</p>
<ul>
<li><p>Point-Point<br>
就是式 (1) 下方的传统方式，计算两个点的欧式距离。</p></li>
<li><p>Point-Plane<br>
表示点与对应平面之间的距离： <span class="math display">\[\mathop{\arg\min}\limits _ {R\in\mathcal{SO}(3),t\in\mathbb{R} ^ 3}\left\{\sum _ {k=1} ^ K w _ k\left\Vert \mathrm{n} _ k * (\mathrm{x} _ k-(R\mathrm{y} _ k+t))\right\Vert ^ 2\right\} \tag{2}\]</span> 其中 \(w _ k\) 是匹配对权重，\(\mathrm{n _ k}\) 是面的法向量。</p></li>
<li><p>Plane-Plane<br>
表示面与对应平面之间的距离： <span class="math display">\[\mathop{\arg\min}\limits _ {R\in\mathcal{SO}(3),t\in\mathbb{R} ^ 3}\left\{\sum _ {k=1} ^ K \left\Vert \mathrm{nx} _ k-(R\mathrm{ny} _ k+t)\right\Vert ^ 2\right\} \tag{3}\]</span> 其中 \(\mathbf{nx,ny}\) 是对应的法向量。</p></li>
<li><p>Generalized ICP<br>
<span class="math display">\[\mathop{\arg\min}\limits _ {T}\left\{\sum _ {k=1} ^ K \left\Vert d ^T(C _ k ^ Y+\mathbf{T} C _ k ^ X\mathbf{T} ^ T) ^ {-1}\right\Vert ^ 2\right\} \tag{4}\]</span> 其中 \(\{C _ k ^ X\}\)，\(\{C _ k ^ Y\}\) 为点云 \(X,Y\) 之间的协方差矩阵。当 \(\{C _ k ^ X=0\}\)，\(\{C _ k ^ Y=I\}\) 时，就是标准的 point-point ICP；\(\{C _ k ^ X = 0\}\)，\(\{C _ k ^ Y = P _ k ^ {-1}\}\) 时就是 pont-plane ICP，其中 \(P _ k ^ {-1}\) 为法向量。<br>
　　根据匹配点度量方式获得匹配点后，即可优化求解位姿矩阵，有三种方法：</p></li>
<li><p>SVD-based<br>
用奇异值分解的方式求解。</p></li>
<li><p>Lucas-Kanade<br>
包括 Levenberg-Marquardt 方法，用雅克比矩阵及近似高斯牛顿法优化求解。</p></li>
<li><p>Procrustes analysis<br>
将位姿估计转换为线性最小二乘问题。位姿闭式解为 \(P=(X _ 2 ^ HX _ 1)^ { -1 }X _ 2^Hx _ 1\)。</p></li>
</ul>
<h3 id="graph-based">4.2. Graph-based</h3>
<p>　　将点云建模为非参图模型，包括边与顶点。GM 方法目的就是通过边与顶点去寻找两个图中的匹配点，GM 可分为 second-order 与 high-order 方法，前者只考虑边与边，顶点与顶点的相似性，后者则会考虑多于两个点的相似性，比如三角对相似性。</p>
<h3 id="gmm-based">4.3. GMM-based</h3>
<p>　　高斯混合模型法核心是将点云注册问题建模为最大化似然的过程。求解后，可得到位姿和混合高斯参数。</p>
<h3 id="semi-definite-registration">4.4. Semi-definite Registration</h3>
<p>　　将问题近似为其它问题求解。</p>
<h2 id="feature-learning">5. Feature-learning</h2>
<p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/feature-learning.png" width="90%" height="90%" title="图 3. Feature-learning"> 　　基于特征学习的方法，是提取点云的点级别特征，然后作一次性精准匹配，最后直接用 SVD 等后端优化方法得到，无需进行多次迭代。<a href="/Object-Registration-with-Point-Cloud/" title="Object Registration with Point Cloud">Object Registration with Point Cloud</a> 中介绍的也属于这种方法。<br>
　　对于点云特征提取的方法，Learning on volumetric data 以及 Learning on point cloud 都介绍的已经非常多了，这里不作展开。</p>
<h2 id="end-to-end-learning-based">6. End-to-end Learning-based</h2>
<p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/end-end.png" width="90%" height="90%" title="图 4. End-to-end"> 　　如图 4. 所示，端到端的方法主要分为 Registration by regression 和 Registration by optimization and neural network 方法。</p>
<h2 id="cross-source">7. Cross-source</h2>
<p><img src="/A-Comprehensive-Survey-on-Point-Cloud-Registration/cross-source.png" width="90%" height="90%" title="图 5. cross-source"> 　　如图 5. 所示，多源点云数据的注册，方法也分为 Optimization-based 和 Learning-based，思路也差不多，这里不作展开。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Huang, Xiaoshui, et al. "A comprehensive survey on point cloud registration." arXiv preprint arXiv:2103.02690 (2021).</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
        <tag>Point Cloud Registration</tag>
      </tags>
  </entry>
  <entry>
    <title>4D Panoptic LiDAR Segmentation</title>
    <url>/4D-Panoptic-LiDAR-Segmentation/</url>
    <content><![CDATA[<p>　　4D 激光点云的全景分割任务是在 3D 基础上，引入时间维度，同时在时间和空间下作点云的语义分割以及实例分割，输出的是每个点的语义类别信息，以及时序实例 ID。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种简单 4D 全景分割框架。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/4D-Panoptic-LiDAR-Segmentation/framework.png" width="70%" height="70%" title="图 1. Framework"> 　　如图 1. 所示，将时序点云作累积，然后用 Encoder-Decoder 网络，输出量有：</p>
<ul>
<li>Semantic Map，每个点的类别；</li>
<li>Objectness Map，目标中心点，或者靠近中心的某点；</li>
<li>Point Embeddings，每个点的特征；</li>
<li>Point Variance Map，每个点的位置方差；</li>
</ul>
<p>有了这些信息后，就可以聚类出目标实例，方法类似 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>，将 variance map 拓展到 Embedding 任意空间，应该是目前比较先进的策略了。<br>
　　假设目标实例的点云符合高斯分布，那么在已知中心点，或者属于该实例的某点的情况下(不用必须是中心点，这样能处理遮挡等情况)，就可以判断其它点属于该实例的概率。具体的，已知实例中心点 \(\mathbf{p} _ i\)，以及对应的 Embedding 特征 \(e _ i\)，可以计算其它点 \(p _ j\) 是否属于该实例的概率： <span class="math display">\[\hat{p} _ {ij}=\frac{1}{(2\pi) ^ {D/2}|\Sigma _ i| ^ {\frac{1}{2}}}\mathrm{exp}\left(-\frac{1}{2}(e _ i-e _ j)^T\Sigma _ i^ {-1}(e _ i-e _ j)\right) \tag{1}\]</span> 其中 \(\Sigma _ i\) 是通过点 \(p _ i\) 预测的 \(\sigma _ i\) 所构建的对角矩阵。值得注意的是，Embedding 特征串联了空间和时序量，这样更有利于聚类，同时预测对应的 variance map。<br>
　　ID 则是通过时序下作实例的数据关联得到的。</p>
<h2 id="loss">2. Loss</h2>
<p>　　网络输出都是点级别的。总的 Loss 为： <span class="math display">\[L=L _ {class}+L _ {obj}+L _ {ins}+L _ {var} \tag{2}\]</span> 具体的：</p>
<ul>
<li>Semantic Segmentation<br>
用 Cross-entropy 分类损失函数，并且通过采样的方法来解决类别不平衡问题。</li>
<li>Point Centerness<br>
计算每个点与其实例中心点，或者实例点云重心点的距离，归一化后作为中心点得分损失函数： <span class="math display">\[L _ {obj} = \sum _ {i=1} ^ N(\hat{o} _ i-o _ i) ^ 2,\;\;\;\hat{o} _ i,o _ i\in[0,1]\tag{3}\]</span></li>
<li>Instance Probability<br>
计算每个实例中每个点属于该实例的概率，回归其值到 1: <span class="math display">\[L _ {ins}=\sum _ {j=1} ^ K\sum _ {i=1} ^ N(\hat{p} _ {ij}-p _ {ij}) ^ 2,\;\;\;p _ {ij} = 1,\;\mathrm{if}\; p _ i\in I _ j,\;\mathrm{else}\; 0\tag{4}\]</span></li>
<li>Variance Smooth<br>
类似 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>，作实例内每个点 Variance 的一致性约束： <span class="math display">\[L _ {var} = \frac{1}{|I _ j|}\sum _ {i\in I _ j}\Vert \sigma _ i-\sigma _ j\Vert ^ 2\tag{5}\]</span></li>
</ul>
<h2 id="measuring-performance">3. Measuring Performance</h2>
<p>To be continued...</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Aygün, Mehmet, et al. "4D Panoptic LiDAR Segmentation." arXiv preprint arXiv:2102.12472 (2021).</p>
]]></content>
      <categories>
        <category>Segmentation</category>
        <category>Instance Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Instance Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>SPVConv &amp; 3D-NAS</title>
    <url>/SPVConv-3D-NAS/</url>
    <content><![CDATA[<p>　　基于点云的神经网络学习方法在 <a href="/Deep-Learning-for-3D-Point-Clouds/" title="Deep Learning for 3D Point Clouds">Deep Learning for 3D Point Clouds</a> 中已经有较为详细的描述，从网络结构上看，可分为 Multi-view-based(Projection-based)，Volumetric-based，Point-based 等三种方法。其中 Point-based 方法对点云的信息提取分辨率最高，但是前两种将点云空间离散化的方法更容易学到特征信息。所以如何结合这两种网络结构来更有效得学习点云特征就显得非常重要。<br>
　　此外，<a href="/Rethinking-of-Sparse-3D-Convolution/" title="Rethinking of Sparse 3D Convolution">Rethinking of Sparse 3D Convolution</a> 中阐述了 Sparse Convolution 相比传统卷积在点云特征学习中的优势，其能更有效学习点云特征信息。<br>
　　本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种 Sparse Point-Voxel 的基本网络结构，并采用 NAS 网络搜索方法来搜索最优的卷积通道数量及深度。</p>
<h2 id="sparse-point-voxel-convolutionspvconv">1. Sparse Point-Voxel Convolution(SPVConv)</h2>
<p>　　Voxel-based 特征提取会损失信息源，但是提取效率较高；Point-based 能保留信息分辨率，但是提取效率较低。本文设计的 SPVConv 则融合了二者的优势。 <img src="/SPVConv-3D-NAS/spvconv.png" width="90%" height="90%" title="图 1. SPVconv"> 　　如图 1. 所示，网络模块由两个分支组成，Point-Based 分支对点云进行点级别的 MLP 运算；Sparse Voxel-Based 分支将点云进行体素化，然后用稀疏卷积进行特征提取，然后反体素化得到点级别的特征。最后将二者提取的特征融合到一起。<br>
　　数学描述上，首先是数据表示：</p>
<ul>
<li>稀疏体素化张量(Sparse Voxelized Tensor)表示为 \(\mathbf{S}=\{(\mathbf{p} _ m ^ s,\mathbf{f} _ m ^ s),v\}\)，其中 \(\mathbf{p} _ m ^ s=(\mathbf{x} _ m ^ s,\mathbf{y} _ m ^ s, \mathbf{z} _ m ^ s)\) 表示第 \(m\) 个体素的 3D 坐标，\(\mathbf{f} _ m ^ s\) 表示其特征向量；\(v\) 是该层级下的体素尺寸；</li>
<li>点云张量(Point Cloud Tensor)表示为 \(\mathbf{T} = \{(\mathbf{p} _ k ^ t,\mathbf{f} _ k ^ t)\}\)，其中 \(\mathbf{p} _ k=(\mathbf{x} _ k, \mathbf{y} _ k,\mathbf{z} _ k)\) 表示第 \(k\) 个点的 3D 坐标，\(\mathbf{f} _ k\) 为其特征向量。</li>
</ul>
<p>然后将点云进行稀疏体素化映射，计算每个体素的特征向量：</p>
<p><span class="math display">\[\hat{\mathbf{p}} _ k ^ t=(\hat{\mathbf{x}} _ k ^ t,\hat{\mathbf{y}} _ k ^ t,\hat{\mathbf{z}} _ k ^ t) = \left(\mathrm{floor}(\mathbf{x} _ k ^ t/v),\mathrm{floor}(\mathbf{y} _ k ^ t/v),\mathrm{floor}(\mathbf{z} _ k ^ t/v)\right) \tag{1}\]</span> <span class="math display">\[\mathbf{f} _ m ^ s=\frac{1}{N _ m}\sum _ {k=1} ^ n\mathbb{1}[\hat{\mathbf{x}} _ k ^ t=\mathbf{x} _ m ^ s,\hat{\mathbf{y}} _ k ^ t=\mathbf{y} _ m ^ s,\hat{\mathbf{z}} _ k ^ t=\mathbf{z} _ m ^ s]\cdot\mathbf{f} _ k ^ t\tag{2}\]</span> 直接计算的复杂度为 \(\mathcal{O}(mn)\)，为了实时计算，需要对体素化和反体素化进行哈希索引。哈希表的 key 为 3D 坐标，value 为稀疏张量的 index，所以最终的建立哈希表和查询复杂度为 \(\mathcal{O}(m+n)\)。<br>
　　对于反体素化，采用 trilinear interpolation，在稀疏体素中采样得到点级别的特征。该特征与点级别 MLP 得到的特征作特征维度的串联操作，即得到提取的点特征向量。</p>
<h2 id="d-nas">2. 3D-NAS</h2>
<p>　　以 SPVConv 模块为基础结构，搜索最优的网络 channel 数，以及网络深度。卷积核固定为 3x3。<br>
　　搜索策略为：首先训练一个最大的网络，那么搜索的子网络权重可以从大网络中继承，具体搜索策略可参考代码。</p>
<p><img src="/SPVConv-3D-NAS/3dnas.png" width="90%" height="90%" title="图 2. 3D-NAS"></p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Tang, Haotian, et al. "Searching efficient 3d architectures with sparse point-voxel convolution." European Conference on Computer Vision. Springer, Cham, 2020.</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>Depth Prediction/Completion</title>
    <url>/Depth-Prediction-Completion/</url>
    <content><![CDATA[<p>　　准确的深度信息获取在自动驾驶中非常重要，所以激光点云以其稳定精确的深度测量优势在自动驾驶传感器配置中不可或缺。但是激光雷达昂贵而且获取的点云较为稀疏，由此可考虑两种替代方案：基于单目、双目的深度预测；以及基于单目，激光雷达的深度补全。<br>
　　深度预测与深度补全都是为了获得稠密的图像像素级别深度信息，所以在自监督的损失函数上，非常相似。本文就提取归纳一些共同的优化点，并对各自的优化方向作阐述。</p>
<h2 id="self-supervised-depth-estimation">1. Self-supervised Depth Estimation</h2>
<p>　　室外场景下，图像像素级别的深度信息真值很难获得，在位姿准确的情况下可累积激光点云的测量信息来填充大部分像素区域，以此作为真值；也可以借助 SLAM 建图定位技术，重建场景稠密的深度信息，从而使像素均能索引到真实的深度。但是以上两种方法都只能获得静态场景的深度信息。所以采用自监督的深度估计技术，就显得尤为重要。<br>
　　本文介绍两篇文章：<a href="#1" id="1ref">[1]</a> 是基于图像的深度估计，<a href="#2" id="2ref">[2]</a> 是基于图像和激光雷达的深度补全。二者的网络结构都比较简单，主要是无监督/半监督的 Loss 设计。</p>
<h3 id="digging-into-self-supervised-monocular-depth-estimation">1.1. Digging Into Self-Supervised Monocular Depth Estimation</h3>
<p><img src="/Depth-Prediction-Completion/digging.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，深度估计的网络输入为单帧，经过 Encoder-Decoder 输出像素级别的深度。Pose 估计的网络输入前后帧，输出前后帧的本体位姿变换。无监督 Loss 通过当前帧与前后帧的像素匹配计算，在遮挡或是非连续的情况下，当前帧的某些像素区域在前后帧中是不可见的(如图 2. 所示)，所以会产生较大的 Loss，本文提出了取最小化 Loss 的方法，有效解决该问题。此外，计算不同尺度下的 Loss。 <img src="/Depth-Prediction-Completion/min-reproj.png" width="60%" height="60%" title="图 2. Per-pixel Minimum Reprojection"></p>
<h3 id="self-supervised-sparse-to-dense-self-supervised-depth-completion-from-lidar-and-monocular-camera">1.2. Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera</h3>
<p><img src="/Depth-Prediction-Completion/complete.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 3. 所示，类似的，该方法输入为 RGB+Sparse LiDAR 数据，通过 Depth Loss 半监督，通过 Photometric Loss 无监督学习深度信息。其中 Pose 通过提取前后帧角点，匹配，然后求解 PnP 的方式计算得到。</p>
<h2 id="loss">2. Loss</h2>
<p>　　无监督/半监督学习深度信息，Loss 的设计就很关键，以下列举几种常用的 Loss 设计方法。</p>
<h3 id="sparse-depth-loss">2.1. Sparse Depth Loss</h3>
<p>　　对于有对应的 LiDAR 点云的情况，可对点云打到的图像像素区域作有监督学习： <span class="math display">\[\mathcal{L} _ {depth} = \Vert \mathbb{1} _ {\{d &gt; 0\}}\cdot (\mathbf{pred-d})\Vert _ 2 ^ 2\tag{1}\]</span></p>
<h3 id="smoothness-loss">2.2. Smoothness Loss</h3>
<p>　　为了使得估计的深度信息在空间上较为平滑，<a href="#2" id="2ref">[2]</a> 采用简单的像素空间二次导数最小化的方法： <span class="math display">\[\mathcal{L} _ {smooth} = \Vert\nabla ^ 2\mathbf{pred}\Vert _ 1\tag{2}\]</span> <a href="#1" id="1ref">[1]</a> 则采用像素空间 Edge-aware 的平滑 Loss，这样在物理空间上深度信息没有拖影的现象： <span class="math display">\[\mathcal{L} _ {smooth} = \vert\partial _ xd _ t ^ * \vert e ^ {-\vert \partial _ xI _ t\vert}+\vert\partial _ yd _ t ^ * \vert e ^ {-\vert \partial _ yI _ t\vert}\tag{3}\]</span></p>
<h3 id="photometric-reprojection-loss">2.3. Photometric Reprojection Loss</h3>
<p>　　设当前图像帧预测的深度信息为 \(\mathbf{pred} _ 0\)，相机内参矩阵为 \(\mathcal{K}\)，那么对于相机相对位姿为 \(T _ {0\rightarrow 1}\) 的图像 \(\mathbf{RGB} _ 1\)，其像素坐标系关系为：\(p _ 1=\mathcal{K}T _ {0\rightarrow 1}\mathbf{pred} _ 0(p _ 0)\mathcal{K} ^ {-1}p _ 0\)，由此可得到图像从 \(0\rightarrow 1\) 的变换： <span class="math display">\[\mathbf{warped _ 1(p _ 0)}=\mathrm{bilinear}(\mathbf{RGB} _ 1(\mathcal{K}T _ {0\rightarrow 1}\mathbf{pred} _ 0(p _ 0)\mathcal{K} ^ {-1}p _ 0)) \tag{4}\]</span> <a href="#2" id="2ref">[2]</a> 采用多尺度的 L1 Loss，并只在无激光点云的像素区域作无监督学习： <span class="math display">\[\mathcal{L} _ {photometric}(\mathbf{warped _ 1,RGB _ 1})=\sum _ {s\in S} \frac{1}{s}\left\Vert\mathbb{1} ^ {(s)} _ {d==0}\cdot(\mathbf{warped _ 1} ^ {(s)}-\mathbf{RGB _ 1} ^ {(s)})\right\Vert _ 1 \tag{5}\]</span> 　　<a href="#1" id="1ref">[1]</a> 采用 L1 与 SSIM<a href="#3" id="3ref"><sup>[3]</sup></a> 的方式来构造 Photometric 损失函数： <span class="math display">\[\mathcal{L} _ {photometric}(\mathbf{warped _ 1,RGB _ 1}) = \frac{\alpha}{2}(1-\mathrm{SSIM}(\mathbf{warped _ 1,RGB _ 1}))+(1-\alpha)\Vert\mathbf{warped _ 1-RGB _ 1}\Vert _ 1\tag{6}\]</span> 其中 SSIM 是描述图像结构信息相似度的函数。此外 <a href="#1" id="1ref">[1]</a> 除了多尺度训练外，还作了两点改进：</p>
<ol type="1">
<li><p>Per-Pixel Minimum Reprojection<br>
如图 2. 所示，在遮挡以及图像边缘情况，\(\mathbf{warped} _ 1\) 可能在 \(\mathbf{RGB} _ 1\) 上找不到对应的像素点，导致损失函数失真变大，所以引入 \(\mathbf{RGB} _ {-1}\)，同时考虑 \(\mathbf{warped} _ {-1}\) 与其匹配: <span class="math display">\[\mathbf{warped _ {-1}(p _ 0)}=\mathrm{bilinear}(\mathbf{RGB} _ {-1}(\mathcal{K}T _ {0\rightarrow -1}\mathbf{pred} _ 0(p _ 0)\mathcal{K} ^ {-1}p _ 0)) \tag{7}\]</span> 取二者最小的误差作为投影误差： <span class="math display">\[\mathcal{L} _ {photometric} = \sum _ p \mathop{\min}\limits \{pe _ {-1}, pe _ {1}\} \tag{8}\]</span> 其中 \(pe\) 是像素级别的 SSIM 与 L1 误差。</p></li>
<li><p>Auto-Masking Stationary Pixels<br>
<img src="/Depth-Prediction-Completion/mask.png" width="70%" height="70%" title="图 4. Auto-Masking"> 对于静态场景以及运动物体，投影误差来描述深度估计都是不准确的，所以用一个像素级别的 mask 来计算最终的损失函数，判断一个像素是否计入损失函数的条件为 \(\mathbf{warped}\) 像素值与目标像素值的误差是否小于原始像素值与目标像素值的误差： <span class="math display">\[\mu = \left[ \mathop{\min}\limits \{pe _ {-1} ^ {warped}, pe _ {1} ^ {warped}\} &lt;\mathop{\min}\limits \{pe _ {-1} ^ {unwarped}, pe _ {1} ^ {unwarped}\} \right] \tag{9}\]</span> 其中 \([\cdot]\) 使得 \(\mu\in \{0,1\}\)。这样就能在静态场景以及运动物体与本车速度相似的情况下，不计入该区域的投影损失误差值。</p></li>
</ol>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Godard, Clément, et al. "Digging into self-supervised monocular depth estimation." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.<br>
<a id="2" href="#2ref">[2]</a> Ma, Fangchang, Guilherme Venturelli Cavalheiro, and Sertac Karaman. "Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera." 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.<br>
<a id="3" href="#3ref">[3]</a> Wang, Zhou, et al. "Image quality assessment: from error visibility to structural similarity." IEEE transactions on image processing 13.4 (2004): 600-612.</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Depth Completion</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
        <tag>Depth Prediction</tag>
        <tag>Depth Completion</tag>
      </tags>
  </entry>
  <entry>
    <title>A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving</title>
    <url>/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/</url>
    <content><![CDATA[<p>　　概率目标检测是将不确定估计应用于目标检测任务中，不确定性估计之前已经描述很多了，包括 Epistemic Uncertainty，Aleatoric Uncertainty，以及 Uncertainty Calibration 相关技术。本文<a href="#1" id="1ref"><sup>[1]</sup></a>则详细阐述概率目标检测的进展。<br>
　　在自动驾驶领域，不管是什么传感器，在极端天气环境或不熟悉的场景下，以及远距离或高遮挡情况下，基于深度学习的目标检测模型失效概率会比较大，或者说预测的不确定性会比较大。人类驾驶员在这方面比较擅长，比如在雨夜看不太清的场景，会先降低速度，增加观察时间，以获得观测的高确定性。所以对网络而言，不确定的估计才能为后续增加观测的确定性作准备。</p>
<h2 id="uncertainty-estimation-in-deep-learning">1. Uncertainty Estimation in Deep Learning</h2>
<p>　　<a href="/Perception-Uncertainty-in-Deep-Learning/" title="Perception Uncertainty in Deep Learning">Perception Uncertainty in Deep Learning</a> 中已经较为详细得阐述了 Uncertainty 的来龙去脉。贝叶斯神经网络框架下，不确定性可分解为认知不确定性(Epistemic Uncertainty)以及偶然不确定性(Aleatoric Uncertainty)。 <img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/uncert-cate.png" width="60%" height="60%" title="图 1. Uncertainty Categorization"> 　　从感知数据流角度，如图 1. 所示，会引入很多不确定性，从最原始的传感器不确定性，到标注不确定性，再到模型训练测试的不确定性，所有这些不确定性构成了最终模型输出结果的不确定性。<br>
　　数学上，对于训练数据集 \(\mathscr{D}\) 贝叶斯神经网络的输出分布表示为: <span class="math display">\[p(\mathbf{y|x}, \mathscr{D}) = \int p(\mathbf{y|x}, \mathbf{W})p(\mathbf{W}| \mathscr{D}) \mathrm{d}\mathbf{W}\tag{1}\]</span> 其中 \(p(\mathbf{y|x,W})\) 表示观测似然，包含了偶然不确定性；\(p(\mathbf{W}|\mathscr{D})\) 表示模型后验分布，包含认知不确定性。</p>
<h3 id="practical-methods-for-uncertainty-estimation">1.1. Practical Methods for Uncertainty Estimation</h3>
<p>　　实际不确定性的估计需要在效率上考虑到其可行性。可分为用于估计模型认知不确定性的 MC-Dropout，Deep Ensembles 方法；用于估计偶然不确定性的 Direct Modeling 方法；以及用于估计模型认知及偶然不确定性的 Error Propagation 方法，以下作详细描述：</p>
<ul>
<li><p>Monte-Carlo Dropout<br>
对网络进行 \(T\) 次前向 Inference，\(\mathbf{W} _ t\) 为网络经过 dropout 后的权重，那么网络预测的概率分布为： <span class="math display">\[p(\mathbf{y|x},\mathscr{D})\approx \frac{1}{T}\sum _ {t=1} ^ T p(\mathbf{y|x,W _ t}) \tag{2}\]</span> 对于回归问题，由此可计算其回归量的均值和方差。</p></li>
<li><p>Deep Ensembles<br>
用网络模型集合来估计输出的概率分布，本质上 Monte-Carlo Dropout 方法也是一种网络模型集合的方法。设 \(M\) 个网络权重为 \(\{\mathbf{W} _ m\} _ {m=1} ^ M\)，那么输出概率分布为： <span class="math display">\[p(\mathbf{y|x},\mathscr{D})\approx \frac{1}{M}\sum _ {m=1} ^ M p(\mathbf{y|x,W _ m}) \tag{3}\]</span></p></li>
<li><p>Direct Modeling<br>
该方法假设模型的回归输出符合多模态混合高斯分布，即 \(p(y|\mathbf{x,W}) = \mathcal{N}(y|\hat{\mu}(\mathbf{x,W}),\hat{\sigma} ^ 2(\mathbf{x,W}))\)，其中 \(\hat{\mu}(\mathbf{x,W})\) 为网络输出，\(\hat{\sigma} ^ 2(\mathbf{x,W})\) 为输出值的方差。最小化负对数似然函数，即可预测输出量的均值和方差： <span class="math display">\[L(\mathbf{x,W})=-\mathrm{log}(p(\mathbf{y|x,W})) \approx \frac{(y-\hat{\mu}(\mathbf{x,W}))^2}{2\hat{\sigma} ^ 2(\mathbf{x,W})}+\frac{\mathrm{log}\hat{\sigma} ^ 2(\mathbf{x,W})}{2}\tag{4}\]</span> 对于分类问题，假设 softmax 预测中的每个 logits 元素符合独立高斯分布，类似回归问题，网络同时预测均值 logits 以及方差 logits。训练的时候，用重采样的方法在每个高斯分布中采样出每个 logits，最后再用标准的分类误差函数计算其损失函数。<br>
直接模型求解只需要增加额外的模型分支即可，引入的计算量并不大，但是会产生预测的方差不准的问题，需要进一步标定。</p></li>
<li><p>Error Propagation<br>
误差传递方法计算效率最高，直接将数据源的误差通过每个操作层进行传递，比如 <a href="/A-General-Framework-for-Uncertainty-Estimation-in-deep-learning/" title="A General Framework for Uncertainty Estimation in Deep Learning">A General Framework for Uncertainty Estimation in Deep Learning</a>。</p></li>
</ul>
<h3 id="evaluation-and-benchmarking">1.2. Evaluation and Benchmarking</h3>
<p>　　评估不确定性估计的指标有：</p>
<ul>
<li><p>Shannon Entropy<br>
用来描述分类任务的不确定性： <span class="math display">\[\mathcal{H}(y|\mathbf{x},\mathscr{D}) = -\sum _ {c=1} ^ C p(y=c|\mathbf{x},\mathscr{D})\mathrm{log}\left(p(y=c|\mathbf{x},\mathscr{D})\right) \tag{5}\]</span> 当 \(p(y=c|\mathbf{x},\mathscr{D}) = 0 \mathrm{or} 1\) 时，不确定性最小。个人认为，该指标只能描述不确定性的大小，无法描述不确定估计的准确性。</p></li>
<li><p>Mutual Information<br>
与 SE 类似，也是描述分类任务的不确定性，但是引入了模型不确定性： <span class="math display">\[\mathcal{I}(y,\mathbf{W|x},\mathscr{D})=\mathcal{H}(y|\mathbf{x},\mathscr{D})-\mathbb{E} _ {p(\mathbf{W}|\mathscr{D})}\left[\mathcal{H}(y|\mathbf{x,W})\right] \tag{6}\]</span> 其中 conditional Shannon Entropy 通过采样模型权重(MC-Dropout)计算得到： <span class="math display">\[\mathcal{H}(y|\mathbf{x,W}) = -\sum _ {c=1} ^ C p(y=c|\mathbf{x,W})\mathrm{log}\left(p(y=c|\mathbf{x,W})\right) \tag{7}\]</span></p></li>
<li><p>Calibration Plot<br>
<a href="/Uncertainty-Calibration/" title="Uncertainty Calibration">Uncertainty Calibration</a> 中已经较为详细的阐述了 Calibration Plot 的作用，这里不作展开。由此可用 ECE Score 来评估不确定的准确度： <span class="math display">\[\mathrm{ECE}=\sum _ {t=1} ^ T\frac{N _ t}{N _ {eval}}|p _ t-\hat{p} _ t| \tag{8}\]</span></p></li>
<li><p>Negative Log Likelihood(NLL)<br>
NLL 也是 Direct Modeling 方法中不确定性的预测的 Loss，其描述了预测的概率分布与真值分布的相似性，所以能评估不确定性预测的准确度： <span class="math display">\[\mathrm{NLL}=-\sum _ {n=1} ^ {N _ {test}}\mathrm{log}\left(p(\mathbf{y _ n|x _ n}, \mathscr{D})\right) \tag{9}\]</span></p></li>
<li><p>Brier Score<br>
用于评估分类概率的准确性: <span class="math display">\[\mathrm{BS} = \frac{1}{N _ {test}}\sum _ {n=1} ^ {N _ {test}}\sum _ {c=1} ^ C (\hat{s} _ {n,c} - y _ {n,c}) ^ 2 \tag{10}\]</span> 其中 \(\hat{s} _ {n,c}\) 表示 softmax score, \(y _ {n,c}\in\{0, 1\}\) 表示真值。越小表示估计的不确定性越好。</p></li>
<li><p>Error Curve<br>
不确定性越大，表示与真值的误差越大。所以逐步去掉不确定性较大的数据，剩下数据的误差会逐渐减少。</p></li>
<li><p>Total Variance(TV)<br>
在回归任务中，计算 Covariance matrix 的 trace，作为回归任务的总的方差。</p></li>
</ul>
<h2 id="probabilistic-object-detection">2. Probabilistic Object Detection</h2>
<p>　　传统的目标检测方法都是确定性的，而 POD 目的是在目标检测的基础上，在分类及回归任务中，进一步估计可靠的不确定性。<br>
<img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/det.png" width="90%" height="90%" title="图 2. POD Pipeline"> 　　我们要估计的不确定性包括 Epistemic Uncertainty 以及 Aleatoric Uncertainty。认知不确定性用 MC-Dropout 来估计。偶然不确定性中的回归问题在 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中已经较为详细的描述，基本就是用 NLL LOSS 来优化；对于分类问题，本文采用重采样高斯分布的 softmax logits 的方法，具体的，预测输出与分类 softmax logits 长度一样的 logits variance，然后对每个高斯分布的 logit 作重采样，作为最终的输出以作 Loss 优化。<br>
　　综上，POD 估计不确定性可归纳为： <span class="math display">\[\left\{\begin{array}{l}
\hat{\mu}(\mathbf{x}) = \frac{1}{T}\sum _ {t=1} ^ T(\mathbf{x,W} _ t) \\
\hat{\sigma} ^ 2(\mathbf{x}) = \hat{\sigma} _ e ^ 2(\mathbf{x}) + \hat{\sigma} _ a ^ 2(\mathbf{x}) \\
\hat{\sigma} _ e ^ 2(\mathbf{x}) = \frac{1}{T}\sum _ {t=1} ^ T\left(\hat{\mu}(\mathbf{x,W} _ t)\right) ^ 2 - \left(\hat{\mu}(\mathbf{x})\right) ^ 2\\
\hat{\sigma} _ a ^ 2(\mathbf{x}) = \frac{1}{T} \sum _ {t=1} ^ T \hat{\sigma} ^ 2(\mathbf{x,W} _ t)
\end{array}\tag{11}\right.\]</span> 通过 T 次蒙特卡洛采样，得到最终总的不确定性。</p>
<h2 id="comparative-study">3. Comparative Study</h2>
<p><img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/pod.png" width="90%" height="90%" title="图 3. Methods"> 　　用 Droupout 估计 Epistemic Uncertainty，对于 Aleatoric Uncertainty 的估计，实现了三种方法：</p>
<ul>
<li><p>Loss Attenuation<br>
网络输出分类 logits 以及回归量的均值和方差，用 Direct Modeling 中的权重 Loss 来优化；</p></li>
<li><p>BayesOD<br>
与 Loss Attenuation 不同的是，后处理用 Data Association + Bayesian Fusion 来代替标准的 NMS 算法；</p></li>
<li><p>Output Redundancy<br>
网络输出不作改变，后处理采用 Data Association + Sample Statistics 来估计不确定性；</p></li>
</ul>
<p><img src="/A-Review-and-Comparative-Study-on-Probabilistic-Object-Detection-in-Autonomous-Driving/res.png" width="90%" height="90%" title="图 4. Evaluation Result"> 　　这几种方法对比如图 4. 所示，不确定性估计对目标检测性能有所提升，并且能提供预测的不确定性。具体方法的实现细节可见<a href="#2" id="2ref">[2]</a>。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving." arXiv preprint arXiv:2011.10671 (2020).<br>
<a id="2" href="#2ref">[2]</a> https://github.com/asharakeh/pod_compare</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
        <tag>Multi-modal Fusion</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving - Datasets, Methods, and Challenges</title>
    <url>/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/</url>
    <content><![CDATA[<p>　　由于相机，激光雷达，毫米波雷达等传感器各有优劣，所以深度多模态数据融合在自动驾驶感知中非常重要。本文<a href="#1" id="1ref"><sup>[1]</sup></a>以目标检测及语义分割为例，详细阐述了深度多模态数据融合的发展及挑战。<br>
　　多模传感器融合的目标检测及语义分割任务，可分解为三大问题：What to Fuse，When to Fuse，How to Fuse。以下就从这三个方面进行分析归纳。</p>
<h2 id="what-to-fuse">1. What to Fuse</h2>
<p>　　自动驾驶中用于全范围感知的有激光雷达，毫米波雷达，相机。<a href="/paper-reading-RadarNet/" title="RadarNet">RadarNet</a> 中比较详细得介绍了激光雷达与毫米波雷达的优劣，并融合二者作目标检测跟踪；<a href="/paper-reading-CenterFusion/" title="CenterFusion">CenterFusion</a> 则融合毫米波雷达与相机二者的优势，作目标检测与速度测量；激光雷达与相机的融合，研究已经较多，这里不作举例。同时融合三个传感器的算法暂时没看到。<br>
　　激光点云的处理方法主要有三种: 1. 将点云物理空间 3D Voxel 化处理；2. 直接在点云连续空间内进行点级别的学习；3. 将点云投影到 2D 空间，如 Bird-View，Apherical-View，Cylinder-View 等，然后作 2D 卷积处理。<br>
　　毫米波雷达数据 \(x,y,v\) 可表示为 2D 特征图，然后用 2D 卷积来处理；也可表示为点云的形式，然后用点云的操作来处理。</p>
<h2 id="how-to-fuse">2. How to Fuse</h2>
<p>　　考虑两个不同的传感器数据源 \(M _ i, M _ j\)，对应的第 \(l\) 层网络特征 \(f _ l ^ {M _ i}, f _ l ^ {M _ j}\)，以及操作 \( G _ l(\cdot)\)。融合方式有以下几种：</p>
<ul>
<li>Addition or Average Mean:<br>
将两个特征图相加或者取平均，\(f _ l=G _ {l-1}\left(f _ {l-1} ^ {M _ i}+f _ {l-1} ^ {M _ j}\right)\)。</li>
<li>Concatenation:<br>
将两个特征图在深度维度进行串联，\(f _ l=G _ {l-1}\left(f _ {l-1} ^ {M _ i}\frown f _ {l-1} ^ {M _ j}\right)\)。</li>
<li>Ensemble:<br>
在目标检测任务中，对 ROI 内的特征进行整合，\(f _ l=G _ {l-1}\left(f _ {l-1} ^ {M _ i}\right)\cup G _ {l-1}\left( f _ {l-1} ^ {M _ j}\right)\)。</li>
<li>Mixture of Experts:<br>
用 experts 网络预测带融合特征的权重，然后作权重融合，\(f _ l=G _ {l}\left(w ^ {M _ i}\cdot f _ {l-1} ^ {M _ i}+w ^ {M _ j}\cdot f _ {l-1} ^ {M _ j}\right)\)，其中 \(w ^ {M _ i}+ w ^ {M _ j} = 1\)。</li>
</ul>
<h2 id="when-to-fuse">3. When to Fuse</h2>
<p><img src="/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/fusion-methods.png" width="90%" height="90%" title="图 1. Fusion Methods"> 　　如图 1 所示，融合的时间点可分为 early，middle，late 三种，<strong>本文归纳发现并没有哪一种融合是最优的，这与传感器类型，数据，网络结构等相关</strong>。设融合操作为 \(f _ l = f _ {l-1} ^ {M _ i}\oplus f _ {l-1} ^ {M _ j}\)，那么各融合方式可归纳为：</p>
<ul>
<li><p>Early Fusion<br>
在传感器原始数据阶段进行数据融合: <span class="math display">\[f _ L = G _ L\left(G _ {L-1}\left(\dots G _ l\left(\dots G _ 2\left(G _ 1\left(f _ 0 ^ {M _ i}\oplus f _ 0 ^ {M _ j}\right)\right)\right)\right)\right)\tag{1}\]</span> 前融合的优势是深度整合传感器数据信息，理论上能挖掘最全的特征信息，以及计算量较小；劣势是模型灵活性较差，以及对多模态数据的空间对齐准确度非常敏感，其空间对齐的精度受传感器之间参数标定，采样频率，传感器缺陷等因素影响。</p></li>
<li><p>Late Fusion<br>
在网络输出后进行融合： <span class="math display">\[f _ L=G _ L ^ {M _ i}\left(G _ {L-1} ^ {M _ i}\left(\dots G _ 1 ^ {M _ i}(f _ 0 ^ {M _ i})\right)\right) \oplus G _ L ^ {M _ j}\left(G _ {L-1} ^ {M _ j}\left(\dots G _ 1 ^ {M _ j}(f _ 0 ^ {M _ j})\right)\right)\tag{2}\]</span> 后融合是模块化的，所以有很强的灵活性；但是需要较多的计算资源，以及没有在特征层面对数据进行融合，可能丧失一定的信息量。</p></li>
<li><p>Middle Fusion<br>
中融合变种非常多，如图 1. 所示，可以是 deep fusion 模式，也可以是 short-cut fusion 模式。网络结构上，还是比较难断定哪种结构是最优的。</p></li>
</ul>
<p><img src="/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/fusion-arch.png" width="100%" height="100%" title="图 2. Fusion Archtectures"> 　　对于目标检测任务来说，two-stage 方法基本都是在 ROI 内作特征融合，经典的方法如图 2. 所示，这里不做展开。</p>
<h2 id="datasets-methodology">4. Datasets &amp; Methodology</h2>
<p><img src="/Deep-Multi-modal-Object-Detection-and-Semantic-Segmentation/challenges.png" width="100%" height="100%" title="图 3. Challenges and Open Questions"> 　　如图 3. 所示，目前基于多传感器融合的感知主要挑战有：</p>
<ul>
<li><p>Multi-modal data preparation<br>
公开数据量及数据的多样性还较少，数据中多传感器的标定，标注准确性存疑。</p></li>
<li><p>Fusion Methodology<br>
"What to fuse" 中融合的传感器数据还较少，还可以融合超声波雷达，V2X 信息，物理模型，先验模型等；"How to fuse" 中目前都是简单的融合，或者说整合，缺少对信息源不确定性的估计(Uncertainty)，可以采用 BNN 对不确定性进行估计；"When to fuse" 中目前基本凭经验去寻找最优的网络融合结构，缺少理论指导。</p></li>
<li><p>Others<br>
评估指标上，还需进一步体现模型的鲁棒性；网络结构上，目前缺少时序融合。</p></li>
</ul>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges." IEEE Transactions on Intelligent Transportation Systems (2020).</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Multi-modal Fusion</tag>
      </tags>
  </entry>
  <entry>
    <title>Uncertainty Calibration</title>
    <url>/Uncertainty-Calibration/</url>
    <content><![CDATA[<p>　　If you don’t know the measurement uncertainty, don’t make the measurement at all!<a href="#1" id="1ref"><sup>[1]</sup></a><br>
　　Uncertainty 在自动驾驶测量中的重要性在之前的文章，如 <a href="/Perception-Uncertainty-in-Deep-Learning/" title="Perception Uncertainty in Deep Learning">Perception Uncertainty in Deep Learning</a> 中已经有较详细的阐述，这里不做赘述。但更重要的是，如何确保 Uncertainty 估计的准确性。如图 1. 所示，本文讨论如何评估 Uncertainty 估计的准确性，以及通过 Uncertainty Calibration 来修正其估计误差。<br>
<img src="/Uncertainty-Calibration/framework.png" width="100%" height="100%" title="图 1. Framework"></p>
<h2 id="uncertainty-estimation-for-object-detection">1. Uncertainty Estimation for Object Detection</h2>
<p>　　以目标检测任务为例，深度学习中的 Uncertainty 估计可分为两大类方法：Ensemble Approach，以及 Direct-modeling Approach。本文以 Direct-modeling 方法为例，假设网络输出符合多多变量高斯分布。对于 Anchor-Free 的 3D 目标检测，分类的预测量为目标类别分数 \(p(y _ c=1|) = s _ {\mathbf{x}}\)，回归预测量为： <span class="math display">\[\mathbf{u _ x} = [\mathrm{cos}(\theta), \mathrm{sin}(\theta), dx,dy,\mathrm{log}(l),\mathrm{log}(w)] \tag{1}\]</span> 假设回归输出量符合多变量独立高斯分布 \(p(\mathbf{y _ r}| \mathbf{x})=(\mathbf{u _ x,\Sigma _ x})\)，那么其协方差矩阵为对角矩阵： <span class="math display">\[\mathbf{\sigma _ x} ^ 2=[\sigma ^2 _ {\mathrm{cos}(\theta)}, \sigma ^2 _ {\mathrm{sin}(\theta)}, \sigma ^ 2 _ {dx}, \sigma ^ 2 _ {dy}, \sigma ^ 2 _ {\mathrm{log}(l)}, \sigma ^ 2 _ {\mathrm{log}(w)}] _ {\mathbf{x}} \tag{2}\]</span> 由此加入预测的 Uncertainty 分支，Loss 项为： <span class="math display">\[L _ {reg}=\frac{1}{2}(\mathbf{y _ r-u _ x})\mathrm{diag}(\frac{1}{\mathbf{\sigma ^ 2 _ x}})(\mathbf{y _ r-u _ x}) ^ T+\frac{1}{2}\mathrm{log}(\mathbf{\sigma ^ 2 _ x})\mathbf{1} ^ T \tag{3}\]</span></p>
<h2 id="uncertainty-evaluation">2. Uncertainty Evaluation</h2>
<p>　　对于数据集 \(\{(\mathbf{x} ^ n,y _ c ^ n,\mathbf{y} _ r ^ n)\} _ {n=1} ^ N\)，\(\mathbf{X}\) 表示输入数据，\(\mathbf{Y} _ c\) 表示分类标签，\(\mathbf{Y _ r}\) 表示回归标签。概率描述为：\(\mathbf{X, Y} _ c \sim \mathbb{P} _ c\)，以及 \(\mathbf{X,Y _ r}\sim\mathbb{P} _ r\)。对于分类问题，，softmax score 预测了目标分类的概率分布，即 \(\mathbf{F} _ c ^ n(y _ c=1)=p(y _ c=1|\mathbf{x} ^ n)=s _ {\mathbf{x} ^ n}\)。对于回归问题，网络预测了概率密度函数 PDF：\(p(\mathbf{y _ r} ^ n | \mathbf{x} ^ n) = \mathcal{N}(\mathbf{u _ {x ^ n},\Sigma _ {x ^ n}})\)，其累积概率分布函数 CDF 定义为 \(\mathbf{F} _ r ^ n(\mathbf{y} _ r)\)，反函数为 \(\mathbf{F} _ r ^ {n ^ {-1}}(p)\)。<br>
　　准确的不确定性预测意味着，预测的概率近似等于统计的频率。具体的：</p>
<ul>
<li><p>分类问题<br>
0.9 的分数意味着 90% 的物体是被分类准确的。对于 \(\forall p\in[0,1]\)，数学形式为： <span class="math display">\[\mathbb{P} _ c(\mathbf{Y} _ c=1|\mathbf{F} _ c(\mathbf{Y} _ c=1)=p)\approx \frac{\sum _ {n=1} ^ N\mathbb{1}(y _ c ^ n=1,F _ c ^ n(y _ c=1)=p)}{\sum _ {n=1} ^ N\mathbb{1}(F _ c ^ n(y _ c=1)=p)} \tag{4}\]</span></p></li>
<li><p>回归问题<br>
对于预测物体，其 90% 置信空间内，90% 的真值物体应该在置信空间内。对于 \(\forall p\in[0,1]\)，数学形式为： <span class="math display">\[\mathbb{P} _ r(\mathbf{Y} _ r\leq\mathbf{F} _ r ^ {-1}(p))\approx\frac{\sum _ {n=1} ^ N\mathbb{1}(y _ r ^ n\leq F _ r ^{n ^ {-1}}(p))}{N} \tag{5}\]</span></p></li>
</ul>
<p><img src="/Uncertainty-Calibration/cali-plot.png" width="60%" height="60%" title="图 2. Calibration Plot"> 　　由此可用 calibration plot 来刻画不确定性估计的准确性。如图 2. 所示，横坐标表示预测的概率，纵坐标表示统计的概率，将概率值划分为 \(0 &lt; p _ c ^ 1 &lt; ... &lt; p _ c ^ m &lt; ... &lt; 1\) 个置信区域，理想的 Calibration Plot 是对角线。计算该对角线与实际曲线的 Expected Calibration Error(ECE) 即可作为评估 Uncertainty 估计的准确性： <span class="math display">\[\mathrm{ECE} =\sum _ {m=1} ^ M\frac{N _ m}{N}\vert p ^ m-\hat{p} ^ m\vert\tag{6}\]</span></p>
<h2 id="uncertainty-recalibration">3. Uncertainty Recalibration</h2>
<p>　　为了使得 Calibration Plot 能完美贴合对角线，需要对 Uncertainty 进行标定。</p>
<h3 id="isotonic-regression">3.1. Isotonic Regression</h3>
<p>　　对于预测的累积概率分布函数 \(p=\mathbf{F} _ r(y _ r)\)，预测一个额外模型 \(g(p)\) 使得满足式 (5) 条件，该映射模型函数是非参单调递增的。额外模型通过 validation 数据集训练得到。</p>
<h3 id="temperature-scaling">3.2. Temperature Scaling</h3>
<p>　　对式 (2) 中的各方差作 \(T &gt; 0\) 的尺度变换：\(\hat{\sigma}\leftarrow\sigma ^ 2/T \)。最优的 \(T\) 通过最大化 Negative Log Likelihood(NLL) 实现，等价于最小化式 (3) 的 Loss 项。</p>
<h3 id="calibration-loss">3.3. Calibration Loss</h3>
<p>　　由式 (3) 可知，Variance 的预测是通过无监督的形式隐式来预测的，所以本质上就无法保证 Variance 的绝对正确性，所以可加入监督项来保证其正确性。因为一个准确的 Uncertainty 意味着预测的 Variance 与预测量和真值量的 Variance 是一致的，所以设计 calibration loss： <span class="math display">\[ L _ {calib} =\Vert \mathbf{\sigma _ x} ^ 2-(\mathbf{y _ r-u _ x})\odot (\mathbf{y _ r-u _ x})\Vert\tag{7}\]</span> 最终的 Loss 为： <span class="math display">\[L _ {total} = L _ {reg}+\lambda L _ {calib}\tag{8}\]</span></p>
<h2 id="recalibration-results">4. Recalibration Results</h2>
<p><img src="/Uncertainty-Calibration/calib.png" width="60%" height="60%" title="图 3. Calibration Plot After Recalibration"> 　　如图 3. 所示，经过标定后，预测的概率分布接近于实际统计分布。</p>
<p><img src="/Uncertainty-Calibration/pred.png" width="100%" height="100%" title="图 4. Predictions with Recalibration Uncertainties"> 　　图 4. 可视化了标定前后 Uncertainty 的准确程度，可见标定后，越是遮挡的目标，Uncertainty 越大，符合预期。此外，标定后目标检测的精度也有较大的提升。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "Can we trust you? on calibration of a probabilistic object detector for autonomous driving." arXiv preprint arXiv:1909.12358 (2019).<br>
<a id="2" href="#2ref">[2]</a> Kuleshov, Volodymyr, Nathan Fenner, and Stefano Ermon. "Accurate uncertainties for deep learning using calibrated regression." arXiv preprint arXiv:1807.00263 (2018).</p>
]]></content>
      <categories>
        <category>Uncertainty</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Panoptic Segmentation via Dynamic Shifting Network&quot;</title>
    <url>/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/</url>
    <content><![CDATA[<p>　　实例分割一般与语义分割同时进行，其难点是后处理如何准确的聚类出实例目标。目标聚类可归纳出的策略或方法有：</p>
<ul>
<li><strong>提高语义分割的准确率</strong>，在聚类的时候加入语义约束，能改善不同类别的欠分割，以及同类别的过分割；</li>
<li><strong>Spatial Offset &amp; Embedding Offset</strong>，提升聚类空间下的目标的聚集性，如 <a href="/paper-reading-OccuSeg/" title="OccuSeg">OccuSeg</a>，<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a>，<a href="/paper-reading-JSNet-JSIS3D/" title="JSNet, JSIS3D">JSNet, JSIS3D</a>；</li>
<li><strong>将聚类问题转换为每个点属于对应实例的概率问题</strong>，网络下 End-to-End 来优化聚类效果，如 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>；</li>
<li><strong>引入聚类时的 Bandwidth</strong>，不同尺寸的实例容忍不同程度的 Offset，如 <a href="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/" title="Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a>；</li>
</ul>
<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>属于引入聚类时的 Bandwidth 策略，对于大目标，聚类时自适应选择更大的 offset 容忍度，同时可调节点经过 offset 迭代至中心点的次数来提高聚类准确率。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，DS-Net 由 Cylinder-Convolution Backbone，Semantic Branch，Instance Branch 构成。<a href="/paper-reading-Cylinder3D/" title="Cylinder3D">Cylinder3D</a> 以及 <a href="/paper-reading-Pillar-based-Object-Detection/" title="[paper_reading]-" pillar-based object detection"">[paper_reading]-"Pillar-based Object Detection"</a> 已经较为详细得介绍了在 Cylinder 视野下的卷积过程，其相比 Spherical 和 Bird-eye 视野有一定的优势。Instance Branch 则由 Dynamic Shifting 以及 Consensus-driven Fusion 构成，以聚类实例目标。</p>
<h2 id="instance-branch">2. Instance Branch</h2>
<p>　　Instance Branch 预测每个实例目标的点 \(P\in\mathbb{R} ^ {M\times 3}\) 到实例中心 \(C _ {gt}\in\mathbb{R} ^ {M\times 3}\) 的 offset \(O\in\mathbb{R} ^ {M\times 3}\)。其 Loss 的基本形式为： <span class="math display">\[ L _ {ins} = \frac{1}{M}\sum _ {i=0} ^ M\Vert O[i]-(C _ {gt}[i]-P[i])\Vert \tag{1}\]</span> 其中 \(M\) 是实例目标的点个数，预测的回归中心点 \(O+P\) 可用于实例目标的聚类，一般通过 Heuristic Clustering 方法，本文则提出 Dynamic Shifting 方法。<br>
　　自底向上的 Heuristic Clustering 方法有 Breadth First Search(BFS)，DBSCAN，HDBSCAN，Mean Shift 等。<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a> 采用了 BFS 方法，对于点云这种密度不一样的数据形式，固定的搜索半径是不太合理的，小的搜索半径容易过分割，大的搜索半径则容易欠分割。DBSCAN/HDBSCAN 是 density-based 方法，所以与 BFS 一样，对密度不一致的点云聚类效果不好。Mean Shift 对密度不一致的点云聚类更加友好，但是固定的 bandwidth 也不是一个好的选择。</p>
<h3 id="dynamic-shifting">2.1. Dynamic Shifting</h3>
<p>　　对于待聚类的点云集 \(X\in\mathbb{R} ^ {M\times 3}\)，预测的实例中心由回归的 offset \(S\in\mathbf{R} ^ {M\times 3}\) 与点云坐标计算得到： <span class="math display">\[X\leftarrow X + \eta S\tag{2}\]</span> offset \(S\) 的计算可定义为 \(S=f(X)-X\)，其中 \(f(\cdot)\) 为核函数。一种简单的平面核函数为： <span class="math display">\[f(X)=D ^ {-1} KX \tag{3}\]</span> 其中 \(K=(XX ^ T\leq \delta)\) 表示点周围 bandwidth \(\delta\) 区域的点集；\(D=diag(K\mathbf{1})\) 表示 \(\delta\) 内点集个数。<br>
　　为了自适应不同的 bandwidth，设计 \(l\) 个候选 bandwidth \(L=\{\delta _ 1,\delta _ 2,...,\delta _ l\}\)。对候选 bandwidth 作权重化处理，权重通过 MLP，Softmax 得到 \(\sum _ {j=1} ^ l W[:,j] = \mathbf{1}\)。最终的核函数为： <span class="math display">\[\hat{f}(X) = \sum _ {j=1} ^ l W[:,j]\odot(D _ j ^ {-1}K _ j X) \tag{4}\]</span> 其中 \(K _ j=(XX ^ T\leq\delta _ j)\), \(D _ j=diag(K _ j\mathbf{1})\)。 <img src="/paper-reading-Panoptic-Segmentation-via-Dynamic-Shifting-Network/DSM.png" width="60%" height="60%" title="图 2. Dynamic Shifting Module"> 　　DSM 算法流程如图 2. 所示，种子点通过 offset 预测实例中心点迭代 \(I\) 次，第 \(i\) 次回归的 Loss 为： <span class="math display">\[l _ i=\frac{1}{M &#39;}\sum _ {x=1} ^ {M &#39;}\Vert X _ i[x]-C &#39; _ {gt}[x]\Vert _ 1 \tag{5}\]</span> 总的 Loss 为： <span class="math display">\[L _ {ds} = \sum _ {i=1} ^ I w _ i l _ i \tag{6}\]</span> 其中 \(w _ i\) 为权重，设为 1。</p>
<h3 id="consensus-driven-fusion">2.2. Consensus-driven Fusion</h3>
<p>　　最终实例的类别由点集合投票决定。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Hong, Fangzhou, et al. "LiDAR-based Panoptic Segmentation via Dynamic Shifting Network." arXiv preprint arXiv:2011.11964 (2020).</p>
]]></content>
      <categories>
        <category>Segmentation</category>
        <category>Instance Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Instance Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;CenterFusion&quot;</title>
    <url>/paper-reading-CenterFusion/</url>
    <content><![CDATA[<p>　　自动驾驶领域多传感器融合对感知及定位都非常重要，对于感知而言，融合可分为数据前融合，特征级融合，目标状态后融合等类型。前融合对外参标定要求较高，后融合没有深度融合各传感器特征，特征级融合是比较折中的方法。<a href="/MOT-Fusion/" title="MOT Multimodal Fusion">MOT Multimodal Fusion</a> 中介绍了基于 BCM 来实现目标状态后融合的方法，本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出一种特征级融合相机以及毫米波雷达数据的目标检测方法。<br>
　　<a href="/paper-reading-RadarNet/" title="RadarNet">RadarNet</a> 中详细介绍了毫米波雷达与激光雷达的优劣势，并提出了一种特征级融合方法，能更准确的测量目标的速度。此外，相机与激光雷达一样对恶劣天气环境比较敏感，所以毫米波与相机的结合，能有效利用毫米波能测量目标径向速度，应对恶劣环境以及测量范围较远等优势，并且保留相机高分辨率捕捉环境视觉信息的特性。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-CenterFusion/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，CenterFusion 网络首先用 CenterNet 作图像的 3D 目标检测，然后通过 Frustum Association Module 提取并融合对应的图像特征以及毫米波雷达特征，最后通过网络进一步准确估计目标的 3D 属性。<br>
　　<a href="/Anchor-Free-Detection/" title="Anchor Free Detection">Anchor Free Detection</a> 以及 <a href="/CenterTrack/" title="CenTrack">CenTrack</a> 已经较为详细的介绍了 CenterNet 的网络结构和 Loss 形式，其真值 Heatmap 生成方式与 <a href="/paper-reading-AFDet/" title="AFDet">AFDet</a> 也类似，这里不做展开。<br>
<img src="/paper-reading-CenterFusion/vel.png" width="60%" height="60%" title="图 2. Radial Velocity"> 　　目前广泛使用的 3D 毫米波雷达测量的量有 \(x,y,v\)，如图 2. 所示，其中 \(v\) 是径向速度，为目标实际速度在径向的投影。为了准确估计目标的实际速度，需要估计目标的运动方向。</p>
<h2 id="association-and-feature-fusion">2. Association and Feature Fusion</h2>
<p>　　图像经过 CenterNet 得到目标的 2D size, Center Offset 等 2D 属性，以及 dimensions，depth, rotation 等 3D 属性。接下来要将 CenterNet 得到的目标与毫米波雷达的测量量进行数据关联，以便作进一步的特征融合与属性估计。</p>
<h3 id="frustum-association-mechanism">2.1. Frustum Association Mechanism</h3>
<p>　　基于图像的目标检测结果与毫米波雷达关联最简单的方法是将毫米波的测量点投影到图像中，看其是否处于图像 2D 框内。但是毫米波雷达测测量量没有 \(z\) 信息，所以这种方式不准确。<br>
<img src="/paper-reading-CenterFusion/asso.png" width="90%" height="90%" title="图 3. Frustum Association"> 　　本文提出一种锥形关联方法，如图 3. 所示，在俯视三维坐标下的锥形中进行关联，其中 \(\sigma\) 用来控制感兴趣锥形的尺寸，因为基于图像的目标 depth 估计准确度较差，所以 \(\sigma\) 可用来调节 depth 范围。图像目标只关联距离坐标原点最近的毫米波测量量。</p>
<h3 id="radar-feature-fusion">2.2. Radar Feature Fusion</h3>
<p>　　 当图像 2D 目标框与毫米波雷达测量量关联上后，将毫米波雷达的测量信息融合到 2D 框中。具体的，在 2D 目标框提取出的图像特征上，concate 毫米波雷达测量的 heatmap。heatmap 尺寸与 2D 目标框尺寸相关，heatmaps 定义为： <span class="math display">\[ F ^ j _ {x,y,i} = \frac{1}{M _ i}
\left\{\begin{array}{l}
f _ i \;\;\vert x - c _ x ^ j\vert \leq \alpha w ^j \;\mathrm{and}\;\vert y- c _ y ^ i\vert\leq\alpha h ^ j\\
0 \;\;\mathrm{otherwise}
\end{array}\tag{1}\right.\]</span> 其中 \(\alpha\) 是 heatmap 尺寸比例；\(i\in 1,2,3\) 是 heatmaps 特征维度；\(M _ i\) 是归一化系数；\(f _ i\in d, v _ x, v _ y\) 为毫米波雷达的测量量。<br>
　　有了目标框内融合的图像及毫米波雷达特征后，可进一步精确估计目标的位置，朝向，速度，尺寸等 3D 属性。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Nabati, Ramin, and Hairong Qi. "CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection." arXiv preprint arXiv:2011.04841 (2020).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
        <category>Fusion</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Radar</tag>
        <tag>Fusion</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth&quot;</title>
    <url>/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/</url>
    <content><![CDATA[<p>　　基于点云的 Instance Segmentation 方法之前已经介绍过几种，其中将点云在 Bird-View 进行 Instance Segmentation 的思路基本与图像 Instance Segmentation 相似。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种图像 Instance Segmentation 方法，其能处理各种尺寸的目标，以及不用做聚类后处理，可直接得到目标实例。由于能处理较大尺寸的目标，所以也能应用于车道线检测领域，其思路值得借鉴。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-Instance-Segmentation-by-Jointly-Optimizing-Spatial-Embeddings-and-Clustering-Bandwidth/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，网络输出 Seed Branch 以及 Instance Branch。Seed Branch 中分数较高的表示每个类别每个实例的中心点，中心点具体坐标由该像素坐标以及对应的 offset 预测值决定。<br>
　　Instance Branch 输出 offset vectors 以及 sigma maps。offset vectors 表示该像素点指向的对应实例的中心位置；sigma maps 表示 offset 指向中心的宽松度，是本方法能预测较大尺寸目标的关键。</p>
<h2 id="loss">2. Loss</h2>
<p>　　Instance Segmentation 的目标是将一堆二维像素点 \(\mathcal{X}=\{x _ 0, x _ 1, ..., x _ N\}\)，聚类成实例 \(\mathcal{S} = \{S _ 0, S _ 1, ..., S _ K\}\)。</p>
<h3 id="instance-branch">2.1. Instance Branch</h3>
<p>　　传统的做法是将每个像素点 \(x _ i\) 回归其与对应实例中心 \(C _ k=\frac{1}{N}\sum _ {x\in S _ k} x\) 的 offset 向量 \(o _ i\)，得到的 \(e _ i=x _ i+o _ i\) 即为该像素点指向的实例中心点。Loss 设计为： <span class="math display">\[\mathcal{L} _ {regr} = \sum _ {i=1} ^ n\Vert o _ i-\hat{o} _ i\Vert\tag{1}\]</span> 其中 \(\hat{o} _ i=C _ k-x _ i\)。<br>
　　因为 \(e _ i\) 很难正好指向实例中心点，所以引入 hinge loss，让其指向实例中心周围 \(\sigma\) 范围区域： <span class="math display">\[\mathcal{L} _ {hinge} = \sum _ {k=1} ^ K\sum _ {e _ i\in S _ k}\mathrm{max}(\Vert e _ i-C _ k\Vert-\sigma, 0) \tag{2}\]</span> 从而保证： <span class="math display">\[e _ i\in S _ k \iff \Vert e _ i-C _ k\Vert &lt; \sigma \tag{3}\]</span> 但是这种方法需要根据最小目标来选择 \(\sigma\) 值，对于大目标，选取的 \(\sigma\) 又不太合理。<br>
　　为了选取的 \(\sigma\) 能处理不同尺寸的实例目标，本文设计网络输出 sigma maps，在以实例中心为高斯概率分布下，每个像素属于该实例的概率为： <span class="math display">\[\phi _ k(e _ i) = \mathrm{exp}\left(-\frac{\Vert e _ i-C _ k\Vert ^ 2}{2\sigma _ k ^ 2}\right)\tag{4}\]</span> 当 \(\phi _ k(e _ i) &gt; 0.5\) 时，表示该像素点属于该实例。即： <span class="math display">\[e _ i\in S _ k \iff \mathrm{exp}\left(-\frac{\Vert e _ i-\hat{C} _ k\Vert ^ 2}{2\hat{\sigma} _ k ^ 2}\right) &gt; 0.5 \tag{5}\]</span> 由此像素回归指向中心点的区域可由 \(\sigma\) 控制： <span class="math display">\[\mathrm{margin} = \sqrt{-2\sigma _ k ^ 2\mathrm{ln}0.5} \tag{6}\]</span> 进一步得，可将 \(\sigma\) 分解为两个方向的值，形成椭圆状的二维高斯分布，这样能适应狭长型的目标： <span class="math display">\[\phi _ k(e _ i) = \mathrm{exp}\left(-\frac{\Vert e _ {ix}-C _ {kx}\Vert ^ 2}{2\sigma _ {kx} ^ 2}-\frac{\Vert e _ {iy}-C _ {ky}\Vert ^ 2}{2\sigma _ {ky} ^ 2}\right)\tag{7}\]</span> 以及，可将实例中心点像素坐标向量用特征向量代替： <span class="math display">\[\phi _ k(e _ i) = \mathrm{exp}\left(-\frac{\Vert e _ i-\frac{1}{\vert S _ k\vert}\sum _ {e _ j\in S _ k}e _ j\Vert ^ 2}{2\sigma _ k ^ 2}\right)\tag{8}\]</span> \(\sigma _ k\) 定义为： <span class="math display">\[\sigma _ k=\frac{1}{\vert S _ k\vert}\sum _ {\sigma _ i\in S _ k}\sigma _ i\tag{9}\]</span> 　　采用 Lovase-hinge loss 作用于像素点属于对应实例的概率图，\(\sigma\) 通过概率图隐式地学到。以及为了保证式 (9) 的一致性，增加 \(\sigma\) 平滑 Loss： <span class="math display">\[\mathcal{L} _ {smooth}=\frac{1}{\vert S _ k\vert}\sum _ {\sigma _ i\in S _ k}\Vert\sigma _ i-\sigma _ k\Vert ^ 2\tag{10}\]</span></p>
<h3 id="seed-branch">2.2. Seed Branch</h3>
<p>　　实例中心点的预测采用回归方法，背景点标签为零，前景标签是以实例中心为原点的高斯分布。Loss 设计为： <span class="math display">\[\mathcal{L} _ {seed} = \frac{1}{N}\sum _ i ^ N\mathbb{1} _ {\{s _ i\in S _ k\}}\Vert s _ i-\phi _ k(e _ i)\Vert ^ 2+\mathbb{1} _ {\{s _ i\in\mathbf{bg}\}}\Vert s _ i-0\Vert ^ 2 \tag{11}\]</span> 此时 \(\phi _ k(e _ i)\) 不作梯度反传。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> even, Davy, et al. "Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
]]></content>
      <categories>
        <category>Segmentation</category>
        <category>Instance Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Instance Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Perception Uncertainty in Deep Learning</title>
    <url>/Perception-Uncertainty-in-Deep-Learning/</url>
    <content><![CDATA[<p>　　原文发布于 <a href="https://mp.weixin.qq.com/s/cSWIk7i2UhxqdadMDH_lmQ" target="_blank" rel="noopener">Deeproute 招聘公众号</a>。<br>
　　基于深度学习感知技术的发展极大推动了自动驾驶行业的落地，然而基于深度学习的感知技术还存在很多问题，比如针对经典的目标检测任务，算法无法保证在所有场景下做到 100% 的检测准确率。由此，针对 L3/L4 自动驾驶，产品落地只能寄期望于 ODD(operational design domain) 的精心设计。要实现真正意义上的 L4/L5 自动驾驶，精准的环境感知很重要，但更重要的是，算法能否给出当前传感器数据及模型感知的不确定性(uncertainty)。换句话说，我们期望算法模型知道什么，但更期望算法模型不知道什么。<br>
　　另一方面，毫米波雷达，相机，激光雷达等传感器各有优势，比如毫米波雷达能应对下雨等恶劣环境，相机能捕捉更丰富的语义信息，激光雷达能精确测量物理信息。所以多传感器融合是 L4/L5 自动驾驶的基础，而传感器及模型的不确定性估计，则又是多传感器后融合的基础。以下以多目标状态估计任务为例，来描述不确定的作用及估计方式。</p>
<h2 id="基于多传感器的目标状态估计">1. 基于多传感器的目标状态估计</h2>
<p>　　这里考虑基于毫米波雷达，相机，激光雷达等三种传感器的目标状态估计后融合方案。假设 3D 毫米波雷达测量的目标属性为 \((x, y, v)\)；基于深度学习的相机与激光雷达测量的目标属性均为 \((x, y, z, l, w, h, \theta)\)。经过前后多目标的数据关联后，这些量通过 KF 或 UKF 进行融合，最终得到目标状态的鲁棒估计。以最简单的 KF 为例，融合的过程需要测量量以及测量的方差。这里的方差就是我们要讨论的不确定性，即假设测量满足高斯分布下对应的方差。<br>
　　传统的做法是将测量方差设为经验固定值，但是在多传感器融合框架里，这样无法区分 1) 单传感器在不同场景下算法模型的性能；2) 多传感器在同一场景下算法模型的性能。由此，基于深度学习的感知测量输出需要同时估计测量值的不确定性，以此作更鲁棒的状态估计。</p>
<h2 id="不确定性概述">2. 不确定性概述</h2>
<p>　　不确定性由传感器及模型产生。传感器方面，比如激光雷达的点云测量存在厘米级别的误差。模型方面，在贝叶斯框架下，可以建模两大类不确定性:</p>
<ul>
<li>认知不确定性(Epistemic Uncertainty)，描述模型因为缺少训练数据而存在的未知，可通过增加训练数据解决；</li>
<li>偶然不确定性(Aleatoric Uncertainty)，描述数据不能解释的信息，可通过提高数据的精度来消除，与传感器相关；
<ul>
<li>数据依赖/异方差不确定性(Data-dependent/Heteroscedastic Uncertainty)，与模型输入数据有关，可作为模型预测输出；</li>
<li>任务依赖/同方差不确定性(Task-dependent/Homoscedastic Uncertainty)，与模型输入数据无关，且不是模型的预测输出，不同任务有不同的值；</li>
</ul></li>
</ul>
<p>　　认知不确定性多应用于 Active Learning，数据可标注性等领域，能有效挖掘提升模型感知能力的数据。同方差不确定性多应用于神经网络的多任务学习，能根据各个任务对应的数据方差学习其损失函数权重，提升所有任务的整体性能。异方差不确定性是反映在线网络预测量是否可信的主要不确定性。</p>
<h2 id="贝叶斯深度学习中不确定性的数学描述">3. 贝叶斯深度学习中不确定性的数学描述</h2>
<p>　　针对一批训练数据集\(\{\mathbf{X,Y}\}\)，训练模型 \(\mathbf{y=f^W(x)}\)，在贝叶斯框架下，预测量的后验分布为： <span class="math display">\[p\left(\mathbf{y\vert x,X,Y}\right) = \int p\left(\mathbf{y\,|\,f^W(x)}\right) p\left(\mathbf{W\,|\,X,Y}\right)d\mathbf{W} \tag{1}\]</span> 其中 \(p(\mathbf{W\,|\,X,Y})\) 为模型参数的后验分布，描述了模型的不确定性，即认知不确定性；\(p\left(\mathbf{y\,|\,f^W(x)}\right)\) 为观测似然，描述了观测不确定性，即偶然不确定性。</p>
<h3 id="认知不确定性">3.1. 认知不确定性</h3>
<p>　　认知不确定性表征的是模型的不确定性，对于训练集 \(\mathrm{D}=\{\mathbf{X},\mathbf{Y}\}\)，认知不确定性即为权重参数的分布 \(p(\mathbf{\omega} | \mathbf{X},\mathbf{Y})\)。可采用 Monte-Carlo 采样方法来近似估计模型权重分布: <span class="math display">\[p(\omega|\mathbf{X},\mathbf{Y})\approx q(\mathbf{\omega};\mathbf{\Phi})=Bern(\mathbf{\omega};\mathbf{\Phi}) \tag{2}\]</span> 其中 \(\mathbf{\Phi}\) 是 Bernolli Rates，具体的采样通过 Dropout 实现。在训练阶段，Dropout 等价于优化网络权重 \(W\) 的 Bernoulli 分布；在测试阶段，使用 Dropout 对样本进行多次测试，能得到模型权重的后验分布，由此模型的不确定性即为 T 次采样的方差： <span class="math display">\[\mathbf{Var} _ {p(\mathbf{y}|\mathbf{x})} ^ {model}(\mathbf{y})=\sigma _ {model} = \frac{1}{T}\sum _ {t=1} ^ T(\mathbf{y} _ t-\bar{\mathbf{y}}) ^ 2\tag{3}\]</span> 其中 \(\{\mathbf{y} _ t\} _ {t=1} ^ T\) 是不同权重 \(\omega ^ t\sim q(\omega;\mathbf{\Phi})\) 采样下的输出。</p>
<p>　　这种模型不确定性的计算方式，直观的理解为：当模型对某些数据预测比较好，误差比较小的时候，那么模型对这些数据的冗余度肯定是较高的，所以去掉模型的一部分网络，模型对这些数据的预测与原模型应该会有较高的一致性，即不确定性会较小。</p>
<h3 id="偶然不确定性">3.2. 偶然不确定性</h3>
<p>　　偶然不确定性估计是最大化高斯似然过程。对于回归任务，定义模型输出为高斯分布： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \mathcal{N}\left(\mathbf{f^W(x)}, \sigma ^2\right) \tag{4}\]</span> 其中 \(\sigma\) 为观测噪声方差，描述了模型输出中含有多大的噪声。对于分类任务，玻尔兹曼分布下的模型输出概率分布为： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)},\sigma\right) = \mathrm{Softmax}\left(\frac{1}{\sigma ^2}\mathbf{f^W(x)}\right) \tag{5}\]</span> 由此对于多任务，模型输出的联合概率分布为： <span class="math display">\[p\left(\mathbf{y}_1,\dots,\mathbf{y}_K\vert\mathbf{f^W(x)}\right) = p\left(\mathbf{y}_1\vert\mathbf{f^W(x)}\right) \dots p\left(\mathbf{y}_K\vert\mathbf{f^W(x)}\right) \tag{6}\]</span></p>
<p>　　对于回归任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) \propto -\frac{1}{2\sigma ^2} \Vert \mathbf{y-f^W(x)} \Vert ^2 - \mathrm{log}\sigma \tag{7}\]</span> 对于分类任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}=c\vert\mathbf{f^W(x)}, \sigma\right) = \frac{1}{2\sigma ^2}f_c^{\mathbf{W}}(\mathbf{x})- \mathrm{log}\sum_{c&#39;} \mathrm{exp}\left(\frac{1}{\sigma^2}f^{\mathbf{W}}_{c&#39;}(\mathbf{x}) \right) \tag{8}\]</span></p>
<p>　　最大化高斯似然，等价于最小化其负对数似然函数。现同时考虑回归与分类任务，则多任务的联合 \(Loss\)： <span class="math display">\[\begin{align}
\mathcal{L}(\mathbf{W}, \sigma _1, \sigma _2) &amp;= -\mathrm{log}p\left(\mathrm{y_1,y_2}=c\vert\mathbf{f^W(x)} \right) \\
&amp;= -\mathrm{log}\mathcal{N}\left(\mathbf{y_1};\mathbf{f^W(x)}, \sigma_1^2\right) \cdot \mathrm{Softmax}\left(\mathbf{y_2}=c;\mathbf{f^W(x)},\sigma_2\right) \\
&amp;= \frac{1}{2\sigma_1^2}\Vert \mathbf{y}_1-\mathbf{f^W(x)}\Vert ^2 + \mathrm{log}\sigma_1 - \mathrm{log}p\left(\mathbf{y}_2=c\vert\mathbf{f^W(x)},\sigma_2\right) \\
&amp;= \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\frac{\sum_{c&#39;}\mathrm{exp}\left(\frac{1}{\sigma_2^2}f_{c&#39;}^{\mathbf{W}}(x)\right)}{\left(\sum_{c&#39;}\mathrm{exp}\left(f_{c&#39;}^{\mathbf{W}}(x) \right) \right)^{\frac{1}{\sigma_2^2}}} \\
&amp;\approx \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\sigma_2 \tag{9}
\end{align}\]</span></p>
<p>由此可见，分类及回归的偶然不确定性估计，可通过额外预测对应的方差，并将方差通过上式作用于损失函数实现。实际应用中，为了数值稳定，可令 \(s:=\mathrm{log}\sigma^2\)。</p>
<h2 id="不确定性估计方法">4. 不确定性估计方法</h2>
<h3 id="统计法">4.1. 统计法</h3>
<p>　　观测量的方差(不确定性)与目标的属性有关，如距离，遮挡，类别等。可以按照不同属性，统计不同的方差。这种统计出来的方差实际上就是在特定传感器精度下，标注的不确定性，比如随着距离越远点云越稀少，标注误差也会越大。这样统计出来的方差与实际网络输出的不确定性不是等价的，但是模型训练好后，模型预测的分布是与训练集分布是相似的，所以用训练集的方差来直接代替模型预测的方差也合理。<br>
　　但是更准确的来说，不确定性对每个目标都应该是不同的，这里只统计了特定属性以及标注误差所产生的不确定性，而实际上遮挡大的目标，是更难学习的(目标学习有难易之分，即预测分布与训练集分布会有偏差)，即预测结果会有额外的不确定性，所以这种离线统计方法也有很大的局限性。</p>
<h3 id="网络分支预测法">4.2. 网络分支预测法</h3>
<p>　　假设网络输出符合多变量混合高斯分布，将偶然不确定性设计为网络的输出， <span class="math display">\[\left\{\begin{array}{l}
p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \sum_k \alpha_k \mathcal{N}\left(\mathbf{f^W(x)}_{(k)}, \Sigma(\mathbf{x})_{(k)} \right)\\
\sum_k \alpha_k = 1
\end{array}\tag{10}\right.\]</span> 　　对于 3D Detection 问题，网络输出的 3D 框参数为 \(\mathbf{y}=(x,y,z,l,h,w,\theta)\)，当输出满足 \(K\) 个混合高斯分布时，网络的输出量有：</p>
<ul>
<li>\(K\) 组目标框参数预测量 \(\{\mathbf{y}_k\}\)；</li>
<li>\(K\) 个对数方差 \(\{s_k\}\)；</li>
<li>\(K\) 个混合高斯模型权重参数 \(\{\alpha_k\}\)；</li>
</ul>
<p>　　训练时，找出与真值分布最近的一组预测量，混合高斯模型权重用分类问题的 Loss 求解，找到最相似的分布后，将该分布的方差用式 (9) 作用于回归的 Loss 项；测试时，找到混合高斯模型最大的权重项，对应的高斯分布，即作为最终的输出分布。这里只考虑了输出 3D 框的一个整体的方差，也可以输出定位方差+尺寸方差+角度方差，只要将该方差作用于对应的 Loss 项即可。当 \(K=1\) 时，就是多变量单高斯模型。</p>
<h3 id="assumed-density-filteringadf-估计法">4.3. Assumed Density Filtering(ADF) 估计法</h3>
<p>　　假设传感器得到的数据符合噪音水平 \(\mathbf{v}\) 的高斯分布，那么输入网络的数据 \(\mathbf{z}\) 与其真实数据 \(\mathbf{x}\) 的关系为： <span class="math display">\[q(\mathbf{z}|\mathbf{x})\sim \mathcal{N}(\mathbf{z};\mathbf{x},\mathbf{v})\tag{11}\]</span> 为了计算网络预测量的不确定性，通过 Assumed Density Filtering(ADF) 来传递输入数据的噪声，从而计算模型的偶然不确定性。网络的联合概率分布为： <span class="math display">\[p(\mathbf{z}^{(0:l)})=p(\mathbf{z}^{(0)})\prod _ {i=1} ^ l p(\mathbf{z}^{(i)}|\mathbf{z} ^ {(i-1)}) \tag{12}\]</span> 其中： <span class="math display">\[p(\mathbf{z}^{(i)}|\mathbf{z}^{(i-1)})=\sigma[\mathbf{z} ^ {(i)}-\mathbf{f} ^ {(i)}(\mathbf{z}^{(i-1)})]\tag{13}\]</span> ADF 将其近似为： <span class="math display">\[p(\mathbf{z}^{(0:l)})\approx q(\mathbf{z}^{(0:l)})=q(\mathbf{z}^{(0)})\prod _ {i=1} ^ l q(\mathbf{z}^{(i)}) \tag{14}\]</span> 其中 \(q(\mathbf{z})\) 符合独立高斯分布： <span class="math display">\[q(\mathbf{z}^{(i)})\sim \mathcal{N}\left(\mathbf{z}^{(i)};\mathbf{\mu}^{(i)},\mathbf{v}^{(i)}\right)=\prod _ j\left(\mathbf{z} _ j ^ {(i)};\mathbf{\mu} _ j ^ {(i)}, \mathbf{v} _ j ^ {(i)}\right)\tag{15}\]</span> 特征 \(\mathbf{z}^{(i-1)}\) 通过第 \(i\) 层映射方程 \(\mathbf{f} ^{(i)}\)，得到： <span class="math display">\[\hat{p}(\mathbf{z}^{(0:i)})=p(\mathbf{z}^{(i)}|\mathbf{z}^{(i-1)})q(\mathbf{z}^{(0:i-1)}) \tag{16}\]</span> ADF 的目标是找到 \(\hat{p}(\mathbf{z}^{(0:i)})\) 的近似分布 \(q(\mathbf{z}^{(0:i)})\)，比如 KL divergence： <span class="math display">\[q(\mathbf{z}^{(0:i)})=\mathop{\arg\min}\limits _ {\hat{q}(\mathbf{z}^{(0:i)})}\mathbf{KL}\left(\hat{q}(\mathbf{z}^{(0:i)})\;\Vert\;\hat{p}(\mathbf{z}^{(0:i)})\right)\tag{17}\]</span> 基于高斯分布的假设，误差通过每层的映射方程 \(\mathbf{f} ^{(i)}\) 进行独立传播，故以上解为： <span class="math display">\[\begin{align}
\mathbf{\mu}^{(i)}=\mathbb{E} _ {q(\mathbf{z}^{(i-1)})}[\mathbf{f}^{(i)}(z^{(i-1)})]\\
\mathbf{v}^{(i)}=\mathbb{V} _ {q(\mathbf{z}^{(i-1)})}[\mathbf{f}^{(i)}(z^{(i-1)})]\\
\end{align}\tag{18}\]</span> 以映射方程 \(\mathbf{f} ^{(i)}\) 为卷积层为例，均值传递就是正常的卷积操作，方差传递则需要将卷积权重平方，然后作卷积操作。其它神经网络层都可推导出对应的方差传递方程。<br>
　　结合蒙特卡洛采样计算认知不确定性与 ADF 方法计算偶然不确定性，网络预测的结果与对应的总的不确定性可计算为： <span class="math display">\[\left\{\begin{array}{l}
\mu = \frac{1}{T}\sum _ {t=1} ^ T \mathbf{\mu} _ t ^ {(l)}\\
\sigma _ {tot} = \frac{1}{T}\sum _ {t=1} ^ {T} \mathbf{v} _ t ^ {(l)} + \frac{1}{T}\sum _ {t=1} ^ T\left(\mathbf{\mu} _ t ^ {(l)}-\bar{\mathbf{\mu}}\right) ^ 2
\end{array}\tag{19}\right.\]</span> 其中 \(\{\mathbf{\mu} _ t ^ {(l)},\mathbf{v} _ t ^ {(l)}\} _ {t=1} ^ T\) 是 ADF 网络 \(T\) 次蒙特卡洛采样结果。<strong>由此可见，不同于以往将认知不确定性和偶然不确定性完全作独立假设的方式，本方法是将二者联合来估计的。这也比较好理解，如果数据噪音很大，那么模型就很难训，其模型不确定性也会很大，所以二者不可能是完全独立的</strong>。该方法可归纳为：</p>
<ol start="0" type="1">
<li>以正常方式训练网络；</li>
<li>将现有的网络转换为 ADF 网络形式，增加每层的方差传递函数；</li>
<li>计算 \(T\) 次蒙特卡洛采样的网络输出；</li>
<li>计算网络预测的均值和方差。</li>
</ol>
<h2 id="总结">5. 总结</h2>
<p>　　本文以多传感器目标检测后融合任务出发，介绍了基于深度学习的感知不确定性估计方法。需要注意的是，一般情况下，同一传感器同一模型的不同预测量之间不确定性的相对大小才有意义。所以进行多传感器融合时，需要对不同模型估计的不确定性进行幅值标定，这里不作展开。</p>
<h2 id="参考文献">6. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Kendall, Alex, and Yarin Gal. "What uncertainties do we need in bayesian deep learning for computer vision?." Advances in neural information processing systems. 2017.<br>
<a id="2" href="#2ref">[2]</a> Gal, Yarin. Uncertainty in deep learning. Diss. PhD thesis, University of Cambridge, 2016.<br>
<a id="3" href="#1ref">[3]</a> Loquercio, Antonio , Segù, Mattia, and D. Scaramuzza . "A General Framework for Uncertainty Estimation in Deep Learning." (2019).<br>
<a id="4" href="#1ref">[4]</a> Kendall, Alex, Yarin Gal, and Roberto Cipolla. "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</p>
]]></content>
      <categories>
        <category>Uncertainty</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning for 3D Point Clouds</title>
    <url>/Deep-Learning-for-3D-Point-Clouds/</url>
    <content><![CDATA[<p>　　本文介绍三篇点云相关的综述文章，并作归纳分析。<a href="#1" id="1ref">[1]</a> 的目录结构为：</p>
<ul>
<li>3D Shape Classification
<ul>
<li>Multi-view based Methods</li>
<li>Volumetric-based Methods</li>
<li>Point-based Methods
<ul>
<li>Pointwise MLP Methods</li>
<li>Convolution-based Methods</li>
<li>Graph-based Methods</li>
<li>Hierarchical Data Structure-based Methods</li>
<li>Other Methods</li>
</ul></li>
</ul></li>
<li>3D Object Detection and Tracking
<ul>
<li>3D Object Detection
<ul>
<li>Region Proposal-based Methods</li>
<li>Single Shot Methods</li>
</ul></li>
<li>3D Object Tracking</li>
<li>3D Scene Flow Estimation</li>
</ul></li>
<li>3D Point Cloud Segmentation
<ul>
<li>3D Semantic Segmentation
<ul>
<li>Projection-based Methods</li>
<li>Discretization-based Methods</li>
<li>Hybrid Methods</li>
<li>Point-based Methods</li>
</ul></li>
<li>Instance Segmentation
<ul>
<li>Proposal-based Methods</li>
<li>Proposal-free Methods</li>
</ul></li>
<li>Part Segmentation</li>
</ul></li>
</ul>
<p><a href="#2" id="2ref">[2]</a> 的目录结构为：</p>
<ul>
<li>Classification
<ul>
<li>Projection-based Methods
<ul>
<li>Multi-view Representation</li>
<li>Volumetric Representation</li>
<li>Basis Point Set</li>
</ul></li>
<li>Point-based Methods
<ul>
<li>MLP Networks</li>
<li>Convolutional Networks</li>
<li>Graph Networks</li>
<li>Other Networks</li>
</ul></li>
</ul></li>
<li>Segmentation
<ul>
<li>Semantic Segmentation
<ul>
<li>Projection-based Methods</li>
<li>Point-based Methods</li>
</ul></li>
<li>Instance Segmentation
<ul>
<li>Proposal-based Methods</li>
<li>Proposal-free Methods</li>
</ul></li>
</ul></li>
<li>Detection, Tracking and Flow Estimation
<ul>
<li>Object Detection
<ul>
<li>Projection-based Methods</li>
<li>Point-based Methods</li>
<li>Multi-view Methods</li>
</ul></li>
<li>Object Tracking</li>
<li>Scene Flow Estimation</li>
</ul></li>
<li>Registration
<ul>
<li>Traditional Methods</li>
<li>Learning-based Methods</li>
</ul></li>
<li>Augmentation and Completion
<ul>
<li>Discriminative Methods</li>
<li>Generative Methods</li>
</ul></li>
</ul>
<p><a href="#3" id="3ref">[3]</a> 只分析了 Segmentation 和 Detection 任务，每个任务都从 Point-based，Voxel-based，View-based 三种方法来阐述。<br>
　　本文结合这三篇综述，对不同任务的不同方法作详尽的归纳。</p>
<h2 id="datasets">1. Datasets</h2>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/datasets.png" width="90%" height="90%" title="图 1. Datasets"></p>
<h2 id="metrics">2. Metrics</h2>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/metrics.png" width="90%" height="90%" title="图 2. Metrics"> 　　不同任务的主要度量指标如图 2. 所示，此外还有：</p>
<ul>
<li>Average Precision(AP)<br>
用于 3D 目标检测，计算的是 precision-recall 曲线下的面积：\(AP=\frac{1}{11}\sum _ {r\in\{0,0.1,...,1\}}\mathop{\max}\limits _ {\tilde{r} : \tilde{r}\geq r} p(\tilde{r})\)，其中 \(r\) 表示 recall，\(p(r)\) 表示对应的 precision。</li>
<li>Average Orientation Similarity(AOS)<br>
类似的，\(AOS=\frac{1}{11}\sum _ {r\in\{0,0.1,...,1\}}\mathop{\max}\limits _ {\tilde{r} : \tilde{r}\geq r} s(\tilde{r})\)。</li>
<li>Panoptic Quality(PQ)<a href="#4" id="4ref"><sup>[4]</sup></a><br>
用于全景分割，可分解为 Segmentation Quality(SQ) 以及 Recognition Quality(RQ)，即同时评估语义分割及 Instance 分割，定义为： <span class="math display">\[\begin{align}
PQ &amp;= \frac{\sum _ {(p,g)\in TP}IoU(p,g)}{\vert TP\vert+\frac{1}{2}\vert FP\vert+\frac{1}{2}\vert FN\vert} \\
&amp;= \frac{\sum _ {(p,g)\in TP}IoU(p,g)}{\vert TP\vert} \times\frac{\vert TP\vert}{\vert TP\vert+\frac{1}{2}\vert FP\vert+\frac{1}{2}\vert FN\vert} \\
&amp;= SQ \times RQ
\tag{1}
\end{align}\]</span> \(RQ\) 就相当于 F1-Score，只不过这里的 TP 可能是根据点集交并比来判断的。</li>
</ul>
<h2 id="classification">3. Classification</h2>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/classification.png" width="100%" height="100%" title="图 3. Classification"> 　　分类是最基础的任务，创新的网络结构基本首先在分类任务中进行应用，网络结构的创新目的本质上又是更有效的特征提取。<a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a> 中已经较详细的描述了基于点的点云特征提取的相关进展，如图 3. 所示，所有高层任务所用到的网络结构基本可以在这找到对应的基础网络。</p>
<h3 id="multi-view-based-methods">3.1. Multi-view based Methods</h3>
<p>　　这种方法将点云投影到特定的几种视角平面上，提取特征后再将各个视角的特征进行整合，最后得到全局特征，来作分类。一般选取 Bird-eye View 与 Front View 两个视角，BV 的好处是目标尺寸的一致性，FV 的好处是对狭长型目标较为友好。代表方法由 MVCNN，MHBN，View-GCN 等。</p>
<h3 id="volumetric-based-methods">3.2. Volumetric-based Methods</h3>
<p>　　这种方法将点云量化为 3D 体素(Voxel)，然后采用 3D 卷积作特征提取。方法有 VoxNet，OctNet，O-CNN 等。</p>
<h3 id="point-based-methods">3.3. Point-based Methods</h3>
<p>　　此类方法玩法较多，因为直接在原始点云上进行特征提取，所以没有额外的信息损失。其研究的变种也较多。</p>
<h4 id="pointwise-mlp-methods">3.3.1. Pointwise MLP Methods</h4>
<p>　　典型代表为 PointNet，直接对每个点的特征维度进行 MLP 变换，然后用 Symmetric Operator 在点的维度进行 Reduction 得到全局特征。此外可以加入各种采样策略，作特征的级联采样并提取，其它方法有 DeepSets，PointNet++，Mo-Net，PATs，PointWeb，SRN，JustLookUp，PointASNL 等。</p>
<h4 id="convolution-based-methods">3.3.2. Convolution-based Methods</h4>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/3D-conv.png" width="60%" height="60%" title="图 4. Continuous and Discrete Convolution"> 　　由于点云数据的离散性，传统的图像 2D/3D 卷积无法直接使用。如图 4. 所示，根据卷积核的定义方式，基于点的卷积可分为 Continuous Convolution 和 Discrete Convolution。</p>
<ul>
<li>Continuous Convotion<br>
对于以某一中心点作卷积的点集，其卷积核权重是与点集相对该中心点的空间分布有关。权重，即空间分布的计算，一般通过 MLP 网络对每个点的欧式距离等特征编码得到。</li>
<li>Discrete Convotion<br>
对于以某一中心点作卷积的点集，其卷积核权重是与点集相对该中心点的空间残差有关。首先对点集区域以中心点作一定形状的栅格化，对每个栅格内的点进行特征提取，然后再以栅格为单位，作类似传统的卷积操作。</li>
</ul>
<h4 id="graph-based-methods">3.3.3. Graph-based Methods</h4>
<p>　　该方法将点云中的点建模为图顶点，然后将相邻点进行有向连接得到有向图。点云的特征提取可在 Spatial 或 Spectral 空间内进行。<br>
　　在 Spatial 空间，图顶点通常包含坐标值，反射率，颜色等初始特征，图边通常与连接边的两个顶点空间距离有关，一般通过 MLP 网络构建，<a href="/paper-reading-Point-GNN/" title="Point-GNN">Point-GNN</a> 中比较详细得描述了这一过程。<br>
　　在 Spectral 空间，卷积定义为光谱滤波器，用 Graph Laplacian Matrix 的特征向量相乘实现。</p>
<h4 id="hierarchical-data-structure-based-methods">3.3.4. Hierarchical Data Structure-based Methods</h4>
<p>　　该类方法将点云构建成级联的类树状结构，特征学习通过树叶至树根传递。代表方法有 OctNet，Kd-Net 等。</p>
<h4 id="other-methods">3.3.5. Other Methods</h4>
<h2 id="detection-and-tracking">4. Detection and Tracking</h2>
<h3 id="d-object-detection">4.1. 3D Object Detection</h3>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/detection.png" width="100%" height="100%" title="图 5. detection methods"></p>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/det_comp.png" width="100%" height="100%" title="图 6. detection methods comparison"> 　　目标检测是非常重要的一个任务，不同类型的方法如图 5. 所示，其性能如图 6. 所示。</p>
<h4 id="region-proposal-based-methods">4.1.1. Region Proposal-based Methods</h4>
<p>　　这种方法首先找出候选区域框，然后作进一步的目标属性精修。根据候选框的产生方式，可分为 Multi-view based Methods，Segmentation-based Methods 以及 Frustum-based Methods。<br>
　　Multi-view based Methods 一般速度比较慢，会融合不同模态的数据，算法也比较复杂。<br>
　　Segmentation-based Methods 先通过语义分割的方法去除背景区域点，根据前景点生成高质量的候选框集合，由较高的召回率，且善于处理遮挡等较为复杂的场景。<br>
　　Frustum-based Methods 通常通过图像生成候选目标框，然后根据视锥去提取激光点云或毫米波点云的目标测量，对于中后期的融合，该方法应用也较多。</p>
<h4 id="single-shot-methods">4.1.2. Single Shot Methods</h4>
<p>　　单阶段方法没有提取候选框这个步骤，网络直接预测目标的 3D 属性，可分为 BEV-based，Discretization-based Methods，以及 Point-based Methods。<br>
　　BEV-based Methods，Discretization-based Methods 均通过离散化点云空间，然后作类似图像的 2D 卷积或 3D 卷积操作。Point-based Methods 则直接在点云级别作特征提取以及目标检测。这些方法之前讨论较多了，不做展开。</p>
<h3 id="d-object-tracking">4.2. 3D Object Tracking</h3>
<p>　　相比图图像的 2D 跟踪，3D 跟踪能比较容易的解决遮挡，光照，尺度等问题。本博客讨论过基于传统 ICP 的 <a href="/ADH-Tracker/" title="ADH-Tracker">ADH-Tracker</a> 方法，也介绍过基于深度学习的 <a href="/paper-reading-P2B/" title="P2B">P2B</a> 方法。总体上来讲，3D 目标跟踪套路较少，除非将检测，跟踪，预测联合来优化，比如 <a href="/paper-reading-PnPNet/" title="PnPNet">PnPNet</a>。</p>
<h3 id="d-scene-flow-estimation">4.3. 3D Scene Flow Estimation</h3>
<p>　　3D Scene Flow 问题定义为：给定 \(\mathcal{X,Y}\) 两个点云集，3D Scene Flow \(D=\{d _ i\} ^ N\) 描述了 \(\mathcal{X}\) 中的点 \(x _ i\) 与其在 \(\mathcal{Y}\) 中的最近对应点 \(x _ i '\) 的距离 \(x _ i '=x _ i+d _ i\)。<br>
　　3D Scene Flow 是一个中低层任务，根据 3D Scene Flow 可以进一步作运动物体分割聚类，目标运动速度估计等高层任务。<a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 是较早采用深度学习进行 3D Scene Flow 估计的方法。类似图像中的光流估计，可采用无监督方法，对应的 Loss 有 EMD 等。</p>
<h2 id="segmentation">5. Segmentation</h2>
<h3 id="semantic-segmentation">5.1. Semantic Segmentation</h3>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/semantic-seg.png" width="100%" height="100%" title="图 7. semantic segmentation methods"></p>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/seg_comp.png" width="100%" height="100%" title="图 8. semantic segmentation comparison"> 　　语义分割方法如图 7. 所示，各方法的性能如图 8. 所示。</p>
<h4 id="projection-based-methods">5.1.1. Projection-based Methods</h4>
<p>　　点云语义分割需要尽可能保留点级别的特性信息，投影法基本上可分为 BirdView，Spherical，Cylinde 三种，代表方法有 <a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a>，<a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>，<a href="/paper-reading-Cylinder3D/" title="Cylinder3D">Cylinder3D</a> 等。</p>
<h4 id="discretization-based-methods">5.1.2. Discretization-based Methods</h4>
<p>　　这种方法是将点云空间体素化，然后用 3D 卷积的形式来提取特征。一般会丢失点级别的信息，所以需要点级别的信息提取方式来辅助。</p>
<h4 id="point-based-methods-1">5.1.3. Point-based Methods</h4>
<p>　　在点云语义分割中，点级别的特征提取是非常有必要的，一般采用点级别的 MLP，PointNet Convolution，RNN，Graph-based 等方法，本质上都是对每个点周围的点集作特征提取操作。<a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中有较详细的描述。</p>
<h3 id="instance-segmentation">5.2. Instance Segmentation</h3>
<p><img src="/Deep-Learning-for-3D-Point-Clouds/ins-seg.png" width="80%" height="80%" title="图 9. instance segmentation methods"> 　　实例分割不仅作语义分割，还得将同一目标的点云作聚类处理。其可分为 Proposal-based Methods，以及 Proposal-free Methods。Proposal-based 将实例分割分解为目标检测以及实例 mask 预测两个任务；Proposal-free 则是自下而上的求解该问题，一般通过预测语义及聚类辅助量，最后通过相关聚类策略实现。相关方法如图 9. 所示。</p>
<h2 id="registration">6. Registration</h2>
<p>　　点云注册主要有两大块应用，本车的位姿估计以及目标车的速度估计。对于前后帧点云的注册能估计出前后帧时间内本车的位姿变化；对于目标点云的注册能测量出目标的速度。<br>
　　传统的点云注册方法通过 ICP 实现，详见 <a href="/Object-Registration-with-Point-Cloud/" title="Object Registration with Point Cloud">Object Registration with Point Cloud</a> 中的描述。对于两个点云集合，也可通过网络求解 \(R,t\)。</p>
<h2 id="augmentation-and-completion">7. Augmentation and Completion</h2>
<p>　　激光点云数据往往受噪声，离群点，未测量点影响，可以采用网络对点云进行噪声滤出处理，以及点云补全处理，这里不做展开。</p>
<h2 id="conclusion">8. Conclusion</h2>
<p>　　基于点云的每个任务对应的方法，其实都可分为 View-based，Voxel-based，Point-based 三大类，当然也可以结合来做。其中 View-based 主要由 BirdView，Spherical，Cylinde 三种；Voxel-based 则主要分为 2D/3D；Point-based 则玩法较多，不过本质上还是对点周围的小点集作特征提取。</p>
<h2 id="reference">9. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Guo, Yulan, et al. "Deep learning for 3d point clouds: A survey." IEEE transactions on pattern analysis and machine intelligence (2020).<br>
<a id="2" href="#2ref">[2]</a> Lu, Haoming, and Humphrey Shi. "Deep Learning for 3D Point Cloud Understanding: A Survey." arXiv preprint arXiv:2009.08920 (2020).<br>
<a id="3" href="#3ref">[3]</a> Li, Ying, et al. "Deep learning for LiDAR point clouds in autonomous driving: a review." IEEE Transactions on Neural Networks and Learning Systems (2020).</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Learning Efficient Convolutional Networks through Network Slimming&quot;</title>
    <url>/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/</url>
    <content><![CDATA[<p>　　剪枝是神经网络加速的重要手段之一，<a href="/pruning/" title="Pruning">Pruning</a> 中较详细的描述了剪枝的特性与基本方法，<a href="/Filter-Pruning/" title="Filter-Pruning">Filter-Pruning</a> 则描述了卷积核剪枝的基本方法。Filter Pruning 属于结构化剪枝(Structure Pruners)，能在不改变硬件计算单元的情况下，提升网络计算速度。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 就属于 Filter Pruning 方法，简单有效，较为经典。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，本文提出的方法非常简单：将各通道特征(对应各卷积核)乘以一个尺度系数，并用正则化方法来稀疏化尺度系数。训练后卷积核对应的较小的尺度，则认为其不重要，进行剪枝。最终 Fine-Tune 剩下的网络即可。<br>
<img src="/paper-reading-Learning-Efficient-Convolutional-Networks-through-Network-Slimming/pipeline.png" width="70%" height="70%" title="图 2. Pipeline"> 　　其剪枝及 Fine-Tune 过程如图 2. 所示，可迭代进行，以获得最优的剪枝效果。<br>
　　实际操作中用 BN 层中的 \(\gamma\) 系数来代替该尺度系数。BN 层计算过程为： <span class="math display">\[\hat{z}=\frac{z _ {in}-\mu _ {\mathcal{B}}}{\sqrt{\sigma _ {\mathcal{B}} ^ 2 + \epsilon}}; \; z _ {out}=\gamma \hat{z}+\beta\tag{1}\]</span> 其中 \(z _ {in}, z _ {out}\) 表示 BN 层的输入输出，\(\mathcal{B}\) 表示当前 mini-batch，\(\mu _ {\mathcal{B}},\sigma _ {\mathcal{B}}\) 表示 mini-batch 中 channel-wise 的均值和方差，\(\gamma,\beta\) 为可训练的 channel-wise 的尺度及偏移系数。<br>
　　对 \(\gamma\) 进行 L1 正则化，较小的 \(\gamma\) 对应的卷积核即认为是不重要的，可裁剪掉。</p>
<h2 id="thinking">2. Thinking</h2>
<p>　　本方法非常简单，但是为什么仅凭较小的 \(\gamma\) 即可确定对应的卷积核不重要呢？我的思考如下：</p>
<ul>
<li>为什么不需要考虑卷积核的 L1 值？<br>
因为 BN 的 \(\mu _ {\mathcal{B}},\sigma _ {\mathcal{B}}\) 将输出归一化了，所以卷积核的值幅度对之后没有影响，故其值幅度无法体现其重要性；</li>
<li>为什么不需要考虑 \(\beta\) 值？<br>
因为 \(\beta\) 只会影响输出特征的均值，而不会影响输出特征的方差，神经网络的表达能力在于特征的方差，而不是均值，故 \(\gamma\) 才能体现卷积核的重要性，而 \(\beta\) 不能。</li>
</ul>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Liu, Zhuang, et al. "Learning efficient convolutional networks through network slimming." Proceedings of the IEEE International Conference on Computer Vision. 2017.</p>
]]></content>
      <categories>
        <category>Model Compression</category>
        <category>Pruning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Model Compression</tag>
        <tag>Pruning</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;OccuSeg&quot;</title>
    <url>/paper-reading-OccuSeg/</url>
    <content><![CDATA[<p>　　之前介绍了 <a href="/paper-reading-JSNet-JSIS3D/" title="JSNet, JSIS3D">JSNet, JSIS3D</a>，<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a> 等 Instance 分割方法，为了点云聚类成 Instance，网络输出基本分为每个点距离目标中心的坐标残差以及每个点的 Embedding 特征两种。对于目标中心的坐标残差，之后可以直接在几何空间内作基于距离的聚类；对于每个点的 Embedding 特征，由于训练时要求同一个 Instance 内的 Embedding 相近，不同的距离要远，所以也是通过高维空间的距离计算来作聚类。<br>
　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>大致也是这个思路，此外还预测了每个 voxel 的 Occupancy Regression，表示对应 Instance 包含的 Voxel 数目。最后采用基于图的聚类方法得到 Instance。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-OccuSeg/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，网络输入为 RGBD 点云数据，通过 3D UNet Backbone 网络，输出基于 Voxel 的三种预测结果：</p>
<ul>
<li><strong>Semantic Segmentation</strong><br>
语义分割结果 \(\mathbf{c} _ i\)。</li>
<li><strong>Spatial Embedding and Feature Embedding</strong><br>
基于坐标系的残差预测 \(\mathbf{d} _ i\)，和基于特征空间的 Embedding 预测 \(\mathbf{s} _ i\)，以及对应的方差 \(\mathbf{b} _ i=(\sigma _ d ^ i,\sigma _ s ^ i)\)。</li>
<li><strong>Occupancy Regression</strong><br>
预测该 Voxel 对应 Instance 所含有的 Voxel 数量 \(\mathbf{o} _ i\)。</li>
</ul>
<p>Loss 项目构成为： <span class="math display">\[\mathcal{L} _ {joint} = \mathcal{L} _ {c} + \mathcal{L} _ {e} + \mathcal{L} _ {o} \tag{1}\]</span></p>
<h3 id="spatial-and-feature-embedding">1.1. Spatial and Feature Embedding</h3>
<p>　　Embedding 的 Loss 项构成为： <span class="math display">\[\mathcal{L} _ e = \mathcal{L} _ {sp} + \mathcal{L} _ {se} + \mathcal{L} _ {cov} \tag{2}\]</span> 其中 Spatial Embedding 目的是回归每个 voxel 与目标中心点的残差： <span class="math display">\[\mathcal{sp}=\frac{1}{C}\sum _ {c=1} ^ C\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\Vert \mathbf{d} _ i+\mu _ i-\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\mu _ i\Vert \tag{3}\]</span> \(C\) 表示 Instance 数量，\(N _ c\) 表示第 \(c\) 个 Instance 包含的 Voxel 数量，\(\mu _ i\) 表示第 \(i\) 个 voxel 的坐标。<br>
　　Feature Embedding 目的是相同 Instance 的 voxel 预测相似的特征，不同的则预测不同的特征，通过 Metric Learning 实现： <span class="math display">\[\begin{align}
\mathcal{L} _ {se} &amp;=\mathcal{L} _ {var}+\mathcal{L} _ {dist} +\mathcal{L} _ {reg}\\
&amp;= \frac{1}{C}\sum _ {c=1} ^ C\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\left[\Vert\mathbf{u} _ c-\mathbf{s} _ i\Vert -\delta _ v\right] _ + ^ 2 + \frac{1}{C(C-1)}\mathop{\sum _ {i=1} ^ C\sum _ {j=1} ^ C} \limits _ {i\neq j}\left[2\delta _ d-\Vert\mathbf{u} _ i-\mathbf{u} _ j\Vert \right] _ + ^ 2 + \frac{1}{C}\sum _ {c=1} ^ C\Vert\mathbf{u} _ c\Vert
\tag{4}
\end{align}\]</span> 其中 \(\mathbf{u} _ c=\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c} \mathbf{s} _ i\) 表示第 \(c\) 个 Instance 的平均 Embedding 特征。以上和 <a href="/paper-reading-JSNet-JSIS3D/" title="JSNet, JSIS3D">JSNet, JSIS3D</a> 基本一致。<br>
　　此外本文还预测了 Covariance 项。设预测的 Feature 和 Spatio Covariance 为 \(\mathbf{b} _ i=(\sigma _ s ^ i, \sigma _ d ^ i)\)，对 Instance 内的 voxel covariance 融合可得到该 Instance 的 Covariance \((\sigma _ s ^ c,\sigma _ d ^ c)\) (<strong>这里需要注意的是，Inference 的时候，即作基于图分割算法的聚类时候，见1,3，是只需要作 super-voxel 内的 Covariance 融合；而训练的时候，是由 Instance 标签的，所以能通过 Instance 的 Covariance 融合，以重构 \(p _ i\)</strong>)。由此可得到第 \(i\) 个 voxel 属于第 \(c\) 个 Instance 的概率： <span class="math display">\[p _ i = \mathrm{exp}\left(-\left(\frac{\Vert\mathbf{s} _ i-\mathbf{u} _ c\Vert}{\sigma _ s ^ c}\right)^2-\left(\frac{\Vert \mu _ i+\mathbf{d} _ i-\mathbf{e} _ c\Vert}{\sigma _ d ^ c}\right)^2\right)\tag{5}\]</span> 其中 \(\mathbf{e} _ c=\frac{1}{N _ c}\sum _ {k=1} ^ {N _ c}(\mu _ k+\mathbf{d} _ k)\) 表示预测的目标中心点。当 \(p _ i\) 大于 0.5 时，就表示该 voxel 属于该 Instance，所以用 binary cross-entropy loss： <span class="math display">\[\mathcal{L} _ {cov} = -\frac{1}{C}\sum _ {c=1}^C\frac{1}{N}\sum _ {i=1} ^ N[y _ i\mathrm{log}(p _ i)+(1-y _ i)\mathrm{log}(1-p _ i)]\tag{6}\]</span> 其中 \(y _ i\) 为标签，1 表示该 voxel 属于该 Instance，0 表示不属于。</p>
<h3 id="occupancy-regression">1.2. Occupancy Regression</h3>
<p>　　每个 Voxel 预测其对应的 Instance 包含的 Voxel 数目 \(o _ i\)，为了预测的鲁棒性，设计为回归其 log 值： <span class="math display">\[\mathcal{L} _ o = \frac{1}{C}\sum _ {c=1} ^ C\frac{1}{N _ c}\sum _ {i=1} ^ {N _ c}\Vert o _ i-\mathrm{log}(N _ c)\Vert \tag{7}\]</span> 其中 \(N _ c\) 是第 \(c\) 个 Instance 包含的 Voxel 数量。</p>
<h3 id="instance-clustering">1.3. Instance Clustering</h3>
<p>　　基于每个 Voxel 的预测：Semantic Label，Spatial and Feature Embedding，Occupancy Regression，本文采用自底向上的图分割算法。<br>
　　设 \(\Omega _ i\) 为上层 super-voxel \(v _ i\) 包含的 Voxel 数量。\(v _ i\) 对应的 Spatial Embedding，Feature Embedding，Occupancy Regression 为： <span class="math display">\[\left\{\begin{array}{l}
\mathbf{D} _ i =\frac{1}{\vert\Omega _ i\vert}\sum _ {k\in\Omega _ i}(\mathbf{d} _ k+\mu _ k)\\
\mathbf{S} _ i =\frac{1}{\vert\Omega _ i\vert}\sum _ {k\in\Omega _ i}(\mathbf{s} _ k)\\
\mathbf{O} _ i =\frac{1}{\vert\Omega _ i\vert}\sum _ {k\in\Omega _ i}(\mathrm{exp}(\mathbf{o} _ k))\\
\end{array}\tag{8}\right.\]</span> 为了指导聚类过程，定义(文章中应该是写反了)： <span class="math display">\[r _ i=\frac{\vert\Omega _ i\vert}{O _ i} \tag{9}\]</span> \(r _ i&gt;1\) 表示该 Instance 聚类的 Voxel 太多了，即欠分割；反之即为过分割。<br>
　　图分割算法定义图 \(G=(V,E,W)\)，其中 \(v _ i\in V\) 表示 super-voxel，\(e _ {i,j}\) 表示 \((v _ i, v _ j)\in E\) 通过全概率权重 \(w _ {i,j}\in W\) 连接。\(w _ {i,j}\) 可定义为两个 super-voxel 的相似度： <span class="math display">\[w _ {i,j}=\frac{\mathrm{exp}\left(-\left(\frac{\Vert\mathbf{S} _ i-\mathbf{S} _ j\Vert}{\sigma _ s}\right) ^ 2-\left(\frac{\Vert \mathbf{D} _ i-\mathbf{D} _ j\Vert}{\sigma _ d}\right) ^ 2\right)}{\mathrm{max}(x,0.5)} \tag{10}\]</span> 自底向上迭代聚类通过 \(w _ {i,j} &gt; T _ 0\) 实现，\(T _ 0\) 设定为 0.5，最后保留 \(0.3 &lt; r&lt;2\) 的 Instance，以减少 FP。</p>
<h2 id="reference">2. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Han, Lei , et al. "OccuSeg: Occupancy-aware 3D Instance Segmentation." (2020).</p>
]]></content>
      <categories>
        <category>Segmentation</category>
        <category>Instance Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Instance Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;P2B&quot;</title>
    <url>/paper-reading-P2B/</url>
    <content><![CDATA[<p>　　3D 目标状态估计中，对目标的跟踪测量非常重要，所谓跟踪测量，指的是给定前后目标框或者目标点云，计算目标的 R,t 的过程，由此可得到目标位置及速度的观测。<strong>一般的测量量有：目标框中心点距离，目标点云重心距离，目标点云 ICP 结果</strong>。<strong>在目标点云观测较为完备的情况下，理论上类 ICP 方法的结果是最为准确的，但是在点云较少的情况下，通过基于深度学习脑补预测出来的目标框中心点可能会更靠谱</strong>。ICP 一个比较大的问题是速度较慢，<a href="/ADH-Tracker/" title="ADH-Tracker">ADH-Tracker</a> 中的测量量本质上类似 ICP，但是其通过退火算法将 T 的搜索空间进行压缩，获得了极大的效率提升，但是对旋转量的估计还是比较棘手。<br>
　　将跟踪测量问题往外扩，就是图像领域常说的单目标跟踪：给定上一帧目标位置，找到当前帧目标的位置。点云场景中，一般不会作目标的单目标跟踪，而是直接采用目标运动模型预测当前帧位置以及目标检测当前帧位置，所以目标的定位精度基本完全靠目标检测结果以及卡尔曼平滑结果。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出一种基于深度学习的点云单目标跟踪方法。类似图像中的单目标跟踪，输入为上一帧目标或目标的模型，以及当前帧搜索区域，输出为当前帧目标的位置。进一步讲，<strong>套用目标框中心，点云重心以及类 ICP 计算后，该模块结果可作为另一种更准确（相比直接采用检测框）的跟踪测量量</strong>。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-P2B/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，P2B 首先将目标(目标模型)特征与搜索区域的特征作融合，用来预测潜在的目标中心点，然后作端到端的目标 proposal 以及 verification。其网络主要由两部分组成：1. Target-specific feature augmentation; 2. 3D target proposal and verification。</p>
<h3 id="target-specific-feature-augmentation">1.1. Target-specific feature augmentation</h3>
<p>　　目标点云为 \(P _ {tmp}\in\mathbb{R} ^ {N _ 1\times 3}\)，搜索区域的点云为 \(P _ {sea}\in\mathbb{R} ^ {N _ 2\times 3}\)。经过 PointNet++ 采样及提取特征后，得到目标点云的种子点集 \(Q=\{q _ i\} _ {i=1} ^ {M _ 1}\)，搜索区域的种子点集 \(R=\{r _ j\} _ {j=1} ^ {M _ 2}\)，每个点特征向量为 \([x;f]\in\mathbb{R} ^ {3+d _ 1}\)。计算 \(Q,R\) 的相似度： <span class="math display">\[Sim _ {j,i} =\frac{f _ {q _ i} ^ T\cdot f _ {r _ j}}{\Vert f _ {q _ i}\Vert _ 2\cdot\Vert f _ {r _ j} \Vert _ 2},\;\forall q _ i\in Q,\;r _ j\in R \tag{1}\]</span> <img src="/paper-reading-P2B/feat.png" width="90%" height="90%" title="图 2. Feature Aggregation"> 　　得到的相似性特征图 \(Sim\) 与目标种子点集 \(Q\) 的顺序有关，如图 2. 所示，Target-Specific Feature Augmentation 模块目的就是消除 \(Q\) 顺序的影响。其基本思想也是通过对称操作将 \(Q\) 所在的 \(M _ 1\) 维度进行压缩。具体的，将 \(Sim \in\mathbb{R} ^ {M _ 2\times M _ 1}\) 变换为 \(M _ 2\times (3+d _ 2)\) 维度的特征。图 2 采用了 \(Q\) 特征，也可以采用其它方式，实验表明这种方式最好。</p>
<h3 id="d-target-proposal-and-verification">1.2. 3D target proposal and verification</h3>
<p>　　有了搜索区域每个种子点的特征后，可以基于此作候选目标框的预测。接下来分为两个分支，一个分支输出 \(M _ 2\times 1\)，表示每个种子点作为目标中心的概率分数；另一个分支采用 VoteNet<a href="#2" id="2ref"><sup>[2]</sup></a> ，输出相同维度及尺寸的特征，表示种子点与中心点的<strong>坐标及特征残差</strong>。VoteNet 中种子点的坐标残差用目标中心点与该种子点的距离来监督，而特征残差没有监督 Loss。这与 Instance-Seg 里面的套路非常相似，特征残差其实可以加上类内 Pull Loss 项，详见 <a href="/paper-reading-JSNet-JSIS3D/" title="JSNet-JSIS3D">JSNet-JSIS3D</a>。<br>
　　对每个种子点作残差补偿后，类似 Instance-Seg 中的套路，可采用 Ball-Query 作目标的点集聚类，然后对聚类后的点集归一化并用 PointNet 即可预测目标的分数以及目标的 3D 属性，这里不作展开，详见<a href="#2" id="2ref">[2]</a>。<br>
　　Loss 项由残差回归，是否是 proposal 的概率，proposal 质量分数及目标 3D 属性回归等组成： <span class="math display">\[L = L _ {reg} +\gamma _ 1L _ {cla} + \gamma _ 2L _ {prop} + \gamma _ 3L _ {box} \tag{2}\]</span> 其中 \(\gamma\) 分别设计为 \(0.2, 1.5, 0.2\)。</p>
<h2 id="workflow">2. Workflow</h2>
<p><img src="/paper-reading-P2B/workflow.png" width="60%" height="60%" title="图 3. Workflow"> 　　P2B 的整个算法流程如图 3. 所示，需要注意的是，最后得到的 \(K\) 个 Proposal 只需要取分数最高的即可。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Qi, Haozhe, et al. "P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br>
<a id="2" href="#2ref">[2]</a> Qi, Charles R., et al. "Deep hough voting for 3d object detection in point clouds." Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
        <tag>MOT</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;JSNet, JSIS3D&quot;</title>
    <url>/paper-reading-JSNet-JSIS3D/</url>
    <content><![CDATA[<p>　　<a href="/paper-reading-PointGroup/" title="PointGroup">PointGroup</a> 通过预测每个点与对应 instance 重心的 offset，然后在三维物理坐标系下作 instance 聚类。<a href="#1" id="1ref">[1]</a> 也是这种方案。另一种思路，是通过 Metric Learning 技术，预测每个点的高维特征(Embedding Space)，然后作 instance-level 聚类。本文介绍的 JSNet<a href="#2" id="2ref"><sup>[2]</sup></a> 以及 JSIS3D<a href="#3" id="3ref"><sup>[3]</sup></a> 就是采用的这种方式。</p>
<h2 id="jsnet">1. JSNet</h2>
<p><img src="/paper-reading-JSNet-JSIS3D/JSNet.png" width="90%" height="90%" title="图 1. JSNet"> 　　如图 1. 所示，整个网络共享的 Backbone 只有点云特征的 Encode 阶段，两个分支分别作 Decode 并通过 PCFF 模块，最终输出用于 Instance-Seg 的特征 \(F _ {IS}\in\mathbb{R} ^ {N _ a\times 128}\)，以及用于 Semantic-Seg 的特征 \(F _ {SS}\in\mathbb{R} ^ {N _ a\times 128}\)。这一阶段完全可以用其它 Voxel 或 Point 网络代替。然后通过 JISS 模块进行两个分支的特征融合，最终输出点云类别，以及用于点云 Instance 聚类的特征 Embedding。最后采用 Mean-Shift 聚类方法即可根据 Embedding 作 Instance 聚类。</p>
<h3 id="pcff">1.1. PCFF</h3>
<p>　　PCFF 类似图像 2D 卷积中上采样特征融合模块，如图 1.a 所示，目的是为了融合不同尺度的点云特征。PCFF 及之前的网络均可用其它点云特征网络代替。</p>
<h3 id="jiss">1.2. JISS</h3>
<p>　　JISS 模块目的是将 Instance-Seg 和 Semantic-Seg 两个任务的特征作充分的融合。Semantic-Seg 一般比 Instance-Seg 更底层，所以相同深度的网络，理论上能学到更加抽象(高层)的特征，所以如图 1.c 所示，先将 \(F _ {SS}\) 特征融入 \(F _ {IS}\) 特征中，然后在 Instance-Seg 分支作进一步特征提取后，再将特征返回来与 \(F _ {SS}\) 特征作融合。此外，每个分支还引入了 Self-Attention 模块，通过 Sigmoid 操作实现。<br>
　　最终输出的是每个点的类别分数 \(P _ {SSI}\in\mathbb{R} ^ {N _ a\times C}\)，以及用于 Instance 聚类的点云特征 \(E _ {ISS}\in\mathbb{R} ^ {N _ a\times K}\)。</p>
<h3 id="loss">1.3. Loss</h3>
<p>　　Loss 由 Semantic-Seg 以及 Instance-Seg 两个任务组成： <span class="math display">\[\mathcal{L}=\mathcal{L} _ {sem}+\mathcal{L} _ {ins}\tag{1}\]</span> 其中语义分割的 Loss 项 \(\mathcal{L} _ {sem}\) 为传统的分类 Loss。\(\mathcal{L} _ {ins}\) 则要求能区分不同 Instance 的点云 Embedding 特征，但是又要保证同一 Instance 的点云 Embedding 特征的相似性，设计为： <span class="math display">\[\begin{align}
\mathcal{L} _ {ins} &amp;=\mathcal{L} _ {pull}+\mathcal{L} _ {push}\\
&amp;= \frac{1}{M}\sum _ {m=1} ^ M\frac{1}{N _ m}\sum _ {n=1} ^ {N _ m}\left[\Vert\mu _ m-e _ n\Vert _ 1-\delta _ v\right] _ + ^ 2 + \frac{1}{M(M-1)}\mathop{\sum _ {i=1} ^ M\sum _ {j=1} ^ M} \limits _ {i\neq j}\left[2\delta _ d-\Vert\mu _ i-\mu _ j\Vert _ 1\right] _ + ^ 2
\tag{2}
\end{align}\]</span> 其中 \([x] _ +=\mathrm{max}(0,x)\)，\(|| \cdot || _ 1\) 为 L1 距离，\(\delta _ v,\delta _ d\) 分别为 \(\mathcal{L} _ {pull},\mathcal{L} _ {push}\) 的幅度。</p>
<h2 id="jsis3d">2. JSIS3D</h2>
<p><img src="/paper-reading-JSNet-JSIS3D/JSIS.png" width="90%" height="90%" title="图 2. JSIS"> 　　如图 2. 所示，JSIS3D 由 MT-PNet 网络和 MV-CRF 构成。MV-CRF 是基于 MT-PNet 网络预测的 Semantic Label 和 Embeddings 作基于条件随机场的 instance 聚类，效果比直接对 Embeddings 作聚类要好，这里只讨论 MT-PNet 网络。</p>
<h3 id="mt-pnet">2.1. MT-PNet</h3>
<p><img src="/paper-reading-JSNet-JSIS3D/MT-PNet.png" width="90%" height="90%" title="图 3. MT-PNet"> 　　如图 3. 所示，网络由基本的 PointNet 构成，最终预测的也是每个点的类别以及用于聚类的 Embedding。所以输出方案是与 JSNet 是一样的。Loss 项中的 Embedding(ins) 预测项加入了正则化： <span class="math display">\[\begin{align}
\mathcal{L} _ {ins} &amp;=\alpha\mathcal{L} _ {pull}+\beta\mathcal{L} _ {push}+\gamma\mathcal{L} _ {reg}\\
&amp;= \frac{\alpha}{M}\sum _ {m=1} ^ M\frac{1}{N _ m}\sum _ {n=1} ^ {N _ m}\left[\Vert\mu _ m-e _ n\Vert _ 2-\delta _ v\right] _ + ^ 2 + \frac{\beta}{M(M-1)}\mathop{\sum _ {i=1} ^ M\sum _ {j=1} ^ M} \limits _ {i\neq j}\left[2\delta _ d-\Vert\mu _ i-\mu _ j\Vert _ 2\right] _ + ^ 2 + \frac{\gamma}{M}\sum _ {m=1} ^ M \Vert \mu _ m\Vert _ 2
\tag{3}
\end{align}\]</span> 其中 \(M\) 为 instance 数量，\(N _ m\) 为对应 instance 内点的个数，\(e _ n\) 为点的 Embedding，\(\mu _ m\) 表示第 \(m\) 个 instance 内点的平均 Embedding。设计 \(\sigma _ d &gt; 2\sigma _ v,\alpha=\beta=1,\gamma=0.001\)，可以实现同一个 instance 内点的 Embedding 相近，不同 instance 的平均 Embedding 距离较远，并且正则化使得平均 Embedding 接近 0。</p>
<h3 id="experiments">2.2. Experiments</h3>
<p><img src="/paper-reading-JSNet-JSIS3D/res.png" width="90%" height="90%" title="图 4. Mean-Shift VS. MV-CRF"> 　　如图所示，用 MV-CRF 代替 Means-Shift 聚类，对于大物体，提升效果比较明显，但是小物体，精度会下降。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> F. Zhang, C. Guan, J. Fang, S. Bai, R. Yang, P. Torr, and V. Prisacariu, “Instance segmentation of lidar point clouds,” in ICRA, 2020<br>
<a id="2" href="#2ref">[2]</a> L. Zhao and W. Tao, “JSNet: Joint instance and semantic segmentation of 3D point clouds,” in AAAI, 2020.<br>
<a id="3" href="#3ref">[3]</a> Pham, Quang Hieu , et al. "JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds With Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields." 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) IEEE, 2020.</p>
]]></content>
      <categories>
        <category>Segmentation</category>
        <category>Instance Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Instance Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;PointGroup&quot;</title>
    <url>/paper-reading-PointGroup/</url>
    <content><![CDATA[<p>　　之前一直提到，以 Semantic Segmentation 为基础作目标检测，可以有较高的召回率，而在最终出目标框或目标 Polygon 之前，还需要作 Instance Segmentation。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种 Instance Segmentation 方法。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-PointGroup/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，整个网络由三部分构成：Backbone，Clustering Part，ScoreNet。Backbone 我们已经很熟悉了，输入为点云以及点云的其它属性比如 rgb 信息，输出为每个点提取的局部-全局特征 \(\mathbf{F} = \{F _ i\}\in\mathbb{R} ^ {N\times K}\)，这里不作展开。然后用提取的特征 \(\mathbf{F}\) 通过两个分支分别作 Semantic Segmentation 以及预测每个点与该点对应的目标重心点的 Offset，得到每个点的类别 \(s _ i\) 以及 \(o _ i=(\Delta x _ i,\Delta y _ i,\Delta z _ i)\)。然后经过 Clustering Part 作 Instance 聚类。最后用 ScoreNet 预测 Instance 的分数，用于 NMS 去除重合的 Instance。</p>
<h2 id="backbone">2. Backbone</h2>
<p>　　对于 Semantic Segmentation Branch，在 \(\mathbf{F}\) 之后加入 MLP 网络输出语义类别分数 \(\mathbf{SC}=\{sc _ 1,...,sc _ N\}\in\mathbb{R}^{N\times N _ {class}}\)。最终的类别 \(s _ i = \mathrm{argmax}(sc _ i)\)。<br>
　　对于 Offset Prediction Branch，输出 \(N\) 个点的 \(\mathbf{O}=\{o _ 1,...,o _ N\}\in\mathbb{R} ^ {N\times 3}\)。采用 L1 Loss： <span class="math display">\[ L _ {o_reg} = \frac{1}{\sum _ i m _ i}\sum _ i\Vert o _ i-(\hat{c} _ i-p _ i)\Vert\cdot m _ i\tag{1}\]</span> 其中 \(\mathbf{m} = \{m _ i,...,m _ N\}\) 是一个二进制 mask，\(m _ i=1\) 表示第 \(i\) 个点属于一个 Instance。\(\hat{c} _ i\) 为 Instance 的重心： <span class="math display">\[\hat{c} _ i=\frac{1}{N _ {g(i)} ^ I}\sum _ {j\in I _ {g(i)}}p _ j\tag{2}\]</span> 其中 \(N _ {g(i)} ^ I \) 表示 Instance \(I _ {g(i)}\) 中点的个数。此外，考虑到尺寸大的目标其边缘点的 offset 较难回归，所以加入方向约束的 loss： <span class="math display">\[L _ {o _ dir}=-\frac{1}{\sum _ i m _ i}\sum _ i\frac{o _ i}{\Vert o _ i\Vert _ 2}\cdot\frac{\hat{c} _ i-p _ i}{\Vert \hat{c} _ i-p _ i\Vert _ 2}\cdot m _ i\tag{3}\]</span></p>
<h2 id="clustering-part">3. Clustering Part</h2>
<p>　　有了点云的语义标签以及每个点相对目标物体重心的 offset 后，接下来将点云聚类成对应的 Instance。设点云原始坐标为 \(\mathbf{P}=\{p _ i\}\)，经过 offset 变换后坐标为 \(\mathbf{Q}=\{q _ i = p _ i+o _ i\in\mathbb{R} ^ 3\}\)。根据 \(\mathbf{Q}\) 来作聚类，能更容易的区分相邻的同类别的物体；但是对于目标的边缘点，offset 容易预测错误，所以再加上根据 \(\mathbf{P}\) 来作聚类。最终获得的聚类 Instance 为 \(\mathbf{C}=\mathbf{C} ^ p\cup\mathbf{C} ^ q=\{C _ 1 ^ p,...,C _ {M _ p}^p\}\cup\{C _ 1 ^ q,...,C _ {M _ q} ^ q\}\)。 <img src="/paper-reading-PointGroup/cluster.png" width="50%" height="50%" title="图 2. Clustering"> 　　如图 2. 所示，聚类算法就是一个基于点集的 BFS 搜索，这里需要设定 ball query 的半径 \(r\)。</p>
<h2 id="scorenet">4. ScoreNet</h2>
<p>　　经过基于坐标 \(\mathbf{P},\mathbf{Q}\) 聚类后，总共得到 \(\mathbf{C} = \{C _ 1,...,C _ M\}\)。因为这里面会有重叠的 Instance，所以 ScoreNet 用来评价这些 Instance 的质量，然后作 NMS 操作，从而达到综合两者聚类优势的效果。 <img src="/paper-reading-PointGroup/score.png" width="90%" height="90%" title="图 3. ScoreNet"> 　　如图 3. 所示，对于每个 Cluster，将其点特征加上点坐标作为点特征输入到网络。然后采用 Backbone 相似的结构，最终得到 Clustering 分数：\(\mathbf{S} _ c=\{s _ 1 ^ c,...,s _ M ^ c\}\)。<br>
　　对于评价 cluster 质量的标签，可以直接用 0/1，但是本文使用了 soft 形式： <span class="math display">\[\hat{s} _ i ^ c=\left\{\begin{array}{l}
0  &amp;\;iou _ i &lt; \theta _ l\\
1  &amp;\;iou _ i &lt; \theta _ h\\
\frac{1}{\theta _ h-\theta _ t}\cdot (iou _ i - \theta _ l) &amp;\;otherwise
\end{array}\tag{4}\right.\]</span> 其中 \(\theta _ l,\theta _ h \) 分别设为 0.25，0.75。然后用 binary cross-entropy 作为 Loss： <span class="math display">\[L _ {c_score} = -\frac{1}{M}\sum _ {i=1} ^ M\left(\hat{s} _ i ^ clog(s _ i ^ c)+(1-\hat{s} _ i ^ c)log(1-s _ i ^ c)\right)\tag{5}\]</span></p>
<h2 id="experiments">5. Experiments</h2>
<p><img src="/paper-reading-PointGroup/ablation.png" width="90%" height="90%" title="图 4. Ablation"> 　　如图 4. 所示，用 \(\mathbf{P,Q}\) 作聚类，效果提升还是比较明显的，能同时综合二者的聚类优势。</p>
<h2 id="reference">6. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Li.Jiang, PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</p>
]]></content>
      <categories>
        <category>Segmentation</category>
        <category>Instance Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Segmentation</tag>
        <tag>Autonomous Driving</tag>
        <tag>Instance Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;PnPNet&quot;</title>
    <url>/paper-reading-PnPNet/</url>
    <content><![CDATA[<p>　　自动驾驶的障碍物状态估计功能模块中，包含 perception/Detection，tracking，prediction 三个环节。传统的做法这三个环节是分步进行的，Detection 出目标框检测结果；Tracking 则作前后帧目标的数据关联然后用卡尔曼平滑并估计目标状态；Prediction 预测目标未来的运动轨迹。 <img src="/paper-reading-PnPNet/diff-pipe.png" width="60%" height="60%" title="图 1. Perception and Prediction"> 　　如图 1. 所示，(a) 代表传统的做法，每个步骤都是独立优化并出结果，这种方式将功能模块解耦，容易找到具体问题的位置，但是会降低算法找到最优解的概率；(b) 则将 Detection 与 Prediction 用同一个网络预测，然后用 Tracking 来平滑估计整个运动轨迹(代表方法是 <a href="/paperreading-Fast-and-Furious/" title="Fast and Furious">Fast and Furious</a>)，这种方法下 Tracking 中丰富的时序及空域特征信息没有作用于 Detection 和 Prediction；本文提出的 PnP<a href="#1" id="1ref"><sup>[1]</sup></a>方法则将三个环节作深度的特征再利用，即整个功能模块是 End-to-End 可训练的，更容易得到目标状态及预测的全局最优解，更容易处理遮挡等问题。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-PnPNet/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，PnP 网络包含 Detection、Tracking，Motion Forecasting 三个模块。网络输入为点云及 HD Map。检测模块包含一个任意的 3D 目标检测网络，以及一个存储历史 BEV 特征图的 Memory；Tracking 跟踪模块包含一个存储目标历史轨迹的 Memory，首先作 Track-Detection 的数据关联，然后优化目标历史轨迹并更新存储；Motion Forecasting 模块则根据历史轨迹作目标的运动预测。</p>
<h2 id="object-detection">2. Object Detection</h2>
<p>　　网络的输入为序列点云(本文采用 0.5s)及 HD Map，分别将点云在俯视图下体素化后在特征通道维度进行串联得到 \(\mathbf{x} ^ t\)，然后输入 Backbone 网络得到俯视图下特征图： <span class="math display">\[\mathcal{F} ^ t _ {bev}(\mathbf{x} ^ t) = \mathrm{CNN} _ {bev}(\mathbf{x} ^ t) \tag{1}\]</span> 最后加入 3D 目标检测头，得到 3D 目标框属性 \((u _ i ^ t, v _ i ^ t,w _ i,l _ i,\theta _ i ^ t)\) 的预测： <span class="math display">\[\mathcal{D} ^ t=\mathrm{CNN} _ {det}(\mathcal{F} ^ t _ {bev})\tag{2}\]</span></p>
<h2 id="discrete-continuous-tracking">3. Discrete-Continuous Tracking</h2>
<p>　　<strong>Tracking 模块包括离散的数据关联问题，以及连续的目标运动轨迹(状态)估计问题。</strong>目标运动轨迹的优化估计对之后的目标运动预测非常重要。</p>
<h3 id="trajectory-level-object-representation">3.1. Trajectory Level Object Representation</h3>
<p><img src="/paper-reading-PnPNet/trajectory.png" width="90%" height="90%" title="图 3. Trajectory Level Object Representation"> 　　Tracking 需要优化历史轨迹，Prediction 需要预测未来轨迹，所以轨迹级别的目标特征提取及表达非常重要。本文采用 LSTM 网络来表征。如图 3. 所示，对于轨迹 \(\mathcal{P} _ i ^ t=\mathcal{D} _ i ^ {t _ 0...t}\)，首先提取每个时刻目标的感知特征： <span class="math display">\[f _ i^{bev,t} = \mathrm{BilinearInterp}(\mathcal{F} _ {bev} ^ t,(u _ i ^ t, v _ i ^ t)) \tag{3}\]</span> 然后提取目标运动特征： <span class="math display">\[f _ i ^ {velocity,t}=(\dot{x} _ i ^ t,\dot{x} _ {ego} ^ t, \dot{\theta} _ {ego} ^ t)\tag{4}\]</span> 其中 \(\dot{x} _ i,\dot{x} _ {ego}\) 分别是第 \(i\) 个目标及本车的二维速度，通过位置差计算得到，对于新目标，将其设定为 0。由此得到第 \(i\) 个目标的特征： <span class="math display">\[f(\mathcal{D} _ i ^ t)=\mathrm{MLP} _ {merge}\left(f _ i^{bev,t},f _ i ^ {velocity,t}\right)\tag{5}\]</span> 最后通过 LSTM 网络来提取轨迹级别目标特征： <span class="math display">\[h(\mathcal{P} _ i ^ t)=\mathrm{LSTM}(f(\mathcal{D} _ i ^ {t _ 0...t}))\tag{6}\]</span></p>
<h3 id="data-association">3.2. Data Association</h3>
<p>　　当前时刻检测的目标数量为 \(N _ t\)，上一时刻目标轨迹数量为 \(M _ {t-1}\)，将二者关联匹配就是数据关联问题。这在有新目标出现以及目标出现遮挡的时候变得较为困难。类似传统方法，这里设计检测与跟踪轨迹的相似性矩阵 \(C\in\mathbb{R} ^ {N _ t\times (M _ {t-1}+N _ t)}\)(跟踪轨迹加入\(N _ t\)个目标是为了处理新出现目标的情况)： <span class="math display">\[C _ {i,j}=\left\{\begin{array}{l}
\mathrm{MLP} _ {pair}\left(f(\mathcal{D} _ i ^ t),h(\mathcal{P} _ j ^ {t-1})\right) &amp;\;\; \mathrm{if}\; 1\leq j\leq M _ {t-1},\\
\mathrm{MLP} _ {unary}\left(f(\mathcal{D} _ i ^ t)\right) &amp;\;\; \mathrm{if}\; j=  M _ {t-1} + i,\\
-\mathrm{inf} &amp;\;\; \mathrm{otherwise}
\end{array}\tag{7}\right.\]</span> 其中 \(\mathrm{MLP} _ {pair}\) 计算检测与跟踪轨迹的相似性分数，\(\mathrm{MLP} _ {unary}\) 计算目标是新出现的概率。有了该相似性矩阵，即可通过匈牙利算法求解最佳匹配对。<br>
　　对于被遮挡的物体，跟踪轨迹在当前帧容易出现没有检测的情况，本文引入单目标跟踪的思想作跟踪搜索。设未匹配的跟踪轨迹为 \(\mathcal{P} _ j ^ {t-1}\)，那么根据上一帧该轨迹目标的位置 \(u _ j ^ {t-1}, v _ j ^ {t-1}\)，进行运动补偿后为 \(\tilde{u} _ j ^ {t}, \tilde{v} _ j ^ {t}\)，在其邻域 \(\Omega _ j\) 内寻找最优的检测(跟踪)结果 \(\tilde{\mathcal{D}} _ k ^ t\)： <span class="math display">\[k = \mathop{\arg\max}\limits _ {i\in\Omega _ j} \mathrm{MLP} _ {pair}\left(f(\tilde{\mathcal{D}} _ i ^ t),h(\mathcal{P} _ j ^ {t-1})\right)\tag{8}\]</span> 其中 \(\Omega _ j\) 设计为目标的最大假设速度，如 \(110 km/h\)。<br>
　　最终可得到 \(N _ t+ K _ t\) 个目标轨迹，其中 \(K _ t\) 为未匹配的目标轨迹而通过单目标跟踪方法召回的轨迹数量。</p>
<h3 id="trajectory-estimation">3.3. Trajectory Estimation</h3>
<p>　　当前帧的观测加入到目标轨迹后，可进一步对目标轨迹作优化以减少 FP 以及提高轨迹定位精度。网络预测轨迹的置信度以及最近 \(T _ 0\) 时间内目标位置的残差： <span class="math display">\[\mathrm{score} _ i,\Delta u _ i ^ {t-T _ 0+1:t},\Delta v _ i ^ {t- T _ 0+1:t}=\mathrm{MLP} _ {refine}(h(\mathcal{P} _ i^t))\tag{9}\]</span> 其中 \(T _ 0\) 小于轨迹的总时间。最后用 NMS 去掉重叠的目标轨迹以消除 FP 与重叠项。</p>
<h2 id="motion-forecasting">4. Motion Forecasting</h2>
<p>　　根据优化后的目标轨迹，通过网络预测目标的未来轨迹： <span class="math display">\[\Delta u _ i^{t:t+\Delta T}, \Delta v _ i ^ {t:t+\Delta T}=\mathrm{MLP} _ {predict}(h(\mathcal{P} _ i^t))\tag{10}\]</span></p>
<h2 id="end-to-end-learning">5. End-to-End Learning</h2>
<p>　　整个网络多任务联合训练的 Loss 为： <span class="math display">\[\begin{align}
\mathcal{L} &amp;= \mathcal{L} _ {detect} + \mathcal{L} _ {track} + \mathcal{L} _ {predict}\\
&amp;= \mathcal{L} _ {detect} + \mathcal{L} _ {score} ^ {affinity} + \mathcal{L} _ {score} ^ {sot} + \mathcal{L} _ {socre} ^ {refine} + \mathcal{L} _ {reg} ^ {refine} + \mathcal{L} _ {predict}
\end{align}\tag{11}\]</span> 其中 \(\mathcal{L} _ {score}\) 为 \(max-margin \;loss\): <span class="math display">\[\mathcal{L} _ {score} = \frac{1}{N _ {i,j}}\sum _ {i\in pos,j\in neg} \mathrm{max}(0,m-(a _ i-a _ j))\tag{12}\]</span> 对于 \(\mathcal{L} _ {score} ^ {affinity}\) 和 \(\mathcal{L} _ {score} ^ {sot}\)，计算正样本与所有负样本的 Loss；对于 \(\mathcal{L} _ {score} ^ {refine}\)，与真值框 IoU 较高的，则 score 较高，这样作 NMS 时可以该 score 为准则。</p>
<h2 id="reference">6. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Liang, Ming, et al. "PnPNet: End-to-End Perception and Prediction with Tracking in the Loop." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
        <tag>MOT</tag>
        <tag>Prediction</tag>
      </tags>
  </entry>
  <entry>
    <title>A General Framework for Uncertainty Estimation in Deep Learning</title>
    <url>/A-General-Framework-for-Uncertainty-Estimation-in-deep-learning/</url>
    <content><![CDATA[<p>　　Uncertainty 估计在深度学习网络预测中同样非常重要，因为我们不仅需要知道预测结果，还想知道该结果的不确定性。Uncertainty 可分为偶然不确定性(Aleatoric Uncertainty，详见 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a>) 以及认知不确定性(Epistemic Uncertainty，详见 <a href="/Epistemic-Uncertainty-for-Active-Learning/" title="Epistemic Uncertainty for Active Learning">Epistemic Uncertainty for Active Learning</a>)。<br>
　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种同时估计偶然不确定性与认知不确定性的方法。对于网络输入 \(\mathbf{x}\)，输出的后验概率为 \(p(\mathbf{y}|\mathbf{x})\)，那么由 Aleatoric Uncertainty 和 Epistemic Uncertainty 构成的总的不确定性为 \(\sigma _ {tot} = \mathbf{Var} _ {p(\mathbf{y}|\mathbf{x})}(\mathbf{y})\)。</p>
<h2 id="aleatoric-uncertaintydata-uncertainty">1. Aleatoric Uncertainty(Data Uncertainty)</h2>
<p>　　假设传感器得到的数据符合噪音水平 \(\mathbf{v}\) 的高斯分布，那么输入网络的数据 \(\mathbf{z}\) 与其真实数据 \(\mathbf{x}\) 的关系为： <span class="math display">\[q(\mathbf{z}|\mathbf{x})\sim \mathcal{N}(\mathbf{z};\mathbf{x},\mathbf{v})\tag{1}\]</span> 为了计算网络输出的 Data Uncertainty，通过 Assumed Density Filtering(ADF) 来传递输入数据的噪音。网络的联合概率分布为： <span class="math display">\[p(\mathbf{z}^{(0:l)})=p(\mathbf{z}^{(0)})\prod _ {i=1} ^ l p(\mathbf{z}^{(i)}|\mathbf{z} ^ {(i-1)}) \tag{2}\]</span> 其中： <span class="math display">\[p(\mathbf{z}^{(i)}|\mathbf{z}^{(i-1)})=\sigma[\mathbf{z} ^ {(i)}-\mathbf{f} ^ {(i)}(\mathbf{z}^{(i-1)})]\tag{3}\]</span> ADF 将其近似为： <span class="math display">\[p(\mathbf{z}^{(0:l)})\approx q(\mathbf{z}^{(0:l)})=q(\mathbf{z}^{(0)})\prod _ {i=1} ^ l q(\mathbf{z}^{(i)}) \tag{4}\]</span> 其中 \(q(\mathbf{z})\) 符合独立高斯分布： <span class="math display">\[q(\mathbf{z}^{(i)})\sim \mathcal{N}\left(\mathbf{z}^{(i)};\mathbf{\mu}^{(i)},\mathbf{v}^{(i)}\right)=\prod _ j\left(\mathbf{z} _ j ^ {(i)};\mathbf{\mu} _ j ^ {(i)}, \mathbf{v} _ j ^ {(i)}\right)\tag{5}\]</span> 特征 \(\mathbf{z}^{(i-1)}\) 通过第 \(i\) 层映射方程 \(\mathbf{f} ^{(i)}\)，得到： <span class="math display">\[\hat{p}(\mathbf{z}^{(0:i)})=p(\mathbf{z}^{(i)}|\mathbf{z}^{(i-1)})q(\mathbf{z}^{(0:i-1)}) \tag{6}\]</span> ADF 的目标是找到 \(\hat{p}(\mathbf{z}^{(0:i)})\) 的近似分布 \(q(\mathbf{z}^{(0:i)})\)，比如 KL divergence： <span class="math display">\[q(\mathbf{z}^{(0:i)})=\mathop{\arg\min}\limits _ {\hat{q}(\mathbf{z}^{(0:i)})}\mathbf{KL}\left(\hat{q}(\mathbf{z}^{(0:i)})\;\Vert\;\hat{p}(\mathbf{z}^{(0:i)})\right)\tag{7}\]</span> 基于高斯分布的假设，以上解为： <span class="math display">\[\begin{align}
\mathbf{\mu}^{(i)}=\mathbb{E} _ {q(\mathbf{z}^{(i-1)})}[\mathbf{f}^{(i)}(z^{(i-1)})]\\
\mathbf{v}^{(i)}=\mathbb{V} _ {q(\mathbf{z}^{(i-1)})}[\mathbf{f}^{(i)}(z^{(i-1)})]\\
\end{align}\tag{8}\]</span></p>
<h2 id="epistemic-uncertaintymodel-uncertainty">2. Epistemic Uncertainty(Model Uncertainty)</h2>
<p>　　Model Uncertainty 表征的是模型的不确定性，对于训练集 \(\mathrm{D}=\{\mathbf{X},\mathbf{Y}\}\)，模型的不确定性即为权重参数的分布 \(p(\mathbf{\omega} | \mathbf{X},\mathbf{Y})\)。可采用 Monte-Carlo 采样方法来近似估计模型权重分布，具体的采样通过 Dropout 实现： <span class="math display">\[p(\omega|\mathbf{X},\mathbf{Y})\approx q(\mathbf{\omega};\mathbf{\Phi})=Bern(\mathbf{\omega};\mathbf{\Phi}) \tag{9}\]</span> 其中 \(\mathbf{\Phi}\) 是 Bernolli(dropout) rates，由此模型的不确定性即为 T 次采样的方差： <span class="math display">\[\mathbf{Var} _ {p(\mathbf{y}|\mathbf{x})} ^ {model}(\mathbf{y})=\sigma _ {model} = \frac{1}{T}\sum _ {t=1} ^ T(\mathbf{y} _ t-\bar{\mathbf{y}}) ^ 2\tag{10}\]</span> 其中 \(\{\mathbf{y} _ t\} _ {t=1} ^ T\) 是不同权重 \(\omega ^ t\sim q(\omega;\mathbf{\Phi})\) 采样下的输出。<br>
　　这种模型不确定性的计算方式，直观的理解为：当模型对某些数据预测比较好，误差比较小的时候，那么模型对这些数据的冗余度肯定是较高的，所以去掉模型的一部分网络，模型对这些数据的预测与原模型应该会有较高的一致性。</p>
<h2 id="total-uncertainty">3. Total Uncertainty</h2>
<p><img src="/A-General-Framework-for-Uncertainty-Estimation-in-deep-learning/framework.png" width="95%" height="95%" title="图 1. Framework"> 　　如图 2.2 所示，结合蒙特卡洛采样与 ADF 方法，网络预测的结果与对应的总的 Uncertainty 可计算为： <span class="math display">\[\left\{\begin{array}{l}
\mu = \frac{1}{T}\sum _ {t=1} ^ T \mathbf{\mu} _ t ^ {(l)}\\
\sigma _ {tot} = \frac{1}{T}\sum _ {t=1} ^ {T} \mathbf{v} _ t ^ {(l)} + \frac{1}{T}\sum _ {t=1} ^ T\left(\mathbf{\mu} _ t ^ {(l)}-\bar{\mathbf{\mu}}\right) ^ 2
\end{array}\tag{11}\right.\]</span> 其中 \(\{\mathbf{\mu} _ t ^ {(l)},\mathbf{v} _ t ^ {(l)}\} _ {t=1} ^ T\) 是 ADF 网络 \(T\) 次蒙特卡洛采样结果。<strong>由此可见，不同于以往将 Model Uncertainty 和 Data Uncertainty 完全作独立假设的方式，本文方法是将二者联合来估计的。这也比较好理解，如果数据噪音很大，那么模型的不确定性也会很大，所以二者不可能是完全独立的</strong>。<br>
　　该方法可归纳为：</p>
<ol type="1">
<li>将现有的网络转换为 ADF 网络形式；</li>
<li>手机 \(T\) 次蒙特卡洛采样的网络输出；</li>
<li>计算网络输出的 Mean 和 Variance；</li>
</ol>
<p>其中步骤一不用作任何额外训练的操作，神经网络中的每个操作都有对应的 ADF 操作，详见<a href="#2" id="2ref">[2]</a>，代码可参考<a href="#3" id="3ref">[3]</a>。比如基于 Pytorch的 2D 卷积： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Conv2d(_ConvNd):</span><br><span class="line">    def __init__(self, in_channels, out_channels, kernel_size, stride=1,</span><br><span class="line">        padding=0, dilation=1, groups=1, bias=True,</span><br><span class="line">        keep_variance_fn=None, padding_mode=&apos;zeros&apos;):</span><br><span class="line">        self._keep_variance_fn = keep_variance_fn</span><br><span class="line">        kernel_size = _pair(kernel_size)</span><br><span class="line">        stride = _pair(stride)</span><br><span class="line">        padding = _pair(padding)</span><br><span class="line">        dilation = _pair(dilation)</span><br><span class="line">        super(Conv2d, self).__init__(</span><br><span class="line">            in_channels, out_channels, kernel_size, stride, padding, dilation,</span><br><span class="line">            False, _pair(0), groups, bias, padding_mode)</span><br><span class="line"></span><br><span class="line">        def forward(self, inputs_mean, inputs_variance):</span><br><span class="line">            outputs_mean = F.conv2d(inputs_mean, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)</span><br><span class="line">            outputs_variance = F.conv2d(inputs_variance, self.weight ** 2, None, self.stride, self.padding, self.dilation, self.groups)</span><br><span class="line">            if self._keep_variance_fn is not None:</span><br><span class="line">                outputs_variance = self._keep_variance_fn(outputs_variance)</span><br><span class="line">                return outputs_mean, outputs_variance</span><br></pre></td></tr></table></figure></p>
<p>Softmax: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Softmax(nn.Module):</span><br><span class="line">    def __init__(self, dim=1, keep_variance_fn=None):</span><br><span class="line">        super(Softmax, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self._keep_variance_fn = keep_variance_fn</span><br><span class="line"></span><br><span class="line">        def forward(self, features_mean, features_variance, eps=1e-5):</span><br><span class="line">            &quot;&quot;&quot;Softmax function applied to a multivariate Gaussian distribution.</span><br><span class="line">            It works under the assumption that features_mean and features_variance </span><br><span class="line">            are the parameters of a the indepent gaussians that contribute to the </span><br><span class="line">            multivariate gaussian. </span><br><span class="line">            Mean and variance of the log-normal distribution are computed following</span><br><span class="line">            https://en.wikipedia.org/wiki/Log-normal_distribution.&quot;&quot;&quot;&quot;&quot;</span><br><span class="line">                    </span><br><span class="line">            log_gaussian_mean = features_mean + 0.5 * features_variance</span><br><span class="line">            log_gaussian_variance = 2 * log_gaussian_mean</span><br><span class="line">                    </span><br><span class="line">            log_gaussian_mean = torch.exp(log_gaussian_mean)</span><br><span class="line">            log_gaussian_variance = torch.exp(log_gaussian_variance)</span><br><span class="line">            log_gaussian_variance = log_gaussian_variance*(torch.exp(features_variance)-1)</span><br><span class="line">                    </span><br><span class="line">            constant = torch.sum(log_gaussian_mean, dim=self.dim) + eps</span><br><span class="line">            constant = constant.unsqueeze(self.dim)</span><br><span class="line">            outputs_mean = log_gaussian_mean/constant</span><br><span class="line">            outputs_variance = log_gaussian_variance/(constant**2)</span><br><span class="line">                    </span><br><span class="line">            if self._keep_variance_fn is not None:</span><br><span class="line">                outputs_variance = self._keep_variance_fn(outputs_variance)</span><br><span class="line">                return outputs_mean, outputs_variance</span><br></pre></td></tr></table></figure></p>
<h2 id="experiments">4. Experiments</h2>
<p>　　本文在 Steering Angle Prediction，Object Future Motion Prediction，Object Recognition，Closed-Loop Control of a Quadrotor 等任务上作了 Uncertainty 的估计，用 KL，NLL 来评估预测好坏。KL 本质上用于描述两个分布的距离。NLL(Negative Log-likelihood) 数学形式为 \(\frac{1}{2}\mathrm{log}(\sigma _ {tot})+\frac{1}{2\sigma _ {tot}}(y _ {gt}-y _ {pred}) ^ 2\)，即为 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中预测 Uncertainty 方法中的 Loss 项。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Loquercio, Antonio , Segù, Mattia, and D. Scaramuzza . "A General Framework for Uncertainty Estimation in Deep Learning." (2019).<br>
<a id="2" href="#2ref">[2]</a> Gast, Jochen , and S. Roth . "Lightweight Probabilistic Deep Networks." (2018).<br>
<a id="3" href="#3ref">[3]</a> https://github.com/mattiasegu/uncertainty_estimation_deep_learning/blob/master/contrib/adf.py</p>
]]></content>
      <categories>
        <category>Uncertainty</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;AFDet&quot;</title>
    <url>/paper-reading-AFDet/</url>
    <content><![CDATA[<p>　　点云目标检测方法已趋于完善，为了能在嵌入式系统上高效运行点云目标检测算法，地平线提出了 AFDet <a href="#1" id="1ref"><sup>[1]</sup></a>，该文章发表在 CVPR2020 Workshop 上，算是很工程化的一个工作了，对工程产品落地有很好的参考价值。AFDet 应用了很多 Anchor-Free 2D 目标检测思想，可参考 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-AFDet/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　AFDet 是一种 Anchor-Free，NMS-Free 的检测方法，所以后处理非常简单，高效。如图 1. 所示，AFDet 采用了传统的 Birdview 下的 Point Cloud Encoder，Backbone &amp; Necks，Anchor-Free Detector 三种网络结构。Point Cloud Encoder 可采用 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 结构，Backbone &amp; Necks 这里也不作展开。这里最重要的设计是 Anchor-Free 的检测头，由 Keypoint Heatmap，Local Offset Head，z-axis Location Head，3D Object Size Head，Orientation Head 等五个分支构成。</p>
<h3 id="keypoint-heatmap-local-offset-head">1.1. Keypoint Heatmap &amp; Local Offset Head</h3>
<p>　　BEV 下目标定位由 Heatmap \(M\in\mathbb{R} ^ {W\times H\times C}\) 和 Offset Regression Map \(O\in\mathbb{R} ^ {W\times H\times 2}\) 组成，其中 \(C\) 为 Keypoint 类型。Offset Head 是为了消除 Voxel 后的量化误差以预测更准确的目标位置。<br>
　　对于第 \(k\) 个类别为 \(c _ k\) 的目标，其 3D 属性为：\((x ^ {(k)},y ^ {(k)},z ^ {(k)},w ^ {(k)},l ^ {(k)},h ^ {(k)},\theta ^ {(k)})\)。设 Pillar 边长为 \(b\)，那么在 BEV 栅格图上，目标中心点作为关键点的坐标为 \(\bar{p}=\left(\left\lfloor\frac{x ^ {(k)}-back}{b}\right\rfloor,\left\lfloor\frac{y ^ {(k)}-left}{b}\right\rfloor\right)\in\mathbb{R} ^ 2\)，其中 \([(back,front),(left,right)]\) 为 \(x-y\) 平面检测范围。由此，目标在 BEV 下的 2D 属性框表示为 \(\left(\left\lfloor\frac{x ^ {(k)}-back}{b}\right\rfloor,\left\lfloor\frac{y ^ {(k)}-left}{b}\right\rfloor,\left\lfloor\frac{w ^ {(k)}}{b}\right\rfloor,\left\lfloor\frac{l ^ {(k)}}{b}\right\rfloor,\theta ^ {(k)}\right)\)。<br>
　　对于 BEV Heatmap 分支的真值，需要根据目标框真值来生成。对于 Heatmap 中的像素点 \((x,y)\)，设计其值为： <span class="math display">\[M _ {x,y,z} =
\left\{\begin{array}{l}
1, &amp;\mathrm{if}\;d=0\\
0.8, &amp;\mathrm{if}\; d=1\\
\frac{1}{d}, &amp;\mathrm{else}
\end{array}\tag{1}\right.\]</span> 其中 \(d\) 表示目标框中心点与对应像素点的距离，Heatmap 中预测量 \(\hat{M} _ {x,y,c}=1\) 表示其为目标框中心点，\(\hat{M} _ {x,y,c}=0\) 则表示是背景。Heatmap 中 \(\bar{p}\) 位置定义为正样本点，其余 Pillars 为负样本点，使用 Focal Loss： <span class="math display">\[\mathcal{L} _ {heat} = -\frac{1}{N}\sum _ {x,y,c}
\left\{\begin{array}{l}
\left(1-\hat{M} _ {x,y,c}\right) ^ {\alpha}\;\mathrm{log}\left(\hat{M} _ {x,y,c}\right), \;\mathrm{if}\; M _ {x,y,c} = 1 \\
\left(1-M _ {x,y,c}\right) ^ {\beta}\; \left(\hat{M} _ {x,y,c}\right) ^ {\alpha}\mathrm{log}\left(1-\hat{M} _ {x,y,c}\right), \;\mathrm{else} \\
\end{array}\tag{2}\right.\]</span> 　　另一方面，Offset Regression 分支可以解决量化误差，以及当 Heatmap 中心点分类错误的时候，补救预测准确的中心点位置。选择中心点周围半径 \(r\) 区域作 Offset 预测： <span class="math display">\[\mathcal{L} _ {off} = \frac{1}{N}\sum _ p\sum ^ r _ {\sigma =-r}\sum ^ r _ {\epsilon = -r}\left\vert\hat{O} _ {\bar{p}}-b(p-\bar{p}+(\sigma,\epsilon))\right\vert\tag{3}\]</span> 只对 \(2r+1\) 的矩形区域作 Offset 预测。</p>
<h3 id="z-axis-location-head">1.2. z-axis Location Head</h3>
<p>　　高度预测值 \(\hat{Z}\in\mathbb{R} ^ {W\times H\times 1}\)，其 Loss 为： <span class="math display">\[\mathcal{L _ z} = \frac{1}{N}\sum _ {k=1} ^ N\left\vert\hat{Z} _ {p ^ {(k)}}-z ^ {(k)}\right\vert\tag{4}\]</span></p>
<h3 id="d-object-size-head">1.3. 3D Object Size Head</h3>
<p>　　尺寸预测值 \(\hat{S}\in\mathbb{R} ^ {W\times H\times 3}\)，其 Loss 为： <span class="math display">\[\mathcal{L} _ {size} = \frac{1}{N}\sum _ {k=1} ^ N\left\vert\hat{S} _ {p ^ {(k)}}-s ^ {(k)}\right\vert\tag{5}\]</span> 其中 \(s ^ {(k)} = (w ^ {(k)},l ^ {(k)}, h ^ {(k)})\)。</p>
<h3 id="orientation-head">1.4. Orientation Head</h3>
<p>　　与传统的一样，将角度预测分解为 bin 分类＋ offset 回归两个任务。具体的，分成两个 bin：\(\Psi _ 1 =[-\frac{7\pi}{6}, \frac{\pi}{6}]\)；\(\Psi _ 2 =[-\frac{\pi}{6}, \frac{7\pi}{6}]\)。对于每个 bin，softmax 分类 \(\hat{\mu} _ i ^ {(k)}\in\mathbb{R} ^ 2\)，与 bin 中心夹角 \(\gamma _ i\) 的 sin/cos 值 \(\hat{v} _ i ^ {(k)}\)。Loss 为： <span class="math display">\[\mathcal{L} _ {ori} = \frac{1}{N}\sum _ {k=1}^N\sum _ {i=1}^2\left(\mathrm{softmax}\left(\hat{\mu} _ i ^ {(k)},\eta _ i ^ {(k)}\right)+\eta _ i ^ {(k)}\left\vert\hat{v} _ i ^ {(k)}-v _ i ^ {(k)}\right\vert\right)\tag{6}\]</span> 其中当 \(\theta ^ {(k)}\in\Psi _ i\) 时，\(\eta _ i ^ {(k)} = \mathbb{1}\)，\(v _ i^ {(k)}=\left(\mathrm{sin}(\theta ^ {(k)}-\gamma _ i), \mathrm{cos}(\theta ^ {(k)}-\gamma _ i)\right)\)。由此，预测的角度可通过如下方式解码： <span class="math display">\[\hat{\theta} ^ {(k)}=\mathrm{arctan2}\left(\hat{v} _ {j,1} ^ {(k)},\hat{v} _ {j,2} ^ {(k)}\right)+\gamma _ j\tag{7}\]</span> 　　<strong>因为是 Anchor Free 的方式，所以如果按照传统的方式， bin 数量较大，那么最后输出的 map 所占的内存也会相当大，所以这里只采用了两个 bin</strong>。这么做有很大好处，比如量化时数值的稳定性，所以在工程应用中非常值得借鉴思考。</p>
<h2 id="experiments">2. Experiments</h2>
<p><img src="/paper-reading-AFDet/res1.png" width="70%" height="70%" title="图 2. res1"> <img src="/paper-reading-AFDet/res2.png" width="70%" height="70%" title="图 3. res2"> 　　如图 2. 所示，AFDet 在同等计算量下，基本能达到 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 水平。图 3. 则对比了几种 Anchor-Based 方法，效果也较好。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Ge, Runzhou, et al. "Afdet: Anchor free one stage 3d object detection." arXiv preprint arXiv:2006.12671 (2020).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;EPNet&quot;</title>
    <url>/paper-reading-EPNet/</url>
    <content><![CDATA[<p>　　在大多数场景下，融合激光雷达与图像数据能有效提升各种深度学习任务性能。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种图像数据与激光雷达的前融合框架，并且考虑到分类分数与定位置信度的不一致性，提出了一种约束两者一致性的 Loss。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-EPNet/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，EPNet 主要由 Image Stream，Geometric Stream，LI-Fusion Module，及 Detect Head 组成。<br>
　　Image Stream 中，提取不同尺度的图像特征 \(F _ i(i = 1,2,3,4)\)，然后经过 2D Transposed Convolution 将不同尺度的特征变换到图像分辨率，得到特征 \(F _ U\)。<br>
　　Geometric Stream 用 PointNet++(<a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a>) 作特征提取，对应的 SA，FP 特征为 \(S _ i,P _ i(i =1,2,3,4)\)。\(S _ i,F _ i\) 通过 LI-Fusion 模块进行深度融合，此外 \(F _ U\) 与 \( P _ 4\) 也通过 LI-Fusion 进行融合。</p>
<h2 id="li-fusion-module">2. LI-Fusion Module</h2>
<p><img src="/paper-reading-EPNet/LI.png" width="70%" height="70%" title="图 2. LI-Fusion"> 　　LiDAR-guided Image Fusion Module 是图像点云两个数据流融合的核心模块。如图 2. 所示，LI-Fusion 由 Point-wise Correspondence Generation 和 LiDAR-guided fusion 两部分组成。Point-wise Correspondence Generation 又由 Grid Generator 和 Image Sampler 实现，对于点云中的点 \(p(x,y,z)\)，可得到不同尺度图像上的像素点 \(p'(x',y')\)： <span class="math display">\[p&#39;=M\times p\tag{1}\]</span> 其中 \(M\in\mathbb{R}^ {3\times 4}\)。\(p'\) 可能不是正好位于图像坐标像素点上，所以用双线性插值的方法取邻近像素点的特征值： <span class="math display">\[V^{(p)}=\mathcal{K}\left(F^{\mathcal{N}(p&#39;)}\right)\tag{2}\]</span> 其中 \(V^{(p)}\) 表示点 \(p\) 对应的图像点特征，\(\mathcal{K}\) 表示双线性插值，\(\left(F^{\mathcal{N}(p')}\right)\) 表示图像 \(p'\) 邻近点的特征。<br>
　　LiDAR-guided Fusion 考虑到不能直接将点的图像特征与点特征进行串联融合，因为图像特征容易受光照，遮挡等因素影响，所以通过点云对图像点特征进行重要性权重融合。如图 2. 所示，重要性权重设计为： <span class="math display">\[\mathbf{w}=\sigma\left(\mathcal{W}\;\mathrm{tanh}(\mathcal{U} F _ P+\mathcal{V}F _ I)\right)\tag{3}\]</span> 其中 \(\mathcal{W,U,V}\) 为 MLP 网络，\(\sigma\) 为 sigmoid 归一化函数。<br>
　　最终的融合特征为： <span class="math display">\[F _ {LI}=\mathrm{Concate}(F _ P,\mathbf{w}F _ I)\tag{4}\]</span></p>
<h2 id="consistency-enforcing-loss">2. Consistency Enforcing Loss</h2>
<p>　　NMS 操作时，一般用分类的分数，但是分类分数与定位置信度是不一致的。本文提出 Consistency Enforcing Loss，将定位与分类的分数监督成一致： <span class="math display">\[L _ {ce}=-log\left(c\times\frac{Area(D\cap G)}{Area(D\cup G)}\right)\tag{5}\]</span> 其中 \(D,G\) 分别为预测框与真值框，\(c\) 为分类分数，该 Loss 鼓励定位准的框分类分数越高。<br>
　　这与 IoU Loss 作用相似！</p>
<h2 id="ablation-study">3. Ablation Study</h2>
<p><img src="/paper-reading-EPNet/ablation.png" width="90%" height="90%" title="图 3. Ablation Study"> 　　如图 3. 所示，LI-Fusion 和 CE Loss 对检测性能提升还是比较明显的。此外，本文还对比了三种 Fusion 方式，另外两种为：SC(simple concatenation)，将原始图像像素值串联到对应的点云原始数据中，没有 Image Stream；SS(single scale)，只用最后一层的图像点云特征作融合。<br>
　　实验表明，SC 性能反而下降，SS 有所提升，但是 Multi-scale 的性能最好。结论就是，在一个尺度下，相对靠后的前融合可能比相对靠前的前融合效果更好(类比 <a href="/PointPainting/" title="PointPainting">PointPainting</a>，其虽然是前融合，但是直接提取的是图像的语义分割结果，所以相对靠后，效果也好)，当然多尺度的效果会是最好的。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Huang, Tengteng, et al. "EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection." arXiv preprint arXiv:2007.08856 (2020).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;PV-RCNN&quot;</title>
    <url>/paper-reading-PV-RCNN/</url>
    <content><![CDATA[<p>　　PV-RCNN<a href="#1" id="1ref"><sup>[1]</sup></a> 目前在 Waymo 数据集上排名第二，性能还是比较强悍的，顺便也看了下港中文多媒体实验室开源的 OpenPCDet<a href="#2" id="2ref"><sup>[2]</sup></a> 代码，收获还是蛮多，与图像点云通用的 mmdetection3d<a href="#3" id="3ref"><sup>[3]</sup></a> 各有优劣吧。虽然 PV-RCNN 对于实际应用还是略显复杂，以及超参数较麻烦，但是其相关思想还是非常值得借鉴。本文主要关注其 Point + Vexol 特征提取并融合的方式。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-PV-RCNN/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，PV-RCNN 首先将原始点云体素化，然后用 3D Sparse Convolution(<a href="/Rethinking-of-Sparse-3D-Convolution/" title="Rethinking of Sparse 3D Convolution">Rethinking of Sparse 3D Convolution</a>) 作 Voxel-level 的特征提取，并预测俯视图下的目标 ROI Proposal；另一方面，在原始点云中用 FPS 采样出特定数量的 key-point，然后通过 Voxel Set Abstraction 模块，将提取到的多尺度的 Voxel-level 特征融合到 key-point 特征；最后用 ROI-Grid 模块将 key-point 特征融合到 ROI Grid-Point 中，作进一步的 3D 目标框属性精细化预测。<br>
　　由此不仅利用了 Voxel-level 3D Sparse Convolution 的高效性，还利用了 Point-based 模型对局部信息提取更加精细有效的特性。总体上，PV-RCNN 框架中特征提取操作由两大块组成：1. Voxel-to-Keypoint Scene Encoding；2. Point-to-Grid RoI Feature Abstraction。</p>
<h2 id="voxel-to-keypoint-scene-encoding">2. Voxel-to-Keypoint Scene Encoding</h2>
<p>　　该模块的作用是将提取的特征用特定数量的 Keypoint 来表示。所以有 Keypoints Sampling，Voxel Set Abstraction Module，Extended VSA Module，Predicted Keypoint Weighting 等组成。</p>
<h3 id="keypoints-sampling">2.1. Keypoints Sampling</h3>
<p>　　因为期望采样的 Keypoints 能完全覆盖整个场景，所以采用 Furthest-Point-Sampling(FPS) 来采样 \(n\) 个点 \(\mathcal{K}=\{p _ 1,...,p _ n\}\)。对于 KITTI 数据集，取 \(n=2048\)，对于 Waymo 数据集，取 \(n=4096\)。</p>
<h3 id="voxel-set-abstraction-module">2.2. Voxel Set Abstraction Module</h3>
<p>　　VSA 模块将经过 3D Sparse Convolution 得到的多尺度的 Voxel 特征编码为 Keypoints 表达形式，与 <a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a> 类似，只不过这里点周围不是点，而是 Voxel。<br>
　　具体的，设尺度 \(k\) 的 3D voxel 特征集合为 \(\mathcal{F}^ {(l _ k)}=\{f _ 1 ^ {(l _ k)},...,f _ {N _ k}^{(l _ k)}\}\)，对应的 Voxel 3D 坐标为 \(\mathcal{V}^ {(l _ k)}=\{v _ 1 ^ {(l _ k)},...,v _ {N _ k}^{(l _ k)}\}\)，其中 \(N _ k\) 为非零 Voxel 数量。对于 keypoint \(p _ i\)，在半径 \(r _ k\) 内找到所有 voxel 的特征向量： <span class="math display">\[S _ i ^ {(l _ k)} = \left\{\left[f _ j^{(l _ k)};v _ j^{(l _ k)}-p _ i\right] ^ T 
\left\vert\begin{array}{l}
\Vert v _ j^{(l _ k)}-p _ i\Vert ^ 2 &lt; r _ k,\\
\forall v _ j^{(l _ k)} \in \mathcal{V} ^ {(l _ k)},\\
\forall f _ j ^ {(l _ k)} \in \mathcal{F} ^ {(l _ k)}
\end{array}\right.
\right\}\tag{1}\]</span> 其中 \(v _ j^{(l _ k)}-p _ i\) 为对应的 Voxel 与该点 \(p _ i\) 的相对位置，与特征向量串联得到该 Voxel 在该点的投影特征。由此用 PointNet 方式可得到该 Keypoint 融合领域内 Voxel 特征集 \(S _ i ^ {(l _ k)}\) 后的特征： <span class="math display">\[f _ i ^ {(pv _ k)}=\mathrm{max}\left\{G\left(\mathcal{M}(S _ i^{(l _ k)})\right)\right\}\tag{2}\]</span> 其中 \(\mathcal{M}\) 为随机采样 Voxel 的操作，目的是为了减少计算量；\(G\) 为 MLP 网络。<br>
　　本文采用了 4 个尺度的 Voxel 特征，每个尺度的领域半径 \(r _ k\) 根据感受野而变化。最终得到的多尺度的语义的 Keypoint 特征为： <span class="math display">\[f _ i^{(pv)} = \left[f _ i^{(pv _ 1)},f _ i^{(pv _ 2)},f _ i^{(pv _ 3)},f _ i^{(pv _ 4)}\right],\;\mathrm{for}\; i = 1,...,n\tag{3}\]</span></p>
<h3 id="extended-vsa-module">2.3. Extended VSA Module</h3>
<p>　　此外，Keypoint 还利用了原始点云的特征以及经过 3D Sparse Convolution 和 ToBEV 后的 2D BEV 特征。Keypoint 通过双线性插值的方式从 BEV 特征层中计算对应空间位置的特征。综上，Keypoint 特征为： <span class="math display">\[f _ i^{(p)} = \left[f _ i^{(pv)},f _ i^{(raw)},f _ i^{(bev)}\right],\;\mathrm{for}\; i = 1,...,n\tag{4}\]</span></p>
<h3 id="predicted-keypoint-weighting">2.4. Predicted Keypoint Weighting</h3>
<p><img src="/paper-reading-PV-RCNN/weight.png" width="50%" height="50%" title="图 2. PKW"> 　　通过 FPS 采样得到的 Keypoint 也包含了大量的背景点，所以需要弱化背景点的特征，强化前景点特征，以便之后前景目标框属性的精细化估计。如图 2. 所示，训练阶段，对 Keypoint 进行点级别前景背景分类，标签可由目标框内点自动生成；测试阶段，直接预测点的类别，然后作点特征的权重化整合： <span class="math display">\[\tilde{f _ i} ^ {(p)} = \mathcal{A}(f _ i^{(p)})\cdot f _ i^{(p)}\tag{5}\]</span> 其中 \(\mathcal{A}(\cdot)\) 为 MLP 网络。</p>
<h2 id="point-to-grid-roi-feature-abstraction">3. Point-to-Grid RoI Feature Abstraction</h2>
<p><img src="/paper-reading-PV-RCNN/roi.png" width="50%" height="50%" title="图 3. RoI-grid Pooling"> 　　得到了 Keypoint 特征 \(\mathcal{\tilde{F}}=\{\tilde{f} _ 1^{(p)},...,\tilde{f} _ n^{(p)}\}\) 以及俯视图下的 3D proposal ROI 后，可进一步提取 ROI 特征，以作 3D 目标框属性的精细化估计。如图 3. 所示，类似 Voxel-to-Point 的 Voxel Set Abstraction 模块，本文提出了 Point-to-Grid 的 Set Abstraction，称之为 ROI-Grid Pooling。具体的，对每个 ROI Proposal，采样 \(6\times 6\times 6\) 个栅格点：\(\mathcal{G}=\{g _ 1,...,g _ {216}\}\)。Set Abstraction 操作将 Keypoint 的特征映射到栅格点处。类似 VSA，首先在 \(\tilde{r}\) 半径内查找栅格点的周围 Keypoint： <span class="math display">\[\tilde{\psi} = \left\{\left[\tilde{f} _ j^{(p)};p _ j-g _ i\right] ^ T 
\left\vert\begin{array}{l}
\Vert p _ j-g _ i\Vert ^ 2 &lt; \tilde{r},\\
\forall p _ j\in \mathcal{K},\\
\forall \tilde{f} _ j ^ {(p)} \in \mathcal{\tilde{F}}
\end{array}\right.
\right\}\tag{6}\]</span> 类似的，再通过 PointNet 得到栅格点 \(g _ i\) 的特征： <span class="math display">\[\tilde{f} _ i ^ {(g)}=\mathrm{max}\left\{G\left(\mathcal{M}(\tilde{\psi})\right)\right\}\tag{7}\]</span> 　　由此得到所有 ROI 固定长度的特征向量，进而可在 ROI Proposal 的基础上，作最后的尺寸，角度，位置等属性的精细化估计，这里不做展开。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Shi, Shaoshuai, et al. "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br>
<a id="2" href="#2ref">[2]</a> https://github.com/open-mmlab/OpenPCDet<br>
<a id="3" href="#3ref">[3]</a> https://github.com/open-mmlab/mmdetection3d</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;MotionNet&quot;</title>
    <url>/paper-reading-MotionNet/</url>
    <content><![CDATA[<p>　　基于点云的 3D 感知一般通过 3D 目标框实现，但是如果直接网络出 3D 目标框属性，那么召回率很难达到非常高的水平(所以之前提到检测的任务可以分解为分割+后处理聚类来作，这样召回率会比较高)。本文提出的 MotionNet<a href="#1" id="1ref"><sup>[1]</sup></a> 用于检测俯视图下每个栅格的类别及轨迹，可作为检测的辅助，能召回更小的目标，以及没有标注的动态障碍物目标。这种栅格级别或点级别的目标探测能力相比直接出目标框的优势有：</p>
<ol type="1">
<li>目标框形式一般依赖于目标框区域的特征，不同目标类别之间的特征很难泛化，所以无法检测未见过的类别；</li>
<li>目标框形式一般会作 NMS 等处理去掉不确定性较大的框，而栅格级别的会保留；</li>
</ol>
<p><img src="/paper-reading-MotionNet/compare.png" width="40%" height="40%" title="图 1. Detection VS. Motion Prediction"> 　　如图 1. 所示，对于轮椅这种非正常类别(或预定义类别)的目标，检测任务可能会失效，此时 MotionNet 则会根据时序信息输出该区域的目标速度(未来运动轨迹)，这可作为 Motion Planning 阶段的另一重要线索。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-MotionNet/framework.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. 所示，MotionNet 输入为连续帧点云在当前坐标系下的俯视图表示，然后经过 Spatio-temporal Pyramid 网络作特征提取，最后三个分支网络输出三个信息：</p>
<ul>
<li>Cell Classification，每个 cell 的类别；</li>
<li>Motion Prediction，每个 cell 的未来轨迹；</li>
<li>State Estimation，每个 cell 是否静止的判断；</li>
</ul>
<h2 id="spatio-temporal-pyramid-network">2. Spatio-temporal Pyramid Network</h2>
<p><img src="/paper-reading-MotionNet/stpn.png" width="50%" height="50%" title="图 3. STPN"> 　　 将时序点云组织成多层的俯视图表达后，面临两个问题：1. 如何整合时序信息；2. 如何提取多尺度的空间及时序特征。本文设计了 STPN 网络，如图 3. 所示，STC 模块由 2D Convolution 以及 \(k\times 1\times 1\) 的 3D Convolution (本质上退化成 1D Convolution)组成；此外设计了金字塔式的特征提取结构，最终输出 \(1\times C\times H\times W\) 大小的特征图。相比于直接 3D 卷积形式，这种方式 2D + 1D 卷积方式极大提高了网络计算效率(很多地方用到这种方式，如 <a href="/paperreading-Fast-and-Furious/" title="FaF">FaF</a>)。</p>
<h2 id="output-heads">3. Output Heads</h2>
<p>　　三个分支输出的细节为：</p>
<ol type="1">
<li>Cell Classification，输出尺寸为 \(H\times W\times C\)，其中 \(C\) 为类别；</li>
<li>Motion Prediction，输出尺寸为 \(N\times H\times W\times 2\)，表示时间 \(\tau\in (t,t+N)\) 内 cell 的位置 \(\{X ^ {(\tau)}\} _ {\tau =t} ^ {t + N}\)，其中 \(X ^ {(\tau)}\in\mathbb{R} ^ {H\times W\times 2}\) 只估计平面 2D 的位置。</li>
<li>State Estimation，输出尺寸为 \(H\times W\)，表示 cell 静止的概率。</li>
</ol>
<p>　　直接对静止 Cell 的 Motion Prediction 回归，会引入运动的微小跳变。这里采用两种策略来抑制这种跳变：1. 根据类别分支，如果是背景的类别，则将其 Motion 置 0；2. 根据静止判断分支，如果是静止的，则将其 Motion 也置为 0。这样就能较好的解决静态 cell 出速度轨迹的情况。</p>
<h2 id="loss-function">4. Loss Function</h2>
<p>　　Classification 和 State Estimation 用 Cross-Entropy Loss，Motion Prediction 用 Smooth L1 Loss。此外为了保证空域与时域的一致性，引入另外三种 Loss：</p>
<ul>
<li><strong>Spatial Consistency Loss</strong><br>
属于同一物体的 Cell 的 Motion 应该是一致的(这里其实不太准确，考虑到转向情况，目标区域内 Cell 轨迹其实是不一样的，所以一般有两种思路，一种认为都一样，即每个 Cell 都建模成目标的运动；另一种则基于刚体假设，作类似 Flow 的建模，稍复杂些)。由此设计空间一致性损失函数： <span class="math display">\[L _ s = \sum _ {k}\sum _ {(i,j),(i&#39;,j&#39;)\in o _ k}\left\Vert X _ {i,j}^{(\tau)}-X _ {i&#39;,j&#39;} ^ {(\tau)}\right\Vert \tag{1}\]</span> 其中 \(||\cdot||\) 为 Smooth L1 Loss，\(o _ k\) 为第 \(k\) 个目标，\(X _ {i,j} ^ {(\tau)}\in\mathbb{R} ^ 2\) 为时间 \(\tau\) 时 Cell \((i,j)\) 的 motion。为了减少计算量，这里只是采样一些相邻的 \(X _ {i,j} ^ {(\tau)},X _ {i',j'} ^ {(\tau)}\) 匹配对。</li>
<li><strong>Foreground Temporal Consistency Loss</strong><br>
类似的，属于同一物体的 Motion 在时域上也应该是一致的，所以设计损失函数： <span class="math display">\[L _ {ft} = \sum _ k\left\Vert X _ {o _ k} ^ {(\tau)} - X _ {o _ k} ^ {(\tau+\Delta t)}\right\Vert\tag{2}\]</span> 其中 \(X _ {o _ k} ^ {(\tau)}\in\mathbb{R} ^ 2\) 为第 \(k\) 个目标的 Motion，计算方式为 \(X _ {o _ k} ^ {(\tau)}=\sum _ {(i,j)\in o _ k}X _ {i,j} ^ {(\tau)}/M\)，其中 \(M\) 为目标中 Cell 的个数。</li>
<li><strong>Background Temporal Consistency Loss</strong><br>
对静止的背景区域，其时域上 Motion 应该也是一致的(均为 0)。设计损失函数： <span class="math display">\[L _ {bt} = \sum _ {(i,j)\;\in\; X ^ {(\tau)}\;\cap\; T\left(\tilde{X} ^ {(\tau-\Delta t)}\right)}\left\Vert X _ {i,j} ^ {(\tau)}-T _ {i,j}\left(\tilde{X} ^ {(\tau-\Delta t)}\right)\right\Vert\tag{3}\]</span> 其中 \(T\in SE(3)\) 是 \(\tau-\Delta t\) 到 \(\tau\) 的位姿变换。将 \(\tilde{X} ^ {\tau-\Delta t}\) 变换到当前时刻后，与 \(X ^ {(\tau)}\) 会有一定的重合，将重合部分的背景区域的 Motion 约束为一致。有点 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 的味道。</li>
</ul>
<p>综上，所有的 Loss 为： <span class="math display">\[ L = L _ {cls} + L _ {motion} + L _ {state} + \alpha L _ s+ \beta L _ {ft} + \gamma L _ {bt} \tag{4}\]</span></p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Wu, Pengxiang, Siheng Chen, and Dimitris N. Metaxas. "MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
        <tag>Deep Learning&quot;</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Cylinder3D&quot;</title>
    <url>/paper-reading-Cylinder3D/</url>
    <content><![CDATA[<p>　　Voxel-based 点云分割/检测等任务中，点云的投影表示方法有三种：</p>
<ul>
<li>Spherical</li>
<li>Bird-eye View</li>
<li>Cylinder</li>
</ul>
<p>其中 Spherical 球坐标投影代表为 <a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>；Bird-eye View 则是目前主流的方法。有关 Bird-eye View 点云处理的优劣已经说了很多了，这里不再赘述。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种 Cylinder 柱状投影的点云处理方式，类似 <a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a>，也可以认为是 <a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a> 的 3D 版本。 <img src="/paper-reading-Cylinder3D/vs.png" width="70%" height="70%" title="图 1. Comparison"> 　　<a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a> 中详细说明了 Cylinder 投影比 Spherical 投影的优势，这里不做赘述，如图 1. 所示，相比 Spherical 投影，Cylinder 投影效果提升很明显。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-Cylinder3D/framework.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. Cylinder3D 由 3D 柱坐标投影和 3D U-Net 特征提取两部分组成。框架比较简单，网络结构主要由 DownSample，UpSample，Asymmetry Residual Block，Dimension-Decomposition based Context Modeling 四种组件构成。</p>
<h2 id="cylinder-partition">2. Cylinder Partition</h2>
<p><img src="/paper-reading-Cylinder3D/coord.png" width="80%" height="80%" title="图 3. Cylinder Partition"> 　　如图 3. 所示，将笛卡尔坐标系下的点云 \((x,y,z)\) 转换到柱坐标系下 \((\rho,\theta,z)\)。对于每个扇形 Voxel，作 PointNet 特征提取，最终得到 3D Cylinder 点云特征表示 \(\mathbb{R}\in C\times H\times W\times L\)。</p>
<h2 id="network">3. Network</h2>
<p><img src="/paper-reading-Cylinder3D/block.png" width="70%" height="70%" title="图 4. A & DDCM"></p>
<h3 id="asymmetry-residual-block">3.1. Asymmetry Residual Block</h3>
<p>　　如图 4. 所示，Asymmetry Residual Block 将 \(3\times 3\times 3\) 卷积拆分成 \(1\times 3\times 3\) 和 \(3\times 1\times 3\) 两种，这样作有两个好处：</p>
<ul>
<li>由于待检测的目标都接近于长方体，这种卷积形式更有利于提取长方体样式的特征；</li>
<li>减少 33% 的计算量，类似 Depth-wise Convolution；</li>
</ul>
<p>该模块作为 3D 卷积的基本模块，嵌入在下采样前，以及上采样后。</p>
<h3 id="dimension-decomposition-based-context-modeling">3.2. Dimension-Decomposition based Context Modeling</h3>
<p>　　由于 3D 空间的特征表达是 high-rank 的，所以利用矩阵分解的思想，将其用 height，width，depth 三维的 low-rank 向量来权重化表达，由此设计如图 4. 中的 DDCM 模块。该模块将三个方向的特征计算各自的权重，然后与原始特征作权重化整合。输出的特征用于最终的预测，预测输出是 Voxel-based，维度为 \(Class\times H\times W\times L\)。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Zhou, Hui, et al. "Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation." arXiv preprint arXiv:2008.01550 (2020).</p>
]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;RadarNet&quot;</title>
    <url>/paper-reading-RadarNet/</url>
    <content><![CDATA[<p>　　Radar 相比 LiDAR/Camera，对天气影响鲁棒性较强，而且能直接测量目标速度，所以是多传感器融合感知里比较重要的一个输入。其在 ADAS 领域应用已经较为广泛。但是 Radar 的测量噪声较大，这给有效的多传感器融合带来了难度。本文提出了 RadarNet<a href="#1" id="1ref"><sup>[1]</sup></a>，利用 Radar 的几何数据以及动态数据，同时做特征级别的前融合以及注意力机制下的后融合，达到了较好的效果。</p>
<h2 id="comparison-between-lidar-and-radar">1. Comparison between LiDAR and Radar</h2>
<p><img src="/paper-reading-RadarNet/compare.png" width="90%" height="90%" title="图 1. Comparison"> 　　LiDAR 可分为三种类型：Spinning，Solid State，Flash。目前主要采用的是 Spinning 旋转式的激光雷达。激光雷达的缺点有：</p>
<ul>
<li>对雨雾雪，车尾气等比较敏感；</li>
<li>对玻璃等物体没有反射;</li>
<li>点云密度随着距离增加而下降，远距离探测能力较弱。</li>
</ul>
<p>毫米波雷达则能克服以上缺点，并且能直接测量速度；但是缺点也比较明显：</p>
<ul>
<li>分辨率低，对小目标探测能力较弱；</li>
<li>误检较多；</li>
<li>测量的速度只是径向速度。</li>
</ul>
<p>毫米波雷达可输出三种形式的数据：1. 原始点运数据；2. 经过 DBSCAN 等聚类算法获得的聚类点集数据；3. 对点集作跟踪的数据。三种数据越来越高层，但是噪音越来越大。本文考虑输出点集的数据，设探测到的点集目标为 \(Q = (q, v _ {||},m,t)\)，其中 \(q = (x,y)\) 是俯视图下的位置，\(v _ {||}\) 是径向速度，\(m\) 代表目标是否运动，\(t\) 则为时间戳。我们需要进一步估计出目标的 2D 速度，由此在毫米波雷达的辅助下，能获得更长的检测距离，以及更准确的速度。 <img src="/paper-reading-RadarNet/fusion.png" width="90%" height="90%" title="图 2. Fusion"> 　　激光雷达数据与毫米波雷达数据融合示意图如图 2. 所示。</p>
<h2 id="radarnet">2. RadarNet</h2>
<p><img src="/paper-reading-RadarNet/framework.png" width="90%" height="90%" title="图 3. Framework"> 　　如图 3. 所示，RadarNet 主要由 Voxel-Based Early Fusion，Detection Network 以及 Attention-Based Late Fusion 来做融合检测。前融合将各传感器数据通过俯视图形式进行表示并融合；后融合则通过基于注意力的数据关联及整合机制来对目标速度进行精细估计。检测网络是在传统的分类＋回归基础上，多了速度预测的分支。具体的，所有回归的预测量为 \((x-p _ x,y - p _ y,w,l,\mathrm{cos}(\theta),\mathrm{sin}(\theta),m,v _ x, v _ y)\)，其中 \(p _ x,p _ y\) 为体素/栅格的中心点，\(m\) 为目标是运动的概率，如果 \(m &lt; 0.5\)，则将速度置为 0。</p>
<h3 id="early-fusion">2.1. Early Fusion</h3>
<p>　　对于激光雷达数据，类似 <a href="/paperreading-Fast-and-Furious/" title="FAF">FAF</a>，将时序多帧(0.5s)的点云数据在本车坐标系下打成俯视图体素表示，然后在通道维度进行串联。如果体素内没有点，那么该体素值为 0；如果体素内有点 \(\{(x _ i,y _ i,z _ i),i=1,...,N\}\)，那么体素值为 \(\sum _ i\left( 1- \frac{|x _ i-a|}{dx / 2}\right)\left( 1- \frac{|y _ i-b|}{dy / 2}\right)\left( 1- \frac{|z _ i-c|}{dz / 2}\right)\)，其中 \((a,b,c)\) 为体素中心坐标，\(dx,dy,dz\) 为体素尺寸。<br>
　　对于毫米波雷达数据，将其转到激光雷达坐标系后，也进行 BEV 时序串联，并将每一帧中不同线束的数据在 BEV 下体素化，然后串联。具体的，如果体素(本文丢掉了高度信息，所以退化为栅格)中没有毫米波探测到的目标，那么置为 0，如果探测到动态目标，则置为 1，如果探测到静态目标，则置为 -1。<br>
　　分别得到激光雷达与毫米波雷达的 BEV 表示后，将二者在通道维度串联起来，就完成了前融合。</p>
<h3 id="late-fusion">2.2. Late Fusion</h3>
<p>　　<strong>前融合关注毫米波雷达探测到的目标的位置和密度，后融合则使用毫米波雷达探测到的目标径向速度信息</strong>。检测网络输出的目标状态为 \(D = (c,x,y,w,l,\theta,\mathbf{v})\)，毫米波雷达输出的目标状态为 \(Q=(q,v _ {||},m,t)\)。两者的目标级别的后融合通过 Association 和 Aggregation 两步骤组成。 <img src="/paper-reading-RadarNet/late-fusion.png" width="90%" height="90%" title="图 4. Late-Fusion"> 　　本文将 Association 和 Aggregation 用 End-to-End 的网络来处理。如图 4. 所示，首先进行检测目标与毫米波目标的成对特征提取，然后经过 MLP/Softmax 作匹配分数估计，最后根据分数作速度的加权优化。</p>
<h4 id="pairwise-detection-radar-association">2.2.1. Pairwise Detection-Radar Association</h4>
<p>　　定义 Pairwise Feature 为： <span class="math display">\[\begin{align}
f(D,Q) &amp;= \left(f ^ {det}(D),f ^ {det-radar}(D,Q)\right) \tag{1}\\
f ^ {det}(D) &amp;= \left(w,l,||\mathbf{v}||,\frac{v _ x}{||\mathbf{v}||},\frac{v _ y}{||\mathbf{v}||},\mathrm{cos}(\gamma)\right) \tag{2}\\
f ^ {det-radar}(D,Q) &amp;= \left(dx,dy,dt,v ^ {bp}\right) \tag{3}\\
v ^ {bp} &amp;= \mathrm{min}\left(50,\frac{v _ {||}}{\mathrm{cos}(\phi)}\right) \tag{4}
\end{align}\]</span> 其中 \((\cdot,\cdot)\) 是 Concatenation 操作；\(\gamma\) 是网络检测 \(D\) 的运动方向与径向方向的夹角；\(\phi\) 是 \(D\) 运动方向与雷达探测目标 \(Q\) 的径向方向的夹角；\(v ^ {bp}\) 是径向速度反投影到运动方向(朝向)的速度值；\((dx,dy,dt)\) 为俯视图下 \(D,Q\) 的相对位置和时间。由此，通过 MLP 学习匹配分数： <span class="math display">\[s _ {i,j}=\mathrm{MLP} _ {match}\left(f(D _ i,Q _ j)\right)\tag{5}\]</span></p>
<h4 id="velocity-aggregation">2.2.2. Velocity Aggregation</h4>
<p>　　根据 \(D,Q\) 间的匹配分数，优化<strong>目标的绝对速度值</strong>。为了解决没有匹配的情况，分数 Concate 1，然后计算归一化的匹配分数： <span class="math display">\[s _ i ^ {norm}=\mathrm{softmax}((1,s _ {i,:})) \tag{6}\]</span> 然后优化每个检测目标 \(i\) 的绝对速度值： <span class="math display">\[v _ i&#39; = s _ i ^ {norm}\cdot
\begin{bmatrix}
||\mathbf{v} _ i|| \\
v _ {i,:} ^ {bp}
\end{bmatrix}
\tag{7}\]</span> 最终可得到目标的 2D 速度： <span class="math display">\[\mathbf{v}&#39; _ i = v&#39; _ i\cdot\left(\frac{v _ x}{||\mathbf{v}||},\frac{v _ y}{||\mathbf{v}||}\right) \tag{8}\]</span> 　　<strong>由此可知，该后融合优化的只是目标的绝对速度(毫米波雷达也没办法探测目标的朝向或运动方向)，目标的朝向准确度还是由检测网络决定。</strong></p>
<h2 id="experiments">3. Experiments</h2>
<p>　　根据式(2,3)提取的特征，作者设计了基于 Heuristic 的关联方法，毫米波雷达探测的目标与网络检测的目标关联的条件为： <span class="math display">\[\left\{\begin{array}{rl}
\sqrt{(dx) ^ 2+(dy) ^ 2} &amp;&lt; 3m \\
\gamma &amp;&lt; 40°\\
||\mathbf{v}|| &amp;&gt; 1m/s\\
v ^ {bp} &amp;&lt; 30m/s \\
\end{array}\tag{9}\right.\]</span> 一旦关联上后，去毫米波雷达速度的中位数作目标速度的进一步优化。这种传统的 Heuristic 与本文的 Attention 方法对比如下，优势明显。 <img src="/paper-reading-RadarNet/exp.png" width="80%" height="80%" title="图 5. Heuristic VS. Attention"></p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Yang, Bin, et al. "RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects." arXiv preprint arXiv:2007.14366 (2020).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Pillar-based Object Detection&quot;</title>
    <url>/paper-reading-Pillar-based-Object-Detection/</url>
    <content><![CDATA[<p>　　<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 在俯视图点云特征的基础上，融合了点云的前视图特征，由此解决点云在远处比较稀疏，以及行人等狭长型目标特征信息较少的问题。本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 作了三部分的改进：</p>
<ol type="1">
<li>检测头改为 Anchor-Free 的形式，本文称之为 Pillar-based，其实就是图像中对应的像素点；</li>
<li>前视图用 Cylindrical View 代替 Spherical View，解决目标高度失真的问题；</li>
<li>两个视图的栅格特征反投影回点特征作融合时，采用双线性插值的形式，避免量化误差的影响。</li>
</ol>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-Pillar-based-Object-Detection/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，点云分别投影到 BEV(Brids-Eye)，CYV(Cylindrical) 视角，然后作类似图像卷积的 2D 卷积操作以提取特征，并将特征反投影回点作融合(与 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 一致)，接着将点云特征再次投影到 BEV 下，最后作 Anchor-Free 的分类与回归任务。<br>
　　具体的，设 \(N\) 个点的点云 \(P=\{p _ i\} _ {i=0} ^ {N-1}\subseteq\mathbb{R} ^ 3\)，对应的特征向量为 \(F = \{f _ i\} _ {i=0} ^ {N-1}\subseteq\mathbb{R} ^ K\)。令 \(F _ V(p _ i)\) 返回点 \(p _ i\) 对应的栅格柱子 \(v _ j\) 的索引 \(j\)；\(F _ P(v _ j)\) 则返回栅格柱子 \(v _ j\) 对应的点集。对每个柱子进行特征整合，一般采用类似 PointNet(PN) 的方法： <span class="math display">\[f _ j ^{pillar} = \mathrm{PN} (\{f _ i|\forall p _ i\in F _ P(v _ j)\}) \tag{1}\]</span> pillar 级别的特征经过 CNN \(\phi\) 后得到进一步的 pillar 级别特征：\(\varphi=\phi(f ^ {pillar})\)。然后分别对 BEV，CYV 作 pillar-to-point 的特征投影变换： <span class="math display">\[f _ i^{point}=f _ j^{pillar}\;\mathrm{and}\;\varphi _ i^{point} = \varphi _ j^{pillar},\;\mathrm{where}\; j = F _ V(p _ i) \tag{2}\]</span> 最后的检测头是应用已经较为广泛的 Anchor-Free 形式。</p>
<h2 id="cylindrical-view">2. Cylindrical View</h2>
<p><img src="/paper-reading-Pillar-based-Object-Detection/proj.png" width="60%" height="60%" title="图 2. Projection"> 　　<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 采用 Spherical 投影方式，对于点 \(p _ i=(x _ i, y _ i, z _ i)\)，其球坐标 \(\varphi _ i,\theta _ i,d _ i\) 为： <span class="math display">\[\left\{\begin{array}{l}
\varphi _ i &amp;= \mathrm{arctan}\frac{y _ i}{x _ i}\\
\theta _ i &amp;= \mathrm{arccos}\frac{z _ i}{d _ i}\\
d _ i &amp;= \sqrt{x _ i ^ 2+y _ i ^ 2+z _ i^2}
\end{array}\tag{3}\right.\]</span> 如图 2. 所示，球坐标系下目标高度的形变比较严重，本文采用柱坐标系，其柱坐标 \(\rho _ i,\varphi _ i,z _ i\) 表示为： <span class="math display">\[\left\{\begin{array}{l}
\rho _ i &amp;=\sqrt{x _ i ^ 2+y _ i^2}\\
\varphi _ i &amp;= \mathrm{arctan}\frac{y _ i}{x _ i}\\
z _ i &amp;= z _ i
\end{array}\tag{4}\right.\]</span> 　　在此视角下作 pillar-level 的特征提取，与俯视图视角一样，只不过作卷积的时候，是环状卷积。具体实现方式是，将柱坐标系下的 pillar 展开，然后边缘补对应展开处另一边的 pillar 值，最后作传统的 2D 卷积即可。</p>
<h2 id="pillar-based-prediction">3. Pillar-based Prediction</h2>
<p>　　这里所谓的 Pillar-based 预测，本质上就是图像中常说的 Anchor-Free 的 Pixel-Level 的检测方法。最后特征图上的每个点预测类别概率，以及 3D 框属性 \(\Delta _ x,\Delta _ y,\Delta _ z,\Delta _ l,\Delta _ w,\Delta _ h,\theta ^ p\)。这里不作展开。</p>
<h2 id="bilinear-interpolation">4. Bilinear Interpolation</h2>
<p><img src="/paper-reading-Pillar-based-Object-Detection/bilinear.png" width="60%" height="60%" title="图 3. Bilinear"> 　　将 Pillar-Level 提取的特征反投影到 Point-Level 的特征时，需要进行插值处理。如图 3. 所示，传统的方式是最近邻插值，这种方式会引入量化误差，使得点投影反投影后的空间坐标不一致，产生的影响是同一 Pillar 内的点特征都是一样了。本文采用双线性插值的方法，使得 Point-Pillar-Point 的空间坐标一致，这样保证了 Pillar 内点特征的原始精度。该思想还是非常有借鉴意义的，实验效果提升也比较明显。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Wang, Yue, et al. "Pillar-based Object Detection for Autonomous Driving." arXiv preprint arXiv:2007.10323 (2020).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>VLOAM(Visual-lidar Odometry and Mapping)</title>
    <url>/VLOAM/</url>
    <content><![CDATA[<p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 中 Lidar Odometry 模块将当前累积的 Sweep 点云通过 Sweep-to-Sweep 注册到上一时刻的 Sweep 点云，从而生成高频低精度的位姿；Lidar Mapping 则将完整的当前 Sweep 点云通过 Sweep-to-Model 注册到全局地图中，从而生成低频高精度的位姿。这其中高频低精度的位姿可通过其它方式获得，如 IMU 等其它高频传感器。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 采用高频的 Visual Odometry 来生成高频低精度的位姿，低频高精度的位姿则还是通过 Lidar Odometry(Mapping) 获得，但做了细微的改变。 <img src="/VLOAM/demo.png" width="55%" height="55%" title="图 1. Visual & Lidar Odometry"> 　　如图 1. 所示，VSLAM 结合了高频低精度的 Visual Odometry，以及低频高精度的 Lidar Odometry，最终得到高频高精度的位姿，以及准确的全局点云地图。</p>
<h2 id="framework">1. Framework</h2>
<p>　　本文坐标系以相机坐标系 \(\{S\}\) 为主(x left，y upward，z forward)，所有点云都会通过外参转换到该坐标系下；设世界坐标系 \(\{W\}\) 为起始点。那么位姿求解问题的数学描述为：给定各个坐标系 \(\{S\}\) 下的图像和点云，求解所有 \(\{S\}\) 在 \(\{W\}\) 下的表示，以及 \(\{W\}\) 下地图的构建。 <img src="/VLOAM/framework.png" width="95%" height="95%" title="图 2. VSLAM Framework"> 　　如图 2. 所示，Visual Odometry 作前后帧的特征跟踪(或匹配)，结合点云深度信息，作 Frame-to-Frame 的运动位姿估计；Lidar Odometry 则先通过 Sweep-to-Sweep 作运动粗估计，然后用 Sweep-to-Map 作精估计(其中 Sweep 的定义可见 <a href="/LOAM/" title="LOAM">LOAM</a>)。由此输出低频的全局地图，以及高频的位姿估计。</p>
<h2 id="visual-odometry">2. Visual Odometry</h2>
<p>　　首先用 Visual Odometry 得到的高频位姿估计将点云注册为一个局部的深度图。由此维护三种类型的特征点：1. 从深度图获得深度的特征点；2. 从前后帧三角化获得深度的特征点；3. 没有深度的特征点。这里的特征点提取可采用任意的特征点提取方法，如果采用前后帧特征匹配的策略，则还得作相应的特征描述子提取，如果采用特征跟踪策略，则不需要。<br>
　　设图像帧序号 \(k\in Z ^ +\)，特征点序号 \(i\in\mathcal{I}\)，那么在相机坐标系 \(\{S ^ k\}\) 下，特征点坐标表示为 \(\sideset{^S}{}X ^ k _ i = [\sideset{^S}{}x ^ k _ i,\sideset{^S}{}y ^ k _ i,\sideset{^S}{}z ^ k _ i] ^ T\)，其归一化表示为 \({\sideset{^S}{}{\overline{X}}} ^ k _ i = [\sideset{^S}{}{\overline{x}} ^ k _ i,\sideset{^S}{}{\overline{y}} ^ k _ i,\sideset{^S}{}{\overline{z}} ^ k _ i] ^ T\)。前后匹配的特征点与运动位姿的关系为： <span class="math display">\[{\sideset{^S}{}X} ^ k _ i=R\;{\sideset{^S}{}X} ^ {k-1} _ i+T \tag{1}\]</span> 其中 \({\sideset{^S}{}X} ^ k _ i\) 为当前帧的特征点坐标，由于还未估计出当前帧的位姿，所以该特征点是没有深度信息的。根据特征点 \({\sideset{^S}{}X} ^ {k-1} _ i\) 是否有深度信息，可归纳出方程：</p>
<ol type="1">
<li>\({\sideset{^S}{}X} ^ {k-1} _ i\) 有深度信息 <span class="math display">\[\begin{align}
&amp;{\sideset{^S}{}{\overline{d}}} ^ k _ i{\sideset{^S}{}{\overline{X}}} ^ k _ i=R\;{\sideset{^S}{}X} ^ {k-1} _ i+T\\
\Longrightarrow &amp; \left\{\begin{array}{l}
\left({\sideset{^S}{}{\overline{z}}} ^ k _ i R _ 1-{\sideset{^S}{}{\overline{x}}} ^ k _ i R _ 3\right){\sideset{^S}{}{X}} ^ k _ i + {\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 1-{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 3 = 0\\
\left({\sideset{^S}{}{\overline{z}}} ^ k _ i R _ 2-{\sideset{^S}{}{\overline{y}}} ^ k _ i R _ 3\right){\sideset{^S}{}{X}} ^ k _ i + {\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 2-{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 3 = 0\\
\end{array}\tag{2}\right.
\end{align}\]</span> 其中 \(\sideset{^S}{}{d} ^ k _ i = \left\Vert \sideset{^S}{}{\overline{X}} ^ k _ i\right\Vert \)，\(R _ l, T _ l\) 为第 \(l\in\{1,2,3\}\) 行的 \(R,T\)。</li>
<li>\({\sideset{^S}{}X} ^ {k-1} _ i\) 无深度信息 <span class="math display">\[\begin{align}
&amp;{\sideset{^S}{}{\overline{d}}} ^ k _ i{\sideset{^S}{}{\overline{X}}} ^ k _ i=R\;{\sideset{^S}{}{\overline{d}}} ^ {k-1} _ i\;{\sideset{^S}{}X} ^ {k-1} _ i+T\\
\Longrightarrow &amp;
\begin{bmatrix}
-{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 3  +{\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 2 &amp;{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 3-{\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 1 &amp;-{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 2+{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 1
\end{bmatrix}
R\;{\sideset{^S}{}{\overline{X}}} ^ {k-1} _ i = 0
\tag{3}
\end{align}\]</span> 推导过程比较繁杂，但是也比较简单，依次消去 \(\sideset{^S}{}{d} ^ k _ i,\sideset{^S}{}{d} ^ {k-1} _ i\) 即可。</li>
</ol>
<p>将所有特征点所构成的 residual 累积，然后可用 LM 法求解该非线性问题中 6-DOF 的位姿。考虑到有较大 residual 的特征点大概率是离群点，所以对特征点的 residual 作权重处理，residual 越大，权重越小。<br>
<img src="/VLOAM/feats.png" width="55%" height="55%" title="图 3. Edge & Planar Feature"> 　　为了获取特征点的深度，维护一个从点云中采样的在上一帧图像坐标系下的深度图，深度图维护较新的点云深度信息，并且保持一定的点密度。深度图中的点用极坐标形式的 2D KD-tree 存储，具体的特征点深度值计算通过周围深度点构成的平面插值得到。在无法从深度图中获得特征点的深度信息时，如果特征点被跟踪了较长的距离，那么采用三角测量法获得该特征点深度。三种点的可视化如图 3. 所示。</p>
<h2 id="lidar-odometry">3. Lidar Odometry</h2>
<p>　　高频的 frame-to-frame Visual Odometry 得到的位姿估计是粗糙且有漂移的，接下来用 Lidar Odometry 作进一步的精估计。激光雷达里程计又基于 coarse-to-fine 的思想，分为 sweep-to-sweep 以及 sweep-to-map 两个步骤。这两个步骤的具体计算过程很相似，只不过前者是前后帧点云的匹配以消除运动引入的点云畸变，后者则是当前帧去畸变的点云与世界坐标系下的地图点云匹配，能消除累积误差。总体上这部分与 <a href="/LOAM/" title="LOAM">LOAM</a> 处理方式一致。</p>
<h3 id="sweep-to-sweep">3.1. Sweep-to-Sweep</h3>
<p><img src="/VLOAM/vo_drift.png" width="55%" height="55%" title="图 4. Drift"> 　　与 <a href="/LOAM/" title="LOAM">LOAM</a> 一样，对第 \(m\in Z ^ +\) 个 Sweep 点云 \(\mathcal{P} ^ m\)，提取线特征 \(\mathcal{E} ^ m\) 与面特征 \(\mathcal{H} ^ m\)。如图 4. 所示，将 Visual Odometry 产生的漂移建模为线性运动模型。假设第 \(m\) 个 Sweep 扫描期间其漂移的位姿为 \(T'\in\mathbb{R} ^ {6\times 1}\)，那么，对于点 \(i\in\mathcal{E} ^ m\cup\mathcal{H} ^ m\)，其接收时间 \(t _ i\) 对应的位姿漂移为： <span class="math display">\[T _ i&#39; = T&#39;(t _ i-t ^ m)/(t ^ {m+1}-t ^ m) \tag{4}\]</span> 　　为了求解 \(T'\)，分别找到当前帧特征点 \(\mathcal{E} ^ m,\mathcal{H} ^ m\) 与上一帧特征点的匹配，然后计算距离误差的 residual，累积后即可用 LM 法来求解该非线性最小二乘问题。对于 \(\mathcal{E} ^ m\)，在 \(\mathcal{P} ^ {m-1}\) 中找到最近的两个线特征点，从而计算 point-to-edge 距离；对于 \(\mathcal{H} ^ m\)，在 \(\mathcal{P} ^ {m-1}\) 中找到最近的三个面特征点，从而计算 point-to-plane 距离。找特征点的过程通过 3D KD-tree 实现(工程上为了加速，可以采用其它方法)。由此得到一系列方程： <span class="math display">\[f({\sideset{^S}{}X} ^ m _ i, T _ i&#39;)=d _ i \tag{5}\]</span> 其中 \({\sideset{^S}{}X} ^ m _ i\) 是点 \(i\in\mathcal{E} ^ m\cup\mathcal{H} ^ m\) 在 \(\{S ^ m\}\) 下的坐标。计算 \(T'\) 后，即可得到去畸变的当前帧点云 \(\mathcal{P} ^ m\)。</p>
<h3 id="sweep-to-map">3.2. Sweep-to-Map</h3>
<p>　　去畸变的点云 \(\mathcal{P} ^ m\) 可以进一步注册到点云地图 \(\mathcal{Q} ^ {m-1}\) 中。考虑到点云地图较为稠密，匹配过程为计算局部点集的分布特征值与特征向量。特征值一大两小，即为线特征；特征值两大一小则为面特征。因为没有 Sweep-to-Sweep 中的运动模型，所以可直接用 ICP 方法来优化求解位姿。最终得到低频高精度的位姿结果。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Zhang, Ji, and Sanjiv Singh. "Visual-lidar odometry and mapping: Low-drift, robust, and fast." 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2015.</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>SuMa(Surfel-based Mapping)</title>
    <url>/SuMa/</url>
    <content><![CDATA[<p>　　目前业界比较流行的基于激光雷达的 SLAM 是 <a href="/LOAM/" title="LOAM">LOAM</a>，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 Lidar Odometry 时的匹配定位。实际应用于工业界时，Mapping 的数据结构设计及存取管理对整体系统的效率至关重要，具体可优化的细节以后再写文阐述。<br>
　　本系列文章<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 提出了一种基于 Surfel 和语义信息的建图及定位方法。整体框架与 LOAM 类似，只是这里只用了面区域的特征点，其它模块，如优化方式，也有很大的差异。</p>
<h2 id="suma">1. SuMa</h2>
<p>　　设 \(A\) 坐标系下的点 \(p _ A\)，\(B\) 坐标系下的点 \(p _ B\)，其变换矩阵 \(T _ {BA}\in\mathbb{R}^{4\times 4}\)，使得 \(p _ B = T _ {BA} p _ A\)。变换矩阵 \(T _ {BA}\) 又由 \(R _ {BA}\in\mathbf{SO}(3)\) 和 \(t _ {BA}\in\mathbb{R}^3\) 构成。设每帧点云的雷达坐标系为 \(C _ k,k\in\{0,...,t\}\)，那么 Lidar Odometry 要求解的问题就是当前雷达坐标系在世界坐标系下的表示： <span class="math display">\[T _ {WC _ t} = T _ {WC _ 0}T _ {C _ 0C _ 1}\cdots T _ {C _ {t-1}C _ t} \tag{1}\]</span> 其中 \(T _ {WC _ 0}\) 为已标定的变换矩阵。 <img src="/SuMa/suma.png" width="65%" height="65%" title="图 1. SuMa Framework"> 　　如图 1. 所示，SuMa 根据点云 \(\mathcal{P} = \{p\in\mathbb{R}^3\}\) 估计 \(T _ {WC _ t}\) 的步骤为：</p>
<ol type="1">
<li>当前帧地图计算。将当前帧的三维点云投影到二维，得到顶点图 \(\mathcal{V} _ D\)，以及计算对应的法向量图 \(\mathcal{N} _ D\)；</li>
<li>当前地图计算。对上一帧优化出的 Surfel Map \(\mathcal{M} _ {active}\) 作顶点图和法向量图的渲染 \(\mathcal{V} _ M,\mathcal{N} _ M\)；</li>
<li>位姿计算。根据 \(\mathcal{V} _ D, \mathcal{N} _ D\) 以及 \(\mathcal{V} _ M,\mathcal{N} _ M\) 作 frame-to-model 的 ICP 匹配，得到相对位姿 \(T _ {C _ {t-1}C _ t}\)，最后用式(1)计算当前帧在世界坐标系下的位姿态 \(T _ {WC _ t}\)；</li>
<li>地图更新。根据 \(T _ {WC _ t}\)，更新 Surfel Map \(\mathcal{M} _ {active}\)：初始化首次观测的区域，优化更新再次观测的区域；</li>
<li>闭环检测。在未激活的 Surfel Map \(\mathcal{M} _ {inactive}\) 中搜索当前帧地图的匹配；</li>
<li>闭环检测验证。在接下来 \(\Delta _ {verification}\) 时间内，验证闭环检测的有效性，如果有效，那么加入之后的位姿图优化；</li>
<li>位姿图优化。另一个线程作位姿图优化，输入信息是前后帧的相对位姿里程计以及闭环检测的相对位姿结果，类似 <a href="/AVP-SLAM/" title="AVP-SLAM">AVP-SLAM</a> 中的位姿图优化。优化后的位姿用来更新 Surfel Map。</li>
</ol>
<h3 id="preprocessing">1.1. Preprocessing</h3>
<p>　　与 RangeNet++<a href="#3" id="3ref"><sup>[3]</sup></a> 中对点云的表示一样，顶点图 \(\mathcal{V} _ D\) 的计算方法为： <span class="math display">\[\left(\begin{matrix}
u\\
v\\
\end{matrix}\right)=
\left(\begin{matrix}
\frac{1}{2}[1-\mathrm{arctan}(y,x)\cdot \pi ^ {-1}]\cdot w\\
[1-(\mathrm{arcsin}(z\cdot r ^ {-1})+f _ {up})f ^ {-1}]\cdot h
\end{matrix}\right)
\tag{2}\]</span> 其中 \(r = \Vert p\Vert _ 2\) 为点的距离，\(f = f _ {up} + f _ {down}\) 是雷达的上下视野角，\(w,h\) 为顶点图的宽和高。然后基于 \(\mathcal{V} _ D\) 计算每个顶点的法向量，得到法向量图 \(\mathcal{N} _ D\): <span class="math display">\[\mathcal{N} _ D((u,v)) = \left(\mathcal{V} _ D((u+1,v))-\mathcal{V} _ D((u,v))\right)\times \left(\mathcal{V} _ D((u,v+1))-\mathcal{V} _ D((u,v))\right) \tag{3}\]</span> 其中只计算坐标点 \((u,v)\) 有顶点的法向量。因为 \(u\) 方向物理世界是环状的，所以对边界作环向处理。这种法向量计算的 \(\mathcal{N} _ D\) 由较大噪声，但是实验发现对 Frame-to-Model 的 ICP 匹配不会产生精度影响。 <img src="/SuMa/preprocess_suma.png" width="55%" height="55%" title="图 2. SuMa Preprocessing"> 　　顶点图 \(\mathcal{V} _ D\) 与法向量图 \(\mathcal{V} _ N\) 的可视化结果如图 2. 所示。</p>
<h3 id="map-representation">1.2. Map Representation</h3>
<p>　　不同于 <a href="/LOAM/" title="LOAM">LOAM</a> 中采用了 Edge 和 Surf 两种特征来表示地图，本文只用 Surfel 来表示地图 \(\mathcal{M}\)。<a href="/LOAM/" title="LOAM">LOAM</a> 中计算了每个点的曲率，然后将其归为 Edge 或是 Surf，实际工程应用中，为了存储的高效性，首先将点云地图体素化，然后将体素内的特征点用 Mean，Normal，协方差矩阵的 EigenVector 等信息来存储，Normal 可用来表征 Surf 特征点，EigenVector 则可用来表征 Edge 特征点，这块具体的细节以后开文再详细阐述。<br>
　　本文的 Surfel Map 自然就提取了点云的 Surf 特征，每个Surfel 可以用位置 \(v _ s\in\mathbb{R} ^ 3\)，法向量 \(n _ s\in\mathbb{R} ^ 3\)，半径 \(r _ s\in\mathbb{R}\) 来表示。此外每个 Surf 包含两个时间戳：首次建立的时间 \(t _ c\)，以及最新更新的时间 \(t _ u\)。然后采用贝叶斯滤波方法(详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>)，定义及计算 Surfel 特征的稳定概率： <span class="math display">\[\begin{align}
l _ s ^ {(t)} &amp;= l _ s ^ {t-1} + \mathrm{log}(p\cdot (1-p) ^ {-1}) - \mathrm{log}(p _ {prior}\cdot (1-p _ {prior}) ^ {-1})\\
&amp;= l _ s ^ {t-1} + \mathrm{odds}(p) - \mathrm{odds}(p _ {prior})\\
&amp;= l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior})
\end{align} \tag{4}\]</span> 其中 \(p _ {stable}, p _ {prior}\) 分别为测量为 surfel 是 stable 的概率，以及先验概率。\(\sigma ^ 2\) 为测量噪声方差。\(\alpha\) 为测量的 Surfel 法向量与对应的地图中 Surfel 法向量的夹角，\(d\) 则为测量的 Surfel 与对应的地图中 Surfel 的距离。<br>
　　每个 Surfel 的位置及法向量都是以建立时的位置作为参考系，即 \(C _ {t _ c}\)。这样经过全局位姿优化后，就不需要重新建图，只需要通过 \(T _ {WC _ {t _ c}}\) 将 Surfel 地图更新到世界坐标系即可。<br>
　　\(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 的区分也比较简单：\(\mathcal{M} _ {active}\) 定义为最近更新的 Surfels，即 \(t _ u\geq t - \Delta _ {active}\)；\(\mathcal{M} _ {inactive}\) 则定义为不是最近建立的 Surfels，即 \(t _ c&lt; t - \Delta _ {active}\)。Odometry 只在 \(\mathcal{M} _ {active}\) 中作匹配计算，Loop Closure 则只在 \(\mathcal{M} _ {inactive}\) 中搜索。</p>
<h3 id="odometry-estimation">1.3. Odometry Estimation</h3>
<p>　　里程计是将当前帧点云与地图点云匹配的过程。将上一时刻的地图 \(\mathcal{M} _ {active}\) 渲染成上一时刻局部坐标系下的顶点图 \(\mathcal{V} _ M\) 与法向量图 \(\mathcal{N} _ M\) 形式。然后采用 point-to-plane 的 ICP 匹配方法，其最小化误差为： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{5}\]</span> 其中 \(u\in\mathcal{V} _ D\)，\(v _ u\in\mathcal{V} _ M,n _ u\in\mathcal{N} _ M\) 是地图上对应关联上的点，关联过程为： <span class="math display">\[\begin{align}
v _ u &amp;= \mathcal{V} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\\
n _ u &amp;= \mathcal{N} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)
\end{align} \tag{6}\]</span> 其中 \(T _ {C _ {t-1}\;C _ t} ^ {(t)}\) 为 frame-to-model ICP 得到的里程计估计的相对位姿。\(\Pi(u)\) 是特征点的关联方式，<a href="/LOAM/" title="LOAM">LOAM</a> 中根据前后线束的关系来寻找关联方式，本方案则采用直接坐标映射的方式。<strong>因为点云均投影到了前视图，所以可根据坐标直接搜索关联，这也是本方案最重要的优势之一</strong>。具体的，如图对应的地图顶点图中没有顶点，或者地图法向量点没有定义，那么忽略该待关联的特征点；对于关联的特征点对距离大于 \(\sigma _ {ICP}\) 或是法向量夹角大于 \(\theta _ {ICP}\) 的情况，则认为是离群点，不计入误差项。ICP 初始化为上一帧的相对位姿结果。<br>
　　该问题是典型的非线性最小二乘问题，可在李空间下对位姿进行线性化并用 Gaussian-Newton 求解，这里不做展开。</p>
<h3 id="map-update">1.4. Map Update</h3>
<p>　　得到里程计估计的相对位姿后，要将当前帧的特征点更新到地图中，即要确定哪些 Surfel 要更新，哪些要重新构建新的 Surfel。对于 \(v _ s\in\mathcal{V} _ D\)，首先计算其面元的半径： <span class="math display">\[ r _ s = \frac{\sqrt{2}\Vert v _ s\Vert _ 2\cdot p}{\mathrm{clam}(-v _ s ^ T n _ s\cdot\Vert v _ s\Vert _ 2 ^ {-1}, 0.5, 1.0)} \tag{7}\]</span> 其中 \(p=\mathrm{max}(w\cdot f _ {horiz} ^ {-1}, h\cdot f _ {vert} ^ {-1})\)。<strong>根据式(2)，每个 \(v _ s\) 均能找到地图中对应的 Surfel \(s '\)。</strong>然后通过 \(\vert n _ {s'} ^ T(v _ s-v _ {s'})\vert &lt; \sigma _ M \;\mathrm{and}\; \Vert n _ s\times n _ {s'}\Vert &lt; \mathrm{sin}(\theta _ M)\) 判定当前帧的 Surfel 与地图中的 \(s'\) 是否一致：</p>
<ul>
<li>如果一致。那么更新地图中的 Surfel，如果估计的半径更准，那么也更新： <span class="math display">\[\begin{align}
v _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot v _ s + \gamma\cdot v _ {s&#39;} ^ {(t-1)}\\
n _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot n _ s + \gamma\cdot n _ {s&#39;} ^ {(t-1)}\\
r _ {s&#39;} ^ {(t)} &amp;= r _ s, \; \mathrm{if} \; r _ s &lt; r _ {s&#39;}
\end{align} \tag{8}\]</span></li>
<li>如果不一致。那么将地图中匹配上的 Surfel 作 Stable 概率衰减，然后创建新的 Surfel。如果地图中没有匹配的 Surfel，那么也创建新的 Surfel。</li>
</ul>
<p>最后将 Stable 概率较小的 Surfel 以及时间较早的 Surfel 删除，以此删除动态障碍物特征点以及较老的无关的特征点。</p>
<h3 id="loop-closures">1.5. Loop Closures</h3>
<p>　　检测到闭环后就可以作 Pose Graph 优化。闭环检测由检测与验证两部分组成，检测的过程为在未激活的地图 \(\mathcal{M} _ {inactive}\) 中找到一个最相近的位姿： <span class="math display">\[ j ^ * = \mathop{\arg\min}\limits _ {j\in 0,...,t-\Delta _ {active}} \Vert t _ {WC _ t}-t _ {WC _ j}\Vert \tag{9}\]</span> 然后类似 Odometry 的过程，将当前帧的点云特征注册到 \(T _ {WC _ j ^ * }\) 的地图特征中。为了用 ICP 求解两者的相对位姿 \(T _ {C _ {j ^ * }C _ t}\)，初始化 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 为： <span class="math display">\[\begin{align}
R _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}R _ {WC _ t}\\
t _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}(t _ {WC _ t}-t _ {WC _ {j ^ * }})\\
\end{align} \tag{10}\]</span> 本文将 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 中的位移用 \(\lambda t _ {C _ {j ^ * }C _ t}\) 代替，其中 \(\lambda = \{0.0,0.5,1.0\}\)。由此可得到三种初始化后 ICP 迭代的结果，选择最合理的结果即可。<br>
　　验证阶段，在 \(t + 1,...,t+ \Delta _ {verification}\) 时间段内，在 \(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 地图中分别作 Odometry 累加，查看两者的一致性，如果一致则认为该闭环检测是有效的。</p>
<h2 id="suma-1">2. SuMa++</h2>
<p><img src="/SuMa/suma++.png" width="95%" height="95%" title="图 3. SuMa++ Framework"> 　　SuMa++ 相比 SuMa，只增加了语义信息，算法框架没有改变。如图 3. 所示，SuMa++ 也有当前帧地图计算，当前地图计算，位姿计算，地图更新，闭环检测，闭环检测验证，位姿图优化等七个部分组成，其中，在地图计算中加入了有 RangeNet++ 产生的语义信息，在 \(\mathcal{V} _ D,\mathcal{N} _ D\) 的基础上，增加 \(\mathcal{S} _ D\) 特征；在地图更新中，根据语义信息加入了动态障碍物过滤的策略；在位姿计算中，用语义信息来权重化特征的 ICP 匹配迭代。</p>
<h3 id="semantic-map">2.1. Semantic Map</h3>
<p>　　RangeNet++ 也是基于式(2)投影试图下的分割模型，由此可得到 Surfel 特征图 \(\mathcal{V} _ D\) 中每个像素点的语义类别以及对应的类别概率。由于语义分割预测的噪声，本文用 Flood-fill 算法对网络输出的语义分割图 \(\mathcal{S} _ {raw}\) 作优化，得到顶点图对应的语义信息 \(\mathcal{S} _ D\)。 <img src="/SuMa/preprocess.png" width="65%" height="65%" title="图 4. SuMa++ Preprocessing"> 　　考虑到语义分割在物体中心区域确定性较高，而在边缘处不确定性较高，所以 Flood-fill 算法采用两个步骤：</p>
<ol type="1">
<li>用腐蚀算法将与周围语义类别不一致的像素点移除，得到腐蚀后的语义图 \(\mathcal{S} _ {raw} ^ {eroded}\)；</li>
<li>结合有深度信息的顶点图 \(\mathcal{V} _ D\)，对腐蚀的边缘像素点填充为周围相近距离的顶点对应的语义类别，得到 \(\mathcal{S} _ D\)；</li>
</ol>
<p>如图 4. 所示，该方法能修正边缘类别错误的情况。由此，\(\mathcal{V} _ D, \mathcal{N} _ D,\mathcal{S} _ D\)组成每一帧的特征点信息。</p>
<h3 id="filtering-dynamics">2.2. Filtering Dynamics</h3>
<p><img src="/SuMa/res.png" width="65%" height="65%" title="图 5. Filterring Dynamics"> 　　有了语义类别信息后，在更新地图时，可计算当前帧每个 Surfel 与地图中对应 Surfel 的类别一致性，由此作为地图贝叶斯更新的惩罚项，如果类别不一致，地图中的 Surfel 稳定性概率会降低，直到去除。如图 5. 所示，这种方法能去除大部分动态障碍物区域所构成的 Surfel。地图具体的贝叶斯更新为： <span class="math display">\[\begin{align}
l _ s ^ {(t)} = l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior}) - \mathrm{odds}(p _ {penalty})
\end{align} \tag{11}\]</span></p>
<h3 id="semantic-icp">2.3. Semantic ICP</h3>
<p>　　在式(5)的 ICP 误差项基础上，可加入语义约束，对误差项作权重化： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}w _ u n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{12}\]</span> 其中权重项结合了语义约束与几何约束，以此来减少离群特征点对优化的影响： <span class="math display">\[w _ u ^{(k)} = \rho _ {Huper}\left(r _ u ^ {(k)}C _ {semantic}(\mathcal{S} _ D(u),\mathcal{S} _ M(u))\right)\mathbb{1}\left\{l _ s ^ {(k)}\geq l _ {stable}\right\} \tag{13}\]</span> 其中 \(\rho _ {Huber}(r)\) 是 Huber 核函数： <span class="math display">\[\rho _ {Huber}(r)=\left\{\begin{array}{l}
1 &amp;,\mathrm{if}\;\vert r\vert &lt; \sigma\\
\sigma\vert r\vert ^ {-1} &amp;,\mathrm{otherwise}
\end{array}\tag{14}\right.\]</span> 语义约束项为： <span class="math display">\[C _ {semantic}\left((y _ u,P _ u),(y _ {v _ u}, P _ {v _ u})\right)=\left\{\begin{array}{l}
P(y _ u|u) &amp;,\mathrm{if}\;y _ u=y _ {v _ u}\\
1-P(y _ u|u) &amp;,\mathrm{otherwise}
\end{array}\tag{15}\right.\]</span></p>
<p><img src="/SuMa/icp.png" width="65%" height="65%" title="图 6. Weights of ICP"> 　　如图 6. 所示，在语义信息的约束下，如果当前帧某个 Surfel 的类别与地图中对应的 Surfel 类别不一致，那么就会减少该 Surfel 匹配对的误差项。</p>
<h2 id="thinkings">3. Thinkings</h2>
<p>　　利用检测或分割得到的语义信息去过滤当前帧以及地图中的动态障碍物，在 SLAM/Odometry 中已经非常常见，其实可以大概率相信语义信息，然后直接将对应的点云干掉。而本文以融合迭代的思路，想通过将信将疑的方式来完成有效的 ICP 匹配（既要滤掉大多数动态障碍物的影响，也期望一堆车停在场景中时然后保留足够匹配的特征点）。但是一般工程上，直接干掉也够用，毕竟场景够大，不太可能出现特征点不够匹配的情景。<strong>而本方法的高效性在于，寻找当前帧与地图中的 Surfel 匹配时，直接采用图像索引然后顶点图距离及法向量图角度判断有效性的形式，没有 KD-Tree，极大提高效率</strong>，类似 ICPCUDA<a href="#4" id="4ref"><sup>[4]</sup></a>。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Behley, Jens, and Cyrill Stachniss. "Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments." Robotics: Science and Systems. 2018.<br>
<a id="2" href="#2ref">[2]</a> Chen, Xieyuanli, et al. "Suma++: Efficient lidar-based semantic slam." 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br>
<a id="3" href="#3ref">[3]</a> Milioto, Andres, et al. "RangeNet++: Fast and accurate LiDAR semantic segmentation." 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br>
<a id="4" href="#4ref">[4]</a> https://github.com/mp3guy/ICPCUDA</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>SLAM</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>AVP-SLAM</title>
    <url>/AVP-SLAM/</url>
    <content><![CDATA[<p>　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内停车场环境，相比于采用特征点的 ORB-SLAM，性能较为鲁棒。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/AVP-SLAM/framework.png" width="95%" height="95%" title="图 1. AVP-SLAM Framework"> 　　如图 1. 所示，AVP-SLAM 由 Mapping，Localization 两部分组成。Mapping 阶段，将车周围的四张图通过 IPM 拼接并变换到俯视图，然后作 Guide Signs，Parking Lines，Speed Bumps 等语义信息的提取，接着通过 Odometry 将每帧的特征累积成局部地图，最后通过回环检测，全局优化出全局地图。Localization 阶段，提取出每帧的语义信息后，用 Odometry 初始化位姿，然后用 ICP 匹配求解当前帧在全局地图中的位姿，得到基于地图的位姿观测量，最后用 EKF 融合该观测量与 Odometry 信息，得到本车的最终位姿。<br>
　　有了本车在全局地图下的位姿后，然后通过语义信息识别停车位，即可达到本车自动泊车的目的。</p>
<h2 id="mapping">2. Mapping</h2>
<h3 id="ipm">2.1. IPM</h3>
<p>　　传感器为车身四周四个鱼眼相机，相机内外参已知。IPM(Inverse Perspective Mapping) 是将图像中的像素点投影到车身物理坐标系下的俯视图中，具体的： <span class="math display">\[\frac{1}{\lambda}\;
\begin{bmatrix}
x ^ v \\
y ^ v \\
1
\end{bmatrix} =
[\mathbf{R} _ c \;\mathbf{t} _ c] ^ {-1} _ {col:1,2,4} \;\pi _ c ^ {-1}
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix}
\tag{1}\]</span> 其中 \(\pi _ c(\cdot)\) 为鱼眼相机的内参矩阵，\([\mathbf{R} _ c\;\mathbf{t} _ c]\) 为每个相机到车身坐标系的外参矩阵，\([x ^ v\; y ^ v]\) 为车身坐标系下语义特征的位置。关于 IPM 更多细节可参考 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a>。<br>
　　进一步将 IPM 图拼接成一张全景图： <span class="math display">\[\begin{bmatrix}
u _ {ipm}\\
v _ {ipm}\\
1
\end{bmatrix}=\mathbf{K} _ {ipm}
\begin{bmatrix}
x ^ v \\
y ^ v \\
1
\end{bmatrix}
\tag{2}\]</span> 其中 \(\mathbf{K} _ {ipm}\) 是全景图的内参。</p>
<h3 id="feature-detection">2.2. Feature Detection</h3>
<p><img src="/AVP-SLAM/segment.png" width="65%" height="65%" title="图 2. Segmentation in IPM Image"> 　　将每张 IPM 图拼接成一张大全景图，然后用基于深度学习的语义分割方法，对全景图作像素级别作 lane，parking line，guide sign，speed bump，free space，obstacle，wall 等类别的语义分割。如图 4. 所示，parking line，guide sign，speed bump 是稳定的特征，用于定位；parking line 用于车位的识别；free space 与 obstacle 用于路径规划。</p>
<h3 id="local-mapping">2.3. Local Mapping</h3>
<p>　　全景图语义分割得到的用于定位的特征(parking line，guide sign，speed bump)需要反投影回车身物理坐标系： <span class="math display">\[\begin{bmatrix}
x ^ v \\
y ^ v \\
1
\end{bmatrix}=\mathbf{K} _ {ipm} ^ {-1}
\begin{bmatrix}
u _ {ipm}\\
v _ {ipm}\\
1
\end{bmatrix}
\tag{3}\]</span> 然后基于 Odometry 的相对位姿，将当前的语义特征点转换到世界坐标系下： <span class="math display">\[\begin{bmatrix}
x ^ w \\
y ^ w \\
z ^ w
\end{bmatrix}=\mathbf{R _ o}
\begin{bmatrix}
x ^ v \\
y ^ v \\
0
\end{bmatrix} + \mathbf{t _ o}
\tag{4}\]</span> 由此得到局部地图，本文保持车身周边 30m 内的局部地图。</p>
<h3 id="loop-detection">2.4. Loop Detection</h3>
<p><img src="/AVP-SLAM/loop_det.png" width="65%" height="65%" title="图 3. Loop Detection"> 　　因为 Odometry 有累计误差，所以这里对局部地图作一个闭环检测。如图 3. 所示，通过 ICP 对两个局部地图作匹配，一旦匹配成功，就说明检测到了闭环，ICP 匹配的相对位姿用于之后的全局位子图优化，以消除里程计累计误差。</p>
<h3 id="global-optimization">2.5. Global Optimization</h3>
<p>　　检测到闭环后，需进行全局位姿图优化。位姿图中，节点(node)为每个局部地图的位姿：\((\mathbf{r, t})\)；边(edge)有两种：odometry 相对位姿以及闭环检测中 ICP 匹配位姿。由此位姿图优化的损失函数为： <span class="math display">\[\chi ^ * = \mathop{\arg\min}\limits _ \chi \sum _ t\Vert f(\mathbf{r} _ {t+1},\mathbf{t} _ {t+1}, \mathbf{r} _ t, \mathbf{t} _ t) - \mathbf{z} ^ o _ {t,t+1}\Vert ^ 2 + \sum _ {i,j\in\mathcal{L}}\Vert f(\mathbf{r} _ i,\mathbf{t} _ i,\mathbf{r} _ j, \mathbf{t} _ j)-\mathbf{z} ^ l _ {i,j}\Vert ^ 2 \tag{5}\]</span> 其中 \(\chi = [\mathbf{r} _ 0,\mathbf{t} _ 0,...,\mathbf{r} _ t,\mathbf{t} _ t] ^ T\) 是所有局部地图的位姿，也是待优化的参数。\(\mathbf{z} ^ 0 _ {t,t+1}\) 为 Odometry 得到的位姿。\(\mathbf{z} ^ l _ {i,j}\) 为闭环检测 ICP 得到的位姿。\(f(\cdot)\) 为计算两个局部地图相对位姿的方程。该优化问题可通过 Gauss-Newton 法求解。<br>
　　用优化后的位姿将局部地图叠加起来，就获得了整个场景的全局地图。</p>
<h2 id="localization">3. Localization</h2>
<p><img src="/AVP-SLAM/loc.png" width="65%" height="65%" title="图 4. Localization"> 　　有了全局地图后，基于全局地图的定位观测量可通过当前帧与全局地图的匹配得到。如图 4. 所示，绿色为当前帧检测到的语义特征，与全局地图匹配后即可得到当前的绝对位置。匹配通过 ICP 算法实现： <span class="math display">\[ \mathbf{r ^ * ,t ^ * } =  \mathop{\arg\min}\limits _ {\mathbf{r,t}}\sum _ {k\in\mathcal{S}}\Vert\mathbf{R(r)}
\begin{bmatrix}
x ^ v  _ k\\
y ^ v  _ k\\
0
\end{bmatrix} + \mathbf{t} - 
\begin{bmatrix}
x ^ w _ k \\
y ^ w _ k\\
z ^ w _ k
\end{bmatrix}
\Vert ^ 2 \tag{6}\]</span> 其中 \(\mathcal{S}\) 为当前帧语义特征点集，\([x _ k ^ w\; y _ k ^ w\; z _ k ^ w]\) 分别为对应的全局地图中最近的特征点集。<br>
　　ICP 的初始化非常重要，本文提出了两种初始化方法：1. 直接在地图上标记车库入口作为全局坐标点；2. 室外 GPS 信号初始化，然后用 Odometry 累积到车库。</p>
<h2 id="extended-kalman-filter">4. Extended Kalman Filter</h2>
<p>　　Visual Localization 在语义特征较少的情况下，比如车辆停满了，定位会不稳定，所以这里采用 EKF 对 Visual Localization 与 Odometry 中的轮速计和 IMU 作扩展卡尔曼融合，这里不做展开。</p>
<h2 id="thinkings">5. Thinkings</h2>
<p>　　Semantic SLAM 相比基于几何特征点的 SLAM 更加鲁棒。但是在车库场景下，一旦车子停满后，停车线等语义信息会急剧减少，所以实际商业应用中，AVP-SLAM 能满足室内自动泊车的需求吗？<br>
　　对此我持怀疑态度。我认为，对于车库自动泊车的商业落地，可能最有效且低成本的方法还是得基于室内 UWB 定位技术。至少 UWB 可作为辅助。当然要将 UWB 应用于车载装置，目前好像还没有，不过随着车载软硬件系统的完善，手机上能做的事，车载平台问题也不大。</p>
<h2 id="reference">6. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Qin, Tong, et al. "AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot." arXiv preprint arXiv:2007.01813 (2020).</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>SLAM</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud&quot;</title>
    <url>/paper-reading-Point-GNN/</url>
    <content><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。</p>
<h2 id="different-point-cloud-representations">1. Different Point Cloud Representations</h2>
<p><img src="/paper-reading-Point-GNN/repr.png" width="65%" height="65%" title="图 1. Point Cloud Representations"> 　　如图 1. 所示，目前点云表示方式以及对应的特征学习方式有三种：Grids，栅格化后类似图像 2D/3D 卷积的形式；Sets，以 PointNet 为代表的最近邻查找周围点并学习的形式；Graph，将无序点集转换为图模型，特征信息通过点云顶点传递学习的形式。Grids 及 Sets 形式我们已经比较熟悉了，Graph 则查询效率比 Sets 高，特征提取能力又比 Grids 高。Graph 的建图时间复杂度为 \(\mathcal{O}(cN)\)，领域查询复杂度则为 \(\mathcal{O}(1)\)，Sets 中的 KNN 建树及查询复杂度可见 <a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a>。当然 KNN 式的领域查询方式可以用近似 \(\mathcal{O}(1)\) 方法实现，但是会影响特征学习的准确度。</p>
<h2 id="framework">2. Framework</h2>
<p><img src="/paper-reading-Point-GNN/framework.png" width="95%" height="95%" title="图 2. Framework of Point-GNN"> 　　如图 2. 所示，基于 Graph 的 3D 点云检测，首先对点云作 Graph Construction，然后用 GNN 来学习每个顶点的特征，接着对每个顶点预测目标框，最后作目标框的整合和 NMS。</p>
<h3 id="graph-construction">2.1. Graph Construction</h3>
<p>　　设点云集：\(P=\{p _ 1,...,p _ N\}\)，其中 \(p _ i=(x _ i, s _ i)\) 分别表示坐标 \(x _ i\in\mathbb{R} ^ 3\)，以及该点反射率，领域点相对位置等信息 \(s _ i\in\mathbb{R} ^ k\)。对该点集建图 \(G=(P,E)\)，将距离小于一定阈值的两个点进行连接，即： <span class="math display">\[E = \{(p _ i, p _ j)|\Vert x _ i-x _ j\Vert _ 2 &lt; r\} \tag{1}\]</span> 这种建图方式是 Fixed Radius Near-Neighbors 问题，可在 \(\mathcal{O}(cN)\) 时间复杂度下解决，其中 \(c\) 为最大连接数。<br>
　　建图完成后，要对每个点信息状态 \(s _ i\) 作初始化。这里采用类似 Sets 的特征提取方式，即将该点的反射率，以及与领域内点的相对位置，串联成特征向量，然后用 MLP 作空间变换，最后在点维度上作 Max Pooling，即可得到初始化的该点特征状态量 \(s _ i\)。</p>
<h3 id="graph-neural-network-with-auto-registration">2.2. Graph Neural Network with Auto-Registration</h3>
<p>　　传统的图神经网络，通过边迭代每个顶点的特征。在 \((t+1) ^ {th}\) 迭代时： <span class="math display">\[\begin{align}
 v _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{e _ {ij} ^ t|(i,j)\in E\}\right), v _ i ^ t\right) \\
e _ {ij} ^ t &amp;= f ^ t(v _ i ^ t, v _ j ^ t) \tag{2}
\end{align}\]</span> 其中 \(e ^ t,v ^ t\) 分别是边和顶点特征，\(f ^ t(\cdot)\) 计算两个顶点之间边的特征，\(\rho(\cdot)\) 将与该点连接的边特征整合，得到该点特征增量，\(g ^ t(\cdot)\) 将该点特征增量与原特征进行整合得到本次迭代后该点的最终特征。<br>
　　对于边特征，一种设计方式为，描述领域特征对该点位置的作用力，重写式 (2)： <span class="math display">\[s _ i ^ {t+1} = g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{3}\]</span> 这样就得到了图神经网络的迭代模型。此外，本文还指出，由于边特征对领域点的距离较为敏感，所以作者提出对相对位置作自动补偿，实验表明其实意义不大： <span class="math display">\[\begin{align}
\Delta x _ i ^ t &amp;= h ^ t(s _ i^t) \\
s _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i+\Delta x _ i ^ t,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{4}
\end{align}\]</span> 　　具体的，\(f ^ t(\cdot),g ^ t(\cdot), h ^ t(\cdot)\) 可用 MLP 来建模，\(\rho(\cdot)\) 则采用 Max 操作： <span class="math display">\[\begin{align}
\Delta x _ i ^ t &amp;= MLP _ h ^ t(s _ i^t) \\
e _ {ij} ^ t &amp;= MLP _ f ^ t([x _ j - x _ i + \Delta x _ i ^ t, s _ j ^ t]) \\
s _ i ^ {t+1} &amp;= MLP _ g ^ t\left(MAX(\{e _ {ij}|(i,j)\in E\})\right)+ s _ i ^ t \tag{5}
\end{align}\]</span></p>
<h3 id="loss">2.3. Loss</h3>
<p>　　为了作 3D 检测的任务，网络头输出为每个顶点的类别，目标框中心的 offset，以及目标框的尺寸，朝向。这与传统的基于 Ancho-Free 的 3D 目标检测基本一致，这里不做展开。</p>
<h3 id="box-merging-and-scoring">2.4. Box Merging and Scoring</h3>
<p>　　本方法的 3D 检测需要作 NMS 后处理，由于分类的 Score 不能反应目标框的 Uncertainty，所以基于 Score 的 NMS 是不合理的。这个问题在 2D 检测中也有比较多的研究，比如采用预测 IoU 值的方式来作权重。本文则认为遮挡信息能作为 NMS 操作的指导，由此定义了遮挡值的计算方式。但是实验显示，其实提升并不明显，所以这里不做具体展开。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Shi, Weijing, and Raj Rajkumar. "Point-gnn: Graph neural network for 3d object detection in a point cloud." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>CenterTrack</title>
    <url>/CenterTrack/</url>
    <content><![CDATA[<p>　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检测模块分别对前后帧作目标检测，目标跟踪模块则接收前后帧检测结果，然后用 Motion Model 将上一帧的检测结果预测到这一帧，最后与这一帧的检测结果作数据关联(Data Association)出目标 ID。这里的 Motion Model 可以是 3D 下目标的物理运动模型，也可以是图像下的单目标跟踪结果，如 KCF 算法。详细介绍可参考 <a href="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/" title="Multiple Object Tracking: A Literature Review">Multiple Object Tracking: A Literature Review</a>。<br>
　　随着检测技术的发展，检测与跟踪的整合成为了趋势。<a href="#1" id="1ref">[1]</a> 是较早将跟踪的 “Motion Model” 用 Anchor-based Two-stage 网络来预测的方法，其网络输入为前后帧图像，其中一个分支输出当前帧的检测框，另一个分支用上一帧的检测结果作为 proposal，输出这一帧的跟踪框，最后用传统的数据关联方法得到目标的 ID。随着检测技术往 Anchor-Free One-stage 方向发展，在此基础上整合目标检测与跟踪也就顺理成章。<br>
　　<a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中详细描述了 Anchor-Free 的目标检测方法，相比于 Anchor-Based 的目标检测，其有很多优势，这里不做赘述。本文基于 CenterNet<a href="#2" id="2ref"><sup>[2]</sup></a>，总结了 CenterTrack<a href="#3" id="3ref"><sup>[3]</sup></a>，以及 CenterPoint(3D CenterTrack)<a href="#4" id="4ref"><sup>[4]</sup></a>方法。</p>
<h2 id="centernet">1. CenterNet</h2>
<p>　　CenterNet 在 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中已经较为详细得阐述了。需要补充的是，中心点的正负样本设计为：正样本只有中心点像素，负样本则为其它区域，并加入以中心点为中心的高斯权重，越靠近中心点，负样本权重越小。其 Loss 基于 Focal Loss，数学描述为： <span class="math display">\[L _ k = \frac{1}{N}\sum _ {xyc}\left\{\begin{array}{l}
(1-\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(\hat{Y} _ {xyc}) &amp; \mathrm{if}\; Y _ {xyc} = 1\\
(1- Y _ {xyc})^{\beta}(\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(1-\hat{Y} _ {xyc}) &amp; \mathrm{otherwise}
\end{array}\tag{1}\right.\]</span> 其中 \(Y _ {xyc}\) 为高斯权重后的正负样本分布值。<br>
　　具体的，设图像 \(I\in \mathbb{R}^{W\times H\times 3}\)，CenterNet 输出的每个类别 \(c\in\{0,...,C-1\}\) 的目标为 \(\{(\mathbf{p} _ i, \mathbf{s} _ i)\} _ {i=0} ^ {N-1}\)。其中 \(\mathbf{p}\in \mathbb{R} ^ 2\)，\(\mathbf{s}\in\mathbb{R} ^ 2\) 为目标框的尺寸。对应的，最终输出的 heatmap 位置和尺寸图为：\(\hat{Y}\in [0,1]^{\frac{W}{R}\times\frac{H}{R}\times C}\)，\(\hat{S}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。对 \(\hat{Y}\) 作 \(3\times 3\) 的 max pooling，即可获得目标中心点，\(\hat{S}\) 上对应的的点即为该目标的尺寸。此外还用额外的 heatmap 作位置 offset 的回归，因为 \(\hat{Y}\) 存在量化误差。最终由中心点位置 loss，位置 offset loss，尺寸 loss 三部分组成。</p>
<h2 id="centertrack">2. CenterTrack</h2>
<h3 id="framework">2.1. Framework</h3>
<p><img src="/CenterTrack/centertrack.png" width="85%" height="85%" title="图 1. CenterTrack"> 　　如图 1. 所示，CenterTrack 基于 CenterNet，框架也较为简单：输入前后帧图像，以及上一帧跟踪到的目标中心点所渲染的 heatmap，经过网络后输出为当前帧的检测 heatmap，size map，以及这一帧相对上一帧跟踪的 offset map。最后通过最近距离匹配即可作数据关联获得目标的 ID。算法得到的目标属性有 \(b = (\mathbf{p,s},w,id)\)，分别为目标的 location，size，confidence，identity。<br>
　　相比于 CenterNet，CenterTrack 还预测了这一帧相对上一帧，目标的 2D displacement：\(\hat{D}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。这相当于 Tracking 中 Motion Model 的结果，分别计算上一帧目标经过该 displacement 变换到这一帧后的目标位置与当前帧检测的目标位置的距离误差，用最小距离的贪心法即可将目标作数据关联，得到目标的 ID。</p>
<h3 id="experiments">2.2. Experiments</h3>
<p>　　网络结构相比于 CenterNet 只是增加了输入的四个通道特征，输出的两个通道特征。网络可在视频流图像或者单帧图像上训练，对于单帧图像，可对图像中的目标作伸缩平移变换来模拟目标运动，实验表明，也非常有效。 <img src="/CenterTrack/motion_models2d.png" width="85%" height="85%" title="图 2. Motion Models"> 　　如图 2. 所示，本文比较了 displacement 与 kalman filter，optical flow 等 Motion Model，显示本文效果是最好的，我猜测是因为 displacement 回归的直接是物体级别的像素运动，抗噪性更强。</p>
<h2 id="center-based-3d-object-detection-and-tracking">3. Center-based 3D Object Detection and Tracking</h2>
<h3 id="framework-1">3.1. Framework</h3>
<p><img src="/CenterTrack/centertrack3d.png" width="85%" height="85%" title="图 3. 3D CenterTrack"> 　　如图 3. 所示，CenterPoint 将点云在俯视图下栅格化，然后采用 CenterTrack 一样的网络结构，只是输出为目标的 3D location，size，orientation，velocity。<br>
　　点云俯视图下的栅格化，如果对栅格不做点云的精细化估计，那么会影响到目标位置及速度估计的精度，所以理论上 PointPillars 这种栅格点云特征学习方式能更有效的提取点云的信息，保留特征的连续化信息(但是论文的实验表明 VoxelNet 比 PointPillars 效果更好)。否则，虽然目标位置等信息的监督项是连续量，但是栅格化的特征是离散量，这会降低预测精度。<br>
　　具体的，网络输出为：\(K\) 个类别的 \(K\)-channel heatmap 表示目标中心点，目标的尺寸 \(\mathbf{s}=(w,l,h)\) heatmap，目标的中心点 offset \(\mathbf{o}=(o _ x,o _ y,o _ z)\) heatmap，朝向角 \(\mathbf{e} = (\mathrm{sin}(\alpha),\mathrm{cos}(\alpha))\) heatmap，目标速度 \(\mathbf{v}=(v _ x,v _ y)\) heatmap。与 CenterTrack 非常相似，只不过这里的速度就是真实的物理速度。</p>
<h3 id="experiments-1">3.2. Experiments</h3>
<p><img src="/CenterTrack/detmap.png" width="85%" height="85%" title="图 4. 3D Detection Benchmark"> 　　如图 4. 所示，引入 Velocity 预测，能有效提升检测的性能，这应该是网络输入前一帧信息的结果，对半遮挡情况能有较好效果。 <img src="/CenterTrack/experiment3d.png" width="85%" height="85%" title="图 5. 3D MOT Benchmark"> 　　如图 5. 所示，跟踪性能也是有很大提升，而且数据关联等后处理相对比较简单。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Feichtenhofer, Christoph, Axel Pinz, and Andrew Zisserman. "Detect to track and track to detect." Proceedings of the IEEE International Conference on Computer Vision. 2017.<br>
<a id="2" href="#2ref">[2]</a> Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. "Objects as points." arXiv preprint arXiv:1904.07850 (2019).<br>
<a id="3" href="#3ref">[3]</a> Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. "Tracking Objects as Points." arXiv preprint arXiv:2004.01177 (2020).<br>
<a id="4" href="#4ref">[4]</a> Yin, Tianwei, Xingyi Zhou, and Philipp Krähenbühl. "Center-based 3D Object Detection and Tracking." arXiv preprint arXiv:2006.11275 (2020).</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Autonomous Driving</tag>
        <tag>MOT</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>Rethinking of Sparse 3D Convolution</title>
    <url>/Rethinking-of-Sparse-3D-Convolution/</url>
    <content><![CDATA[<p>　　Sparse 3D Convolution 最早在<a href="#1" id="1ref">[1]</a>中提出，然后该作者又提出了 Submanifold Sparse Convolution<a href="#2" id="2ref"><sup>[2]</sup></a>，并将其应用于 3D 语义分割中<a href="#3" id="3ref"><sup>[3]</sup></a>。<a href="#4" id="4ref">[4]</a>则改进了 Sparse 3D Convolution 的实现方式，并应用于 3D 目标检测中。之前一直没仔细看 Sparse 3D Convolution 原理，以为只是基于稀疏矩阵的矩阵相乘加速，最近的一些实验发现 Sparse 3D Convolution 在点云相关的任务中不仅仅是加速，还能提升网络特征提取的性能，所以回过头来重新思考 Sparse 3D Convolution 原理及作用。</p>
<h2 id="sparse-convolution">1. Sparse Convolution</h2>
<p><img src="/Rethinking-of-Sparse-3D-Convolution/spconv.png" width="85%" height="85%" title="图 1. sparse VS. submanifold sparse"> 　　如图 1. 左图所示，对于稀疏的特征输入，传统的 Sparse Convolution 与 Convolution 一致，只是对于卷积核覆盖的输入特征为零的区域不做计算，直接置为零。这种方式下，随着卷积层的增加，特征层会变得不那么稀疏，这样不仅使得计算量上升，而且会使得提取的信息变得不那么准确。</p>
<h2 id="submanifold-sparse-convolution">2. Submanifold Sparse Convolution</h2>
<p>　　如图 1. 右图所示，Submanifold Sparse Convolution 解决了 Sparse Convolution 存在的问题。原理也很直观：只计算输出特征层映射到输入特征层不为零的位置区域。这种方式下，随着卷积层的增加，不仅能保持稀疏性，而且能保证原始信息的准确性。 <img src="/Rethinking-of-Sparse-3D-Convolution/flops.png" width="85%" height="85%" title="图 2. Flops"> 　　如图 2. 所示，Sparse Convolution 相比传统的 Convolution 已经能减少较多的计算量，而 Submanifold Sparse Convolution 则能减少更多的计算量。特征输入越稀疏，减少的计算量就越多，这对点云的三维特征提取，或者是俯视图下的二维特征提取有很大的帮助。</p>
<h2 id="implementation">3. Implementation</h2>
<p><img src="/Rethinking-of-Sparse-3D-Convolution/speed.png" width="85%" height="85%" title="图 3. Speed"> 　　<a href="#2" id="2ref">[2]</a> 中实现了 Submanifold Sparse Convolution，其中的卷积运算是手写的矩阵相乘，所以速度较慢；<a href="#4" id="4ref">[4]</a> 则基于 GEMM 实现了更高效的 Submanifold Sparse Convolution。如图 3. 所示，其有将近一倍的速度提升。 <img src="/Rethinking-of-Sparse-3D-Convolution/imple.png" width="90%" height="90%" title="图 4. Implementation"> 　　图 4. 描述了<a href="#4" id="4ref">[4]</a>实现的 Submanifold Sparse Convolution 原理。其首先通过 gather 操作将非零的元素进行矩阵相乘，然后通过 scatter 操作将结果映射回原位置。为了加速，前后元素的映射矩阵计算比较关键，这里实现了一种 GPU 计算方法，这里不做展开。</p>
<h2 id="application">4. Application</h2>
<p><img src="/Rethinking-of-Sparse-3D-Convolution/second.png" width="90%" height="90%" title="图 5. SECOND Framework"> 　　Submanifold Sparse Convolution 可应用于点云的分类，分割，检测等任务的特征提取中，SECOND<a href="#4" id="4ref"><sup>[4]</sup></a>是一种点云检测方法，如图 5. 所示，其检测框架与传统的一致，只是将体素化后的点云特征信息，进一步用 Sparse Convolution 来作特征提取。该方法不仅速度较快，而且性能也有不少提升。所以 Submanifold Sparse Convolution 是非常高效的，可作为点云特征提取的基本操作。但是传统的 Convolution，在 GPU 平台下，已经有较多的硬件级优化(cudnn)，在 CPU 平台下也有很多的指令集优化，所以最终在特定硬件下作 Inference 时，到底 Submanifold Sparse Convolution 速度能提升多少，还得看 Submanifold Sparse Convolution 实现的好不好。不过可以猜测，在目前的实现下，Submanifold Sparse Convolution 在 GPU 平台下应该能有不少的速度提升。<br>
　　此外，传统的卷积量化操作也比较成熟，cudnn 已经有基本的操作引擎，而 Submanifold Sparse Convolution 的 INT8 引擎则目前还没有。所以 float32/float16 的 Submanifold Sparse Convolution 与 INT8 的 Convolution，孰快孰慢？这两条路大概就是部署的思路了，当然 INT8 的 Submanifold Sparse Convolution 会更好，但是开发成本会比较高。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Graham, Ben. "Sparse 3D convolutional neural networks." arXiv preprint arXiv:1505.02890 (2015).<br>
<a id="2" href="#2ref">[2]</a> Graham, Benjamin, and Laurens van der Maaten. "Submanifold sparse convolutional networks." arXiv preprint arXiv:1706.01307 (2017).<br>
<a id="3" href="#3ref">[3]</a> Graham, Benjamin, Martin Engelcke, and Laurens Van Der Maaten. "3d semantic segmentation with submanifold sparse convolutional networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.<br>
<a id="4" href="#4ref">[4]</a> Yan, Yan, Yuxing Mao, and Bo Li. "Second: Sparsely embedded convolutional detection." Sensors 18.10 (2018): 3337.</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection&quot;</title>
    <url>/End-to-End-Pseudo-LiDAR-for-3D-Det/</url>
    <content><![CDATA[<p>　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：</p>
<ul>
<li>单目\(\rightarrow\)3D 框，代表文章有<a href="#1" id="1ref">[1]</a>。</li>
<li>单目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li>
<li>双目\(\rightarrow\)3D 框，代表文章有<a href="#2" id="2ref">[2]</a>。</li>
<li>双目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li>
</ul>
<p>由单目或双目直接回归 3D 目标框的属性，这种方法优势是 latency 小，缺点则是，没有显式的预测深度图，导致目标 3D 位置回归较为困难。而在深度图基础上回归 3D 目标位置则相对容易些，这种方法由两个模块构成：深度图预测，3D 目标预测。得到深度图后，可以在前视图下将深度图直接 concate 到 rgb 图上来做，另一种方法是将深度图转换为 pseudo-LiDAR 点云，然后用基于点云的 3D 目标检测方法来做，目前学术界基本有结论：pseudo-LiDAR 效果更好。<br>
　　本文<a href="#3" id="3ref"><sup>[3]</sup></a>即采用双目出深度图，然后基于 pseudo-LiDAR 来作 3D 目标检测的方案，并且解决了两个模块需要两个网络来优化的大 lantency 问题，实现了 End-to-End 联合优化的方式。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/framework.png" width="60%" height="60%" title="图 1. Framework"> 　　基于点云作 3D 目标检测大致可分为 point-based 与 voxel-based 两大类，详见 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a>，传统的基于双目的 pseudo-LiDAR 方案无法 End-to-End 作俯视图下 voxel-based 3D 检测，因为点云信息需要作俯视图离散化，离散的过程是无法作反向传播训练的，本文提出了 Change of Representation(CoR) 模块有效解决了这个问题。如图 1. 所示，本方案中 Depth Estimation 可由任何深度估计网络实现，然后经过 CoR 模块，将深度图变换成点云形式用于 point-based 3D detection，或者是 Voxel 形式用于 voxel-based 3D detection。这里的关键是可求导的 CoR 模块设计。</p>
<h2 id="cor">2. CoR</h2>
<h3 id="quantization">2.1. Quantization</h3>
<p>　　点云检测模块如果采用 voxel-based 方案，那么点云到俯视图栅格的离散化(quantization)是必不可少的。假设点云 \(P = \{p _ 1,...,p _ N\}\)，待生成的 3D 占据栅格(最简单的特征形式) \(T\) 包含 \(M\) 个 bins，即 \(m\in\{1,...,M\}\)，每个 bin 的中心点设为 \(\hat{p} _ m\)。那么生成的 \(T\) 可表示为： <span class="math display">\[ T(m) = \left\{\begin{array}{l}
1, &amp; \mathrm{if}\;\exists p\in P \; \mathrm{s.t.}\; m = \mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2 \\
0, &amp; \mathrm{otherwise}.
\end{array}\tag{1}\right.\]</span> 即如果有点落在该 bin 里，那么该 bin 对应的值置为 1。这种离散方式是无法求导的。<br>
　　本文提出了一种可导的软量化模块(soft quantization module)，即用 RBF 作权重计数，另一种角度来看，<strong>这其实类似于点的空间概率密度表示</strong>。设 \(P _ m\) 为落在 bin \(m\) 的点集： <span class="math display">\[ P _ m=\left\{p\in |, \mathrm{s.t.}\; m=\mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2\right\} \tag{2}\]</span> 那么，\(m'\) 作用于 \(m\) 的值为： <span class="math display">\[ T(m, m&#39;) = \left\{\begin{array}{l}
0 &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert = 0;\\
\frac{1}{\vert P _ {m&#39;}\vert} \sum _ {p\in P _ {m&#39;}} e^{-\frac{\Vert p-\hat{p} _ m\Vert ^2}{\sigma ^ 2}} &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert &gt; 0.
\end{array}\tag{3}\right.\]</span> 最终的 bin 值为： <span class="math display">\[ T(m) = T(m,m)+\frac{1}{\vert \mathcal{N} _ m\vert}\sum _ {m&#39;\in\mathcal{N} _ m}T(m,m&#39;) \tag{4}\]</span> 当 \(\sigma ^2\gg 0\) 以及 \(\mathcal{N} _ m=\varnothing\) 时，回退到式 (1) 的离散化方式。本文实验中采用 \(\sigma ^2 = 0.01\)，\(\mathcal{N} _ m=3\times 3\times 3 -1 = 26\)。传统的点云栅格概率密度计算方式为：将点云中的每个点高斯化，然后统计每个栅格中心坐标上覆盖到的值。与上述方法的高斯原点不一样，但是计算结果是一致的。 <img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/quantization.png" width="90%" height="90%" title="图 2. Quantization"> 　　这种方法可将导数反向传播到 \(m'\) 中的每个点：\(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)}\times\frac{\partial T(m)}{\partial T(m,m')}\times\bigtriangledown _ pT(m,m')\)。如图 2. 所示，蓝色 voxel 表示梯度为正，即 \(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)} &gt; 0\)，红色 voxel 表示梯度为负。那么蓝色 voxel 期望没有点，所以将点往外推，红色 voxel 则将点往里拉，最终使点云与 LiDAR 点云，即 GT 点云一致。</p>
<h3 id="subsampling">2.1. Subsampling</h3>
<p>　　点云检测模块如果采用 point-based 方案，那么就比较容易直接与深度图网络进行 End-to-End 整合。point-based 3D Detection 一般通过 sampling 来扩大感受野，提取局部信息，因为这种方法的计算量对点数比较敏感，所以 sampling 也是降低计算量的有效手段。一个 \(640\times 480\) 的深度图所包含的点云超过 30 万，远远超过一个 64 线的激光雷达，所以对其进行采样就非常关键。<br>
　　本文对深度图点云进行模拟雷达式的采样，即定义球坐标系下栅格化参数：\((r,\theta,\phi)\)。其中 \(\theta\) 为水平分辨率，\(\phi\) 为垂直分辨率。对每个栅格内采样一个点，即可得到一个较为稀疏，且接近激光雷达扫描属性的点云。</p>
<h2 id="loss">3. Loss</h2>
<p>　　Loss 由 depth 估计与 3D Detection 两项构成： <span class="math display">\[\mathcal{L} = \lambda _ {det}\mathcal{L} _ {det} + \lambda _ {depth}\mathcal{L} _ {depth} \tag{5}\]</span></p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Mousavian, Arsalan, et al. "3d bounding box estimation using deep learning and geometry." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br>
<a id="2" href="#2ref">[2]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. "Stereo r-cnn based 3d object detection for autonomous driving." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="3" href="#3ref">[3]</a> Qian, Rui, et al. "End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;PointPainting: Sequential Fusion for 3D Object Detection&quot;</title>
    <url>/PointPainting/</url>
    <content><![CDATA[<p>　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，<strong>数据级或特征级的前融合</strong>，以及<strong>任务级的后融合</strong>。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种将图像分割结果的语义信息映射到点云，进而作 3D 检测的方法。这种串行方式的融合，既有点前融合的意思，也有点后融合的意思，暂且可归为前融合吧。本方法可认为是个框架，该框架下，基于图像的语义分割，以及基于点云的 3D 检测，均为独立模块。实验表明，融合了图像的语义信息后，点云针对行人等小目标的检测有较大的性能提升。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/PointPainting/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，算法框架非常简单，一句话能说明白：1). 首先经过图像语义分割获得语义图；2). 然后将点云投影到图像上，查询点云的语义信息，并连接到坐标信息中；3). 最后用点云 3D 检测的方法作 3D 检测。</p>
<h2 id="experiments">2. Experiments</h2>
<p><img src="/PointPainting/sota.png" width="90%" height="90%" title="图 2. PointPainting Applied to SOTA"> 　　采用 DeepLabv3+ 作为语义分割模块，应用到不同的点云 3D 检测后，结果如图 2. 所示，均有不同程度的提升，尤其是行人这种小目标。 <img src="/PointPainting/pointrcnn.png" width="90%" height="90%" title="图 3. Painted PointRCNN"> 　　图 3. 显示了 Painted PointRCNN 与各个方法的对比结果，mAP 是最高的。 <img src="/PointPainting/per-class.png" width="90%" height="90%" title="图 4. 不同类别的提升程度"> 　　由图 4. 可知，对行人，自行车，雪糕筒等小目标(俯视图下来说)，本方法提升非常显著。这也比较好理解，因为前视图下，这些目标所占的像素会比较多，所以更容易在前视相机图像下提取有效信息，辅助俯视图下作更准确的检测。</p>
<h2 id="rethinking-of-early-fusion">3. Rethinking of Early Fusion</h2>
<p>　　这里将本方法归为前融合，但是并不是真正意义上的前融合。如果是前融合，那么一般是 concate 语义分割网络的中低层特征到点云信息中，然而本文是直接取语义分割网络的最高层特征(即分类结果)。<strong>所以问题来了，所谓的前融合，一定比后融合更好吗？</strong>我想，这篇文章可能给了一些答案(不知道作者有没有做过取其它特征的实验，姑且认为做过，然后选择了本方法的策略)，虽然理论上前融合信息最完整，但是，如果这种完整的信息无法有效学出来或者对标定外参比较敏感，那么这种前融合也提升不了后续任务的性能，更有甚者，由于信息空间的变大或紊乱，导致后续任务性能下降。相反，对于不是那么“前”的后融合，我们能极大得保证各个任务学习结果的有效性，基于此，融合后学习的有效性也会比较确定。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Vora, Sourabh, et al. "Pointpainting: Sequential fusion for 3d object detection." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds&quot;</title>
    <url>/DENFIDet/</url>
    <content><![CDATA[<p>　　俯视图下 Voxel-based 点云 3D 目标检测一般会使用 2D 检测网络及相关策略。但是不同于图像的 2D 目标检测，俯视图下目标的点云信息基本在边缘处，所以如何准确得捕捉目标的边缘信息对特征提取的有效性非常关键。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种能捕捉目标边缘信息的网络结构，从而作更准确的目标检测。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/DENFIDet/framework.png" width="80%" height="80%" title="图 1. Framework of DENFIDet"> 　　如图 1. 所示，在传统的 One-Stage 2D/3D 检测框架下，嵌入了 DENFI 模块，该模块首先通过 DBPM 生成稠密的目标边缘 proposals，然后指导 DENFIConv 去提取更准确的目标特征，输出到检测头作 3D 目标框属性的分类与回归。</p>
<h2 id="denfidense-feature-indicator">2. DENFI(Dense Feature Indicator)</h2>
<h3 id="dbpmdense-boundary-proposal-module">2.1. DBPM(Dense Boundary Proposal Module)</h3>
<p><img src="/DENFIDet/DBPM.png" width="60%" height="60%" title="图 2. DBPM"> 　　DBPM 的输入为 Backbone 输出的 \(H\times W\times C\) 特征图，其由分类和回归两个分支构成：</p>
<ul>
<li>分类分支<br>
分类分支经过 \(1\times 1\) 卷积输出 \(H\times W\times K\) 大小的 pixel 级别的 Score Map，其中 \(K\) 为类别数；</li>
<li>回归分支<br>
回归分支也经过 \(1\times 1\) 卷积，输出 \(H\times W\times (4+n\times 2)\) 大小的回归量。回归量包括 \((l,t,r,b)\) 以及角度 \((\theta ^{bin}, \theta ^{res})\)(角度回归分解成了 n 个 bin 分类与 bin 内残差回归两个问题)。最终解码为描述目标边缘的信息：\((l,t,r,b,\theta)\)。</li>
</ul>
<p>　　Loss 的计算首先得区分正负样本。正负样本的划分思想与传统的差不多，主要思想是正负样本过渡区域引入 Ignore。如图 4. 所示，设 3D 真值框属性表示为 \((x,y,w,l,\theta)\)，正样本区域设计为 \((x,y,\sigma _ 1w,\sigma _ 1l,\theta)\)，定义另一缩小框 \((x,y,\sigma _ 2w,\sigma _ 2l,\theta)\)，其中 \(\sigma _ 1 &lt; \sigma _ 2\)。由此可得，灰色为正样本区域，黄色为 Ignore 区域，其它为负样本区域。<br>
　　对于分类的 Loss，直接对正负样本进行 Focal Loss 计算。对于回归分支，则采用正样本的平均 Loss。回归 Loss 由目标框的 IoU Loss 以及角度 Loss 组成，角度 Loss 又由 bin 分类 Loss 加 bin 残差回归 Loss 组成。这里不做展开。<br>
　　需要注意的是，分类分支只在训练的时候计算，Inference 时候只作回归分支的计算，从而得到每个像素感知到的目标边缘的信息。</p>
<h3 id="denficonv">2.2. DENFIConv</h3>
<p><img src="/DENFIDet/dconv.png" width="80%" height="80%" title="图 3. dconv"> 　　如图 3. 所示，Deformable Convolution 的思想是自动去寻找感兴趣的卷积区域。DBPM 获得每个像素点的目标边缘信息以后，自然的，接下来对像素点的卷积运算，运用可变形卷积可以捕捉更准确的目标区域信息。 <img src="/DENFIDet/DSDC.png" width="60%" height="60%" title="图 4. DSDC"> 　　如图 4. 所示，结合 Deformable Convolution 与 depth-wise separable Convolution，本文提出了 Depth-wise Separable Deformable Convolution。即将 \(3\times 3\) 的可变形卷积拆解成 \(3\times 3\) 的 Depth-wise 卷积以及 \(1\times 1\) 的可变形卷积，极大减少参数量。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Xu, Guodong, et al. "Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds." arXiv (2020): arXiv-2004.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Joint 3D Instance Segmentation and Objection Detection for Autonomous Driving&quot;</title>
    <url>/Instance-Seg-and-Obj-Det/</url>
    <content><![CDATA[<p>　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处理来得到目标框属性。越密集的检测方案，因为样本较多(一定程度增大了样本空间)，所以学习越困难，但是理论上有极高的召回率。随着一系列技术的发展，如 Focal-loss 等，密集检测性能得以超过二阶段的 Anchor-based 方案，具体描述可参考 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>。<br>
　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>借鉴 2D Instance Segmentation 思路，提出了一种同时作 3D Instance Segmentation 与 Detection 的方法。百度 Apollo 中的点云分割方法就是俯视图下 Instance Segmentation 然后后处理得到目标 Polygon 与 BBox 的思路，这种方法虽然后处理较为复杂，但是有超参数较少且召回率高的特点。本文算是该方法的 3D 版本(想法很自然，被人捷足先登。。)。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/Instance-Seg-and-Obj-Det/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，本方法由三部分构成：点级别的分类及回归，候选目标聚类，目标框优化。</p>
<ul>
<li><strong>点级别的分类与回归</strong><br>
原始点云经过 Backbone 网络提取局部及全局特征，这里的 Backbone 网络可以是任意能提取点级别特征的网络。基于 Backbone 网络提取的特征，可进行点级别的 Semantic Segmentation 以及 Instance-aware SE(Spatial Embedding)。SE 回归的是每个点距离目标中心点的 offset，该目标的 size，以及该目标的朝向。</li>
<li><strong>候选目标聚类</strong><br>
基于预测的 SE，将每个点的位置加上距离目标中心点的 offset，然后可通过简单的聚类算法(如 K-means)即可得到各个目标的点云集合，取 top k 个该点云集合回归的目标框属性，作下一步的目标框进一步优化。</li>
<li><strong>目标框优化</strong><br>
基于候选目标聚类得到的目标框，提取目标点集，将其转换到该目标 Local 坐标系下，作进一步的目标框优化。</li>
</ul>
<h2 id="instance-aware-se">2. Instance-aware SE</h2>
<p>　　该框架的关键是 Instance-aware SE 的回归，回归量有：距离目标中心点的 offset，目标 size，目标 orientation。传统的 Instance Segmentation 做法是 Feature Embedding，将相同 Instance 的特征拉近，不同的 Instance 的特征推远，这种方法很难构造有效的 Loss 函数，而且同为车的不同 Instance，其特征已经非常接近。而本文 Spatial Embedding 中 offset 的回归量，经过聚类后处理，可以很容易的得到 Instance Segmentation 结果。<br>
　　Apollo 点云分割的方案中，是在俯视图的 2D 栅格下做的，主要回归量也是这三种，不同的是，2D 栅格是离散的，所以根据 offset 找某一点的中心点时，可以迭代的进行，然后投票出中心点位置，后处理可以做的更细致。这里不做展开，有机会以后写一篇详解。</p>
<h2 id="loss">3. Loss</h2>
<p>　　Loss 项由 Semantic Segmentation，SE，3D BBox regression 组成： <span class="math display">\[ L = L _ {seg-cls}+L _ {SE}+L _ {reg} \tag{1}\]</span> Semantic Segmentation Loss 为： <span class="math display">\[ L _ {seg-cls}=-\sum _ {i=1}^C (y _ i\mathrm{log}(p _ i)(1-p _ i)^{\gamma}\alpha _ i+(1- y _ i)\mathrm{log}(1-p _ i)(p _ i)^{\gamma}(1-\alpha _ i)) \tag{2}\]</span> 其中 \(C\) 表示类别数；如果某点属于某类，那么 \(y _ i=1\)；\(p _ i\) 表示预测为第 \(i\) 类的概率；\(\gamma,\alpha\) 为超参数。<br>
SE Loss 为： <span class="math display">\[ L _ {SE} = \frac{1}{N}\sum _ {i=1}^N\frac{1}{N _ c}\sum _ {i\in ins _ c}^{N _ c}(\mathcal{l} _ {offset}^i+\mathcal{l} _ {size}^i+\mathcal{l} _ {\theta}^i) \tag{3}\]</span> 其中 \(N\) 为 Instance 个数，\(N _ c\) 为内部点数，\(\mathcal{l}\) 为 L1 Smooth Loss。<br>
BBox regression Loss 为 rotated 3D IOU Loss： <span class="math display">\[ L _ {reg} = 1-\mathbf{IoU}(B _ g,B _ d)\tag{4}\]</span></p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Zhou, Dingfu, et al. "Joint 3D Instance Segmentation and Object Detection for Autonomous Driving." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
        <tag>Semantic Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;SA-SSD: Structure Aware Single-stage 3D Object Detection from Point Cloud&quot;</title>
    <url>/SA-SSD/</url>
    <content><![CDATA[<p>　　Voxel-based 3D Detection 相比 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 的缺点是特征提取不仅在 Voxel 阶段损失了一定的点云信息，而且 Voxel 化后丢失了点云之间的拓扑关系。<a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 中详细描述了几种 Point-based 方法，这种方法目前比较棘手的地方是，即使作 Inference 时，也需要作 kd-tree 搜索与采样等运算量较大的操作。那么如何榨干 Voxel-based 的性能对工业界落地就显得比较重要了，本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种单阶段的 Voxel-based 3D 检测方法，并借助了 Point 级别特征提取的相关策略，使得检测性能有较大提升。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/SA-SSD/framework.png" width="100%" height="100%" title="图 1. Framework of SA-SSD"> 　　如图 1. 所示，SA-SSD 由三部分组成：Backbone，Detection Head，Auxiliary Network。<br>
　　Backbone 的输入是栅格化后的点云表示方式，文中栅格大小设定为 \(0.05m,0.05m,0.1m\)。Backbone 由一系列的 3D convolution 组成，因为需要保留空间三维位置信息，作 Voxel-to-Point 的映射。这里如果用 2D convolution 代替，那么 Auxiliary Network 估计也只能作 BridView 的分割了。<br>
　　Detection Head 主体就是传统 Anchor-Free 结构，一个分支用于预测每个特征层像素点的 Confidence，另一个分支用于预测基于每个特征层像素点的 BBox 属性，如，以该点为 "Anchor" 的四个顶点坐标。此外，为了消除 One-Stage 方法中目标框与置信度不对齐的问题，本文引入 Part-sensitive Warping 来实现与 PSRoiAlign 类似的作用，实现两者的对齐。<br>
　　Auxiliary Network 只在训练的阶段起作用，Inference 阶段不需要计算。该模块的作用是训练时通过 Voxel-to-Point 特征映射来反向传播监督 Backbone 中的 Voxel 特征学习 Point 级别的特征，包括点云的空间拓扑关系。<strong>当然 Inference 时也可以保留该分割模块，那么还可以增加点级别的特征反映射到 Voxel 的模块(Point-to-Voxel)，进一步作特征增强。</strong></p>
<h2 id="detachable-auxiliary-network">2. Detachable Auxiliary Network</h2>
<p><img src="/SA-SSD/sa.png" width="60%" height="60%" title="图 2. Structured Aware Feature Learning"> 　　如图 2. 所示，随着 Backbone 特征提取的感受野增大(特征分辨率下降)，背景点会接近目标的边缘，使得目标框大小不容易预测准确。本文提出的 Auxiliary Network，通过增加点级别分割及目标中心坐标预测任务，来监督 Backbone 特征层捕捉这种结构信息，从而达到更准确的目标检测的目的。<br>
　　Auxiliary Network 的输入来自 Backbone 各个分辨率的特征层。将特征层上不为零的特征点，通过 Voxel-to-Point 反栅格化映射到三维空间，设该特征点表示为 \(\{(f _ j,p _ j):j=1,...,M\}\)，其中 \(f\) 为特征向量，\(p\) 为坐标向量。有了栅格对应的伪三维坐标点下的特征表示后，即可插值出实际点云中每个点的特征向量。设点云中点的插值特征为：\(\{(\hat{f} _ i,p _ i):i=1,...,N\}\)，采用 Inverse Distance Weighted 方法进行插值： <span class="math display">\[ \hat{f} _ i = \frac{\sum _ {j=1}^Mw _ j(p _ i)f _ j}{\sum _ {j=1}^Mw _ j(p _ i)} \tag{1}\]</span> 其中： <span class="math display">\[w _ j(p _ i)=\left\{\begin{array}{l}
\frac{1}{\Vert p _ i-p _ j\Vert _ 2} &amp; \mathrm{if} p _ j\in\mathcal{N}(p _ i)\\
0 &amp; \mathrm{otherwise}
\end{array}\tag{2}\right.\]</span> \(\mathcal{N}(p _ i)\) 为球状区域，本文在四个分辨率下分别设定为：0.05m，0.1m，0.2m，0.4m。然后通过 cross-stage link 对各个分辨率下的点特征进行 concatenate 融合。最后通过感知机进行点云分割及目标中心点预测任务的构建。<br>
　　对于点级别前景分割的任务，经过 sigmoid 函数后，应用二分类的 Focal Loss： <span class="math display">\[ \mathcal{L} _ {seg} = \frac{1}{N _ {pos}}\sum _ i^N -\alpha(1-\hat{s} _ i)^{\gamma}\mathrm{log}(\hat{s} _ i) \tag{3}\]</span> 该分割任务使得目标检测的框更加准确，如图 2.c 所示。但是还得优化其尺度与形状。<br>
　　中心点的预测任务则能有效约束目标框的尺度与形状，具体的，预测的是每个属于目标的点云与中心点的相对位置(残差)。可用 Smooth-l1 来构建预测的中心点与实际中心点的 Loss。</p>
<h2 id="part-sensitive-warping">3. Part-sensitive Warping</h2>
<p><img src="/SA-SSD/psw.png" width="60%" height="60%" title="图 3. Part-sensitive Warping"> 　　One-Stage 方法都会有 Confidence 和 BBox 错位的现象，本文提出一种类似 PSROIAlign 但更有高效的 PSW 方法，具体步骤为：</p>
<ol type="1">
<li>对于分类分支，修改为 \(K\) 个 Part-sensitive 的 cls maps，每个 map 包含目标的部分信息，比如当 \(K=4\) 时，可以理解为将目标切分为 \(2\times 2\) 部分；</li>
<li>对于回归分支，将每个目标框的 Feature map 划分为 \(K\) 个子区域，每个区域的中心点作为采样点；</li>
<li>如图 3. 所示，通过采样得到最终 cls map 的平均值。</li>
</ol>
<h2 id="experiment">4. Experiment</h2>
<p><img src="/SA-SSD/ablation.png" width="60%" height="60%" title="图 4. Ablation Study"> 　　如图 4. 所示，Auxiliary Network 能有效提升网络的定位精度，PSWarp 也能有效消除 Confidence 与 BBox 的错位影响。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> henhang, et al. "Structure Aware Single-stage 3D Object Detection from Point Cloud."</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>非线性最小二乘</title>
    <url>/Non-linear-Least-Squares/</url>
    <content><![CDATA[<p>　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。<a href="/LOAM/" title="LOAM">LOAM</a>，，<a href="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/" title="Stereo-RCNN">Stereo-RCNN</a>，<a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving">Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving</a> 等均需要求解非线性最小二乘问题。其中 <a href="/LOAM/" title="LOAM">LOAM</a> 作为非常流行的激光 SLAM 框架，其后端是一个典型的非线性最优化问题，本文会作为实践进行代码级讲解。</p>
<h2 id="问题描述">1. 问题描述</h2>
<p>　　在前端观测-后端优化框架下，设观测数据对集合为：\(\{y _ i,z _ i\} _ {i=1}^m\)，待求解的变量参数 \(x\in\mathbb{R}^n\) 定义了观测数据对的映射关系，即 \(z _ i=h(y _ i;x)\)，由此得到有 \(m\) 个参数方程 \(F(x)=[f _ 1(x),...,f _ m(x)]^T\)，其中 \(f _ i(x) = z _ i-h(y _ i;x)\)。我们要找到最优的参数 \(x\) 来描述观测数据对之间的关系，即求解的最优化问题为： <span class="math display">\[\begin{align}
\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert F(x)\Vert ^2 \iff \mathop{\arg\min}\limits _ x\frac{1}{2}\sum _ i \rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\\
L\leq x \leq U
\end{align}\tag{1}\]</span> 其中 \(f _ i(\cdot)\) 为 Cost Function，\(\rho _ i(\cdot)\) 为 Loss Function，即核函数，用来减少离群点对非线性最小二乘优化的影响；\(L,U\) 分别为参数 \(x\) 的上下界。当核函数 \(\rho _ i(x)=x\) 时，就是常见的非线性最小二乘问题。<br>
　　《视觉 SLAM 十四讲》<a href="#1" id="1ref"><sup>[1]</sup></a>在 SLAM 的状态估计问题中，从概率学角度导出了最大似然估计求解状态的方法，并进一步引出了最小二乘问题。回过头来看，本文很多内容在《视觉 SLAM 十四讲》中已经有非常清晰的描述，可作进一步参考。</p>
<h2 id="问题求解">2. 问题求解</h2>
<p>　　根据 \(F(x)\) 求得雅克比矩阵(Jacobian)：\(J(x) \in\mathbb{R}^{m\times n}\)，即 \(J _ {ij}(x)=\frac{\partial f _ i(x)}{\partial x _ j}\)。目标函数的梯度向量为 \(g(x) = \nabla\frac{1}{2}\Vert F(x)\Vert ^ 2=J(x)^TF(x)\)。在 \(x\) 处将目标函数线性化：\(F(x+\Delta x)\approx F(x)+J(x)\Delta x\)。由此非线性最小二乘问题可转换为线性最小二乘求解残差量 \(\Delta x\) 来近似求解： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2\tag{2}\]</span> 根据如何控制 \(\Delta x\) 的大小，非线性优化算法可分为两大类：</p>
<ul>
<li>Line Search
<ul>
<li>Gradient Descent</li>
<li>Gaussian-Newton</li>
</ul></li>
<li>Trust Region
<ul>
<li>Levenberg-Marquardt</li>
<li>Dogleg</li>
<li>Inner Iterations</li>
<li>Non-monotonic Steps</li>
</ul></li>
</ul>
<p>Line Search 首先确定迭代方向，然后最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 确定迭代步长；Trust Region 则划分一个局部区域，在该区域内求解最优值，然后根据近似程度，扩大或缩减该局部区域范围。Trust Region 相比 Linear Search，数值迭代会更加稳定。这里介绍几种有代表性的方法：属于 Line Search 的梯度下降法，高斯牛顿法，以及属于 Trust Region 的 LM 法。</p>
<h3 id="梯度下降法">2.1. 梯度下降法</h3>
<p>　　将目标函数式(1)在 \(x\) 附近泰勒展开： <span class="math display">\[ \Vert F(x+\Delta x)\Vert ^2 \approx \Vert F(x)\Vert ^2 + J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x \tag{3}\]</span> 其中 \(H\) 是二阶导数(Hessian 矩阵)。<br>
　　如果保留一阶导数，那么增量的解就为： <span class="math display">\[\Delta x = -\lambda J^T(x) \tag{4}\]</span> 其中 \(\lambda\) 为步长，可预先由相关策略设定。<br>
　　如果保留二阶导数，那么增量方程为： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x} \Vert F(x)\Vert ^2+J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x\tag{5}\]</span> 对 \(\Delta x\) 求导即可求解增量的解为： <span class="math display">\[\Delta x = -H^{-1}J^T \tag{6}\]</span> 　　一阶梯度法又称为最速下降法，二阶梯度法又称为牛顿法。一阶和二阶法都是将函数在当前值下泰勒展开，然后线性得求解增量值。最速下降法过于贪心，容易走出锯齿路线，反而增加迭代步骤。牛顿法需要计算 \(H\) 矩阵，计算量较大且困难。</p>
<h3 id="高斯牛顿法">2.2. 高斯牛顿法</h3>
<p>　　 将式(2)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}
&amp;J(x)^TJ(x)\Delta x=-J(x)^TF(x)\\
\iff &amp; H\Delta x=g
\end{align}\tag{7}\]</span> 相比牛顿法，高斯牛顿法不用计算 \(H\) 矩阵，直接用 \(J^TJ\) 来近似，所以节省了计算量。但是高斯牛顿法要求 \(H\) 矩阵是可逆且正定的，而实际计算的 \(J^TJ\) 是半正定的，所以 \(J^TJ\) 会出现奇异或病态的情况，此时增量的稳定性就会变差，导致迭代发散。另一方面，增量较大时，目标近似函数式(2)就会产生较大的误差，也会导致迭代发散。这是高斯牛顿法的缺陷。高斯牛顿法的步骤为：</p>
<ol type="1">
<li>根据式 (7) 求解迭代步长 \(\Delta x\)；</li>
<li>变量迭代：\(x ^ * \leftarrow x+\Delta x\)；</li>
<li>如果 \(\Vert F(x ^ * )-F(x)\Vert &lt; \epsilon\)，则收敛，退出迭代，否则重复步骤 1.；</li>
</ol>
<p>高斯牛顿法简单的将 \(\alpha\) 置为 1，而其它 Line Search 方法会最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 来确定 \(\alpha\) 值。</p>
<h3 id="lm-法">2.3. LM 法</h3>
<p>　　Line Search 依赖线性化近似有较高的拟合度，但是有时候线性近似效果较差，导致迭代不稳定；Region Trust 就是解决了这种问题。高斯牛顿法中采用的近似二阶泰勒展开只在该点附近有较好的近似结果，对 \(\Delta x\) 添加一个信赖域区域，就变为 Trust Region 方法。其最优化问题转换为： <span class="math display">\[\begin{align}
\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^2 \\
\Vert D(x)\Delta x\Vert ^2 \leq \mu\\
L\leq x \leq U\\
\end{align}\tag{8}\]</span> 用 Lagrange 乘子将其转换为无约束优化问题： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2+\frac{1}{\mu}\Vert D(x)\Delta x\Vert ^2 \tag{9}\]</span> 其中 Levenberg 提出的方法中 \(D=I\)，相当于把 \(\Delta x\) 约束在球中；Marquart 提出的方法中将 \(D\) 取为非负数对角阵，通常为 \(J(x)^TJ(x)\) 的对角元素平方根。<br>
　　对于信赖域区域 \(\mu\) 的定义，一个比较好的方式是根据近似模型与实际函数之间的差异来确定这个范围：如果差异小，那么增大信赖域；反之减小信赖域。因此，考虑： <span class="math display">\[\rho = \frac{\Vert F(x+\Delta x)\Vert ^2-\Vert F(x)\Vert ^2}{\Vert J(x)\Delta x+F(x)\Vert ^2-\Vert F(x)\Vert ^2} \tag{10}\]</span> 　　 将式(9)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}
&amp;\left(J(x)^TJ(x)+\frac{2}{\mu}D^T(x)D(x)\right)\Delta x=-J(x)^TF(x)\\
\iff &amp; (H+\lambda D^TD)\Delta x=g
\end{align}\tag{11}\]</span> 当 \(\lambda\) 较小时，接近于高斯牛顿法；当 \(\lambda\) 较大时，接近于最速下降法。LM 法的步骤为：</p>
<ol type="1">
<li>根据式(11)求解迭代步长 \(\Delta x\);</li>
<li>根据式(10)求解 \(\rho\);</li>
<li>若 \(\rho &gt; \eta _ 1\)，则 \(\mu = 2\mu\);</li>
<li>若 \(\rho &lt; \eta _ 2\)，则 \(\mu = 0.5\mu\);</li>
<li>若 \(\rho &gt; \epsilon\)，则 \(x ^ * \leftarrow x+\Delta x\)；</li>
<li>如果满足收敛条件，则结束，否则继续步骤1.；</li>
</ol>
<h2 id="ceres-实践">3. Ceres 实践</h2>
<p>　　Ceres 是谷歌开发的一个用于非线性优化的库，使用 Ceres 库有以下几个步骤：</p>
<ul>
<li>构建 Cost Function，式(1)中的 \(\rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\) 即为代码中需要增加的 ResidualBlock；</li>
<li>累加的 Cost Function 构成最终的 Loss Function 目标函数；</li>
<li>配置求解器参数并求解问题；</li>
</ul>
<h3 id="例子-曲线拟合">3.1. 例子-曲线拟合</h3>
<p>　　以下代码为拟合曲线参数的简单例子：</p>
<p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// copy from http://zhaoxuhui.top/blog/2018/04/04/ceres&amp;ls.html</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ceres/ceres.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> ceres;</span><br><span class="line"></span><br><span class="line"><span class="comment">//vector,用于存放x、y的观测数据</span></span><br><span class="line"><span class="comment">//待估计函数为y=3.5x^3+1.6x^2+0.3x+7.8</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; xs;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ys;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义CostFunctor结构体用于描述代价函数</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">CostFunctor</span>&#123;</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">double</span> x_guan,y_guan;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//构造函数，用已知的x、y数据对其赋值</span></span><br><span class="line">  CostFunctor(<span class="keyword">double</span> x,<span class="keyword">double</span> y)</span><br><span class="line">  &#123;</span><br><span class="line">    x_guan = x;</span><br><span class="line">    y_guan = y;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//重载括号运算符，两个参数分别是估计的参数和由该参数计算得到的残差</span></span><br><span class="line">  <span class="comment">//注意这里的const，一个都不能省略，否则就会报错</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> params,T* residual)</span><span class="keyword">const</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    residual[<span class="number">0</span>]=y_guan-(params[<span class="number">0</span>]*x_guan*x_guan*x_guan+params[<span class="number">1</span>]*x_guan*x_guan+params[<span class="number">2</span>]*x_guan+params[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//生成实验数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">generateData</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  RNG rng;</span><br><span class="line">  <span class="keyword">double</span> w_sigma = <span class="number">1.0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">double</span> x = i;</span><br><span class="line">    <span class="keyword">double</span> y = <span class="number">3.5</span>*x*x*x+<span class="number">1.6</span>*x*x+<span class="number">0.3</span>*x+<span class="number">7.8</span>;</span><br><span class="line">    xs.push_back(x);</span><br><span class="line">    ys.push_back(y+rng.gaussian(w_sigma));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"x:"</span>&lt;&lt;xs[i]&lt;&lt;<span class="string">" y:"</span>&lt;&lt;ys[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//简单描述我们优化的目的就是为了使我们估计参数算出的y'和实际观测的y的差值之和最小</span></span><br><span class="line"><span class="comment">//所以代价函数(CostFunction)就是y'-y，其对应每一组观测值与估计值的残差。</span></span><br><span class="line"><span class="comment">//由于我们优化的是残差之和，因此需要把代价函数全部加起来，使这个函数最小，而不是单独的使某一个残差最小</span></span><br><span class="line"><span class="comment">//默认情况下，我们认为各组的残差是等权的，也就是核函数系数为1。</span></span><br><span class="line"><span class="comment">//但有时可能会出现粗差等情况，有可能不等权，但这里不考虑。</span></span><br><span class="line"><span class="comment">//这个求和以后的函数便是我们优化的目标函数</span></span><br><span class="line"><span class="comment">//通过不断调整我们的参数值，使这个目标函数最终达到最小，即认为优化完成</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  generateData();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//创建一个长度为4的double数组用于存放参数</span></span><br><span class="line">  <span class="keyword">double</span> params[<span class="number">4</span>]=&#123;<span class="number">1.0</span>&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//第一步，创建Problem对象，并对每一组观测数据添加ResidualBlock</span></span><br><span class="line">  <span class="comment">//由于每一组观测点都会得到一个残差，而我们的目的是最小化所有残差的和</span></span><br><span class="line">  <span class="comment">//所以采用for循环依次把每个残差都添加进来</span></span><br><span class="line">  Problem problem;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//利用我们之前写的结构体、仿函数，创建代价函数对象，注意初始化的方式</span></span><br><span class="line">    <span class="comment">//尖括号中的参数分别为误差类型，输出维度(因变量个数)，输入维度(待估计参数的个数)</span></span><br><span class="line">    CostFunction* cost_function = <span class="keyword">new</span> AutoDiffCostFunction&lt;CostFunctor,<span class="number">1</span>,<span class="number">4</span>&gt;(<span class="keyword">new</span> CostFunctor(xs[i],ys[i]));</span><br><span class="line">    <span class="comment">//三个参数分别为代价函数、核函数和待估参数</span></span><br><span class="line">    problem.AddResidualBlock(cost_function,<span class="literal">NULL</span>,params);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第二步，配置Solver</span></span><br><span class="line">  Solver::Options options;</span><br><span class="line">  <span class="comment">//配置增量方程的解法</span></span><br><span class="line">  options.linear_solver_type=ceres::DENSE_QR;</span><br><span class="line">  <span class="comment">//是否输出到cout</span></span><br><span class="line">  options.minimizer_progress_to_stdout=<span class="literal">true</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第三步，创建Summary对象用于输出迭代结果</span></span><br><span class="line">  Solver::Summary summary;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第四步，执行求解</span></span><br><span class="line">  Solve(options,&amp;problem,&amp;summary);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第五步，输出求解结果</span></span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;summary.BriefReport()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p0:"</span>&lt;&lt;params[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p1:"</span>&lt;&lt;params[<span class="number">1</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p2:"</span>&lt;&lt;params[<span class="number">2</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p3:"</span>&lt;&lt;params[<span class="number">3</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="例子-loam">3.2. 例子-LOAM</h3>
<p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 前端提取线和面特征，后端最小化线和面的匹配误差。其源码实现了整个最优化过程，ALOAM<a href="#2" id="2ref"><sup>[2]</sup></a> 将后端代码用 Ceres 实现，这里对其作理解与分析。</p>
<p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarEdgeFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	LidarEdgeFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_a_,</span><br><span class="line">					Eigen::Vector3d last_point_b_, <span class="keyword">double</span> s_)</span><br><span class="line">		: curr_point(curr_point_), last_point_a(last_point_a_), last_point_b(last_point_b_), s(s_) &#123;&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">	<span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line"></span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpa&#123;T(last_point_a.x()), T(last_point_a.y()), T(last_point_a.z())&#125;;</span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpb&#123;T(last_point_b.x()), T(last_point_b.y()), T(last_point_b.z())&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">		Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">		Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">		q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">		lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; nu = (lp - lpa).cross(lp - lpb);</span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; de = lpa - lpb;</span><br><span class="line"></span><br><span class="line">		residual[<span class="number">0</span>] = nu.x() / de.norm();</span><br><span class="line">		residual[<span class="number">1</span>] = nu.y() / de.norm();</span><br><span class="line">		residual[<span class="number">2</span>] = nu.z() / de.norm();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_a_,</span></span></span><br><span class="line"><span class="function"><span class="params">									   <span class="keyword">const</span> Eigen::Vector3d last_point_b_, <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">				LidarEdgeFactor, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line">			<span class="keyword">new</span> LidarEdgeFactor(curr_point_, last_point_a_, last_point_b_, s_)));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	Eigen::Vector3d curr_point, last_point_a, last_point_b;</span><br><span class="line">	<span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>　　对于 Point2Line 误差，为了衡量该线特征上的点是否在地图对应的线特征上，在地图线特征上采样两个点，加上该点，组成两个向量，向量叉乘即可描述匹配误差。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarPlaneFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	LidarPlaneFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_j_,</span><br><span class="line">					 Eigen::Vector3d last_point_l_, Eigen::Vector3d last_point_m_, <span class="keyword">double</span> s_)</span><br><span class="line">		: curr_point(curr_point_), last_point_j(last_point_j_), last_point_l(last_point_l_),</span><br><span class="line">		  last_point_m(last_point_m_), s(s_)</span><br><span class="line">	&#123;</span><br><span class="line">		ljm_norm = (last_point_j - last_point_l).cross(last_point_j - last_point_m);</span><br><span class="line">		ljm_norm.normalize();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">	<span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line"></span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpj&#123;T(last_point_j.x()), T(last_point_j.y()), T(last_point_j.z())&#125;;</span><br><span class="line">		<span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpl&#123;T(last_point_l.x()), T(last_point_l.y()), T(last_point_l.z())&#125;;</span></span><br><span class="line">		<span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpm&#123;T(last_point_m.x()), T(last_point_m.y()), T(last_point_m.z())&#125;;</span></span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; ljm&#123;T(ljm_norm.x()), T(ljm_norm.y()), T(ljm_norm.z())&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">		Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">		Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">		q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">		Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">		lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">		residual[<span class="number">0</span>] = (lp - lpj).dot(ljm);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_j_,</span></span></span><br><span class="line"><span class="function"><span class="params">									   <span class="keyword">const</span> Eigen::Vector3d last_point_l_, <span class="keyword">const</span> Eigen::Vector3d last_point_m_,</span></span></span><br><span class="line"><span class="function"><span class="params">									   <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">				LidarPlaneFactor, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line">			<span class="keyword">new</span> LidarPlaneFactor(curr_point_, last_point_j_, last_point_l_, last_point_m_, s_)));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	Eigen::Vector3d curr_point, last_point_j, last_point_l, last_point_m;</span><br><span class="line">	Eigen::Vector3d ljm_norm;</span><br><span class="line">	<span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>　　对于 Point2Plane 误差，为了衡量该面特征上的点是否在地图对应的面特征上，在地图面特征上采样一个点，加上该点，组成向量，然后点乘面的法向量即可衡量匹配误差。</p>
<h3 id="例子-ba">3.3. 例子-BA</h3>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// copy from https://www.jianshu.com/p/3df0c2e02b4c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/ceres.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/rotation.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read a Bundle Adjustment in the Large dataset.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BALProblem</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  ~BALProblem() &#123;</span><br><span class="line">    <span class="keyword">delete</span>[] point_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] camera_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] observations_;</span><br><span class="line">    <span class="keyword">delete</span>[] parameters_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">num_observations</span><span class="params">()</span>       <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> num_observations_;               &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">double</span>* <span class="title">observations</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> observations_;                   &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_cameras</span><span class="params">()</span>          </span>&#123; <span class="keyword">return</span> parameters_;                     &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_points</span><span class="params">()</span>           </span>&#123; <span class="keyword">return</span> parameters_  + <span class="number">9</span> * num_cameras_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_camera_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_cameras() + camera_index_[i] * <span class="number">9</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_point_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_points() + point_index_[i] * <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">LoadFile</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename)</span> </span>&#123;</span><br><span class="line">    FILE* fptr = fopen(filename, <span class="string">"r"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fptr == <span class="literal">NULL</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_cameras_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_points_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_observations_);</span><br><span class="line"></span><br><span class="line">    point_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    camera_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    observations_ = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">2</span> * num_observations_];</span><br><span class="line"></span><br><span class="line">    num_parameters_ = <span class="number">9</span> * num_cameras_ + <span class="number">3</span> * num_points_;</span><br><span class="line">    parameters_ = <span class="keyword">new</span> <span class="keyword">double</span>[num_parameters_];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_observations_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, camera_index_ + i);</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, point_index_ + i);</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">        FscanfOrDie(fptr, <span class="string">"%lf"</span>, observations_ + <span class="number">2</span>*i + j);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_parameters_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%lf"</span>, parameters_ + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">FscanfOrDie</span><span class="params">(FILE *fptr, <span class="keyword">const</span> <span class="keyword">char</span> *format, T *value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num_scanned = <span class="built_in">fscanf</span>(fptr, format, value);</span><br><span class="line">    <span class="keyword">if</span> (num_scanned != <span class="number">1</span>) &#123;</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Invalid UW data file."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> num_cameras_;</span><br><span class="line">  <span class="keyword">int</span> num_points_;</span><br><span class="line">  <span class="keyword">int</span> num_observations_;</span><br><span class="line">  <span class="keyword">int</span> num_parameters_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>* point_index_;</span><br><span class="line">  <span class="keyword">int</span>* camera_index_;</span><br><span class="line">  <span class="keyword">double</span>* observations_;</span><br><span class="line">  <span class="keyword">double</span>* parameters_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Templated pinhole camera model for used with Ceres.  The camera is</span></span><br><span class="line"><span class="comment">// parameterized using 9 parameters: 3 for rotation, 3 for translation, 1 for</span></span><br><span class="line"><span class="comment">// focal length and 2 for radial distortion. The principal point is not modeled</span></span><br><span class="line"><span class="comment">// (i.e. it is assumed be located at the image center).</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SnavelyReprojectionError</span> &#123;</span></span><br><span class="line">  SnavelyReprojectionError(<span class="keyword">double</span> observed_x, <span class="keyword">double</span> observed_y)</span><br><span class="line">      : observed_x(observed_x), observed_y(observed_y) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> camera,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">const</span> T* <span class="keyword">const</span> point,</span></span></span><br><span class="line"><span class="function"><span class="params">                  T* residuals)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// camera[0,1,2] are the angle-axis rotation.</span></span><br><span class="line">    T p[<span class="number">3</span>];</span><br><span class="line">    ceres::AngleAxisRotatePoint(camera, point, p);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// camera[3,4,5] are the translation.</span></span><br><span class="line">    p[<span class="number">0</span>] += camera[<span class="number">3</span>];</span><br><span class="line">    p[<span class="number">1</span>] += camera[<span class="number">4</span>];</span><br><span class="line">    p[<span class="number">2</span>] += camera[<span class="number">5</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute the center of distortion. The sign change comes from</span></span><br><span class="line">    <span class="comment">// the camera model that Noah Snavely's Bundler assumes, whereby</span></span><br><span class="line">    <span class="comment">// the camera coordinate system has a negative z axis.</span></span><br><span class="line">    T xp = - p[<span class="number">0</span>] / p[<span class="number">2</span>];</span><br><span class="line">    T yp = - p[<span class="number">1</span>] / p[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Apply second and fourth order radial distortion.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; l1 = camera[<span class="number">7</span>];</span><br><span class="line">    <span class="keyword">const</span> T&amp; l2 = camera[<span class="number">8</span>];</span><br><span class="line">    T r2 = xp*xp + yp*yp;</span><br><span class="line">    T distortion = <span class="number">1.0</span> + r2  * (l1 + l2  * r2);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute final projected point position.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; focal = camera[<span class="number">6</span>];</span><br><span class="line">    T predicted_x = focal * distortion * xp;</span><br><span class="line">    T predicted_y = focal * distortion * yp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The error is the difference between the predicted and observed position.</span></span><br><span class="line">    residuals[<span class="number">0</span>] = predicted_x - observed_x;</span><br><span class="line">    residuals[<span class="number">1</span>] = predicted_y - observed_y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Factory to hide the construction of the CostFunction object from</span></span><br><span class="line">  <span class="comment">// the client code.</span></span><br><span class="line">  <span class="keyword">static</span> ceres::<span class="function">CostFunction* <span class="title">Create</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> observed_x,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> <span class="keyword">double</span> observed_y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;SnavelyReprojectionError, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>&gt;(</span><br><span class="line">                <span class="keyword">new</span> SnavelyReprojectionError(observed_x, observed_y)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> observed_x;</span><br><span class="line">  <span class="keyword">double</span> observed_y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  google::InitGoogleLogging(argv[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"usage: simple_bundle_adjuster &lt;bal_problem&gt;\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  BALProblem bal_problem;</span><br><span class="line">  <span class="keyword">if</span> (!bal_problem.LoadFile(argv[<span class="number">1</span>])) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"ERROR: unable to open file "</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">double</span>* observations = bal_problem.observations();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create residuals for each observation in the bundle adjustment problem. The</span></span><br><span class="line">  <span class="comment">// parameters for cameras and points are added automatically.</span></span><br><span class="line">  ceres::Problem problem;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bal_problem.num_observations(); ++i) &#123;</span><br><span class="line">    <span class="comment">// Each Residual block takes a point and a camera as input and outputs a 2</span></span><br><span class="line">    <span class="comment">// dimensional residual. Internally, the cost function stores the observed</span></span><br><span class="line">    <span class="comment">// image location and compares the reprojection against the observation.</span></span><br><span class="line"></span><br><span class="line">    ceres::CostFunction* cost_function =</span><br><span class="line">        SnavelyReprojectionError::Create(observations[<span class="number">2</span> * i + <span class="number">0</span>],</span><br><span class="line">                                         observations[<span class="number">2</span> * i + <span class="number">1</span>]);</span><br><span class="line">    problem.AddResidualBlock(cost_function,</span><br><span class="line">                             <span class="literal">NULL</span> <span class="comment">/* squared loss */</span>,</span><br><span class="line">                             bal_problem.mutable_camera_for_observation(i),</span><br><span class="line">                             bal_problem.mutable_point_for_observation(i));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make Ceres automatically detect the bundle structure. Note that the</span></span><br><span class="line">  <span class="comment">// standard solver, SPARSE_NORMAL_CHOLESKY, also works fine but it is slower</span></span><br><span class="line">  <span class="comment">// for standard bundle adjustment problems.</span></span><br><span class="line">  ceres::Solver::Options options;</span><br><span class="line">  options.linear_solver_type = ceres::DENSE_SCHUR;</span><br><span class="line">  options.minimizer_progress_to_stdout = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">  ceres::Solver::Summary summary;</span><br><span class="line">  ceres::Solve(options, &amp;problem, &amp;summary);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; summary.FullReport() &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　　这里使用了 Bundle Adjustment in the Large<a href="#3" id="3ref"><sup>[3]</sup></a> 数据集，观测量为图像坐标系下路标(特征)的像素坐标系，待优化的参数为各路标的 3D 坐标以及相机内外参，这里相机内外参有 9 个，其中位置及姿态 6 个，畸变系数 2 个，焦距 1 个。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> 高翔. 视觉 SLAM 十四讲: 从理论到实践. 电子工业出版社, 2017.<br>
<a id="2" href="#2ref">[2]</a> https://github.com/HKUST-Aerial-Robotics/A-LOAM<br>
<a id="3" href="#3ref">[3]</a> http://grail.cs.washington.edu/projects/bal/</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>OctoMap</title>
    <url>/OctoMap/</url>
    <content><![CDATA[<p>　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 就是一种能在线检测静态障碍物的栅格地图。自动驾驶领域，地图的用处有：</p>
<ul>
<li><strong>高精度定位</strong>，一般是 3D 栅格地图，但是栅格中近似存储点云原始信息；</li>
<li><strong>路径规划</strong>，不同规划算法依赖不同地图，自动驾驶中比较靠谱又简单的规划算法一般依赖拓扑地图，俗称高精度语义地图，描述一些车道线等路面拓扑关系；而在室内或低速无道路信息场景，则会用如 \(A ^ * \) 算法在栅格地图上进行路径规划；</li>
<li><strong>辅助感知检测未定义类别的障碍物</strong>，有人称之为静态地图，一般是 2.5D 栅格地图，图层可以自定义一些语义信息；</li>
</ul>
<p>下游不同模块对不同存储方式的利用效率是不同的，所以需要针对不同下游任务设计不同地图建模方式。本文<a href="#1" id="1ref"><sup>[1]</sup></a>介绍了一种基于八叉树的栅格地图建模方法。<br>
　　对于机器人而言，类似 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 能建模 FREE，OCCUPIED，UNMAPPED AREAS 的地图是信息量比较丰富的，但是 Grid-Mapping 是 2D 的。这里对 3D 地图有以下要求：</p>
<ul>
<li><strong>Probabilistic Representation</strong><br>
测量都会有不确定性，这种不确定性需要用概率表征出来；另外多传感器融合也需要基于概率的表示；</li>
<li><strong>Modeling of Unmapped Areas</strong><br>
对机器人导航而言，显式得表示哪些区域是观测未知的也非常重要；</li>
<li><strong>Efficiency</strong><br>
地图构建与存储需要非常高效，一般而言，地图的内存消耗会是瓶颈；</li>
</ul>
<p><img src="/OctoMap/maps.png" width="90%" height="90%" title="图 1. Different Representations of Maps"> 　　如图 1. 所示，原始点云地图信息量丰富，但是不能结构化存储；Elevation Maps 与 Multi-level Surface Maps 虽然高效，但是不能表征未观测的区域信息。OctoMap 可以认为是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 的 3D 版本，信息量丰富且高效。</p>
<h2 id="octomap-mapping-framework">1. OctoMap Mapping Framework</h2>
<h3 id="octrees">1.1. Octrees</h3>
<p><img src="/OctoMap/OctoMap.png" width="40%" height="40%" title="图 2. 八叉树地图存储"> 　　如图 2. 所示，八叉树是将空间递归得等分成八份(QuadTree 四叉树则等分为四份)，每个节点可以存储 Occupied，Free，Unknown 信息(Occupied 概率即可)。此外，如果子节点的状态都一样，那么可以进行剪枝，只保留大节点低分辨率的 Voxel，达到紧凑存储的目的。<br>
　　时间复杂度上，对于有 \(n\) 个节点，深度为 \(d\) 的八叉树，那么单次查询的时间复杂度为 \(\mathcal{O}(d)=\mathcal{O}(\mathrm{log}\,n)\)；遍历节点的时间复杂度为 \(\mathcal{O}(n)\)。\(d = 16, r = 1cm\)，可以覆盖 \((655.36m)^3\)的区域。</p>
<h3 id="probabilistic-sensor-fusion">1.2. Probabilistic Sensor Fusion</h3>
<p>　　时序概率融合也是基于贝叶斯滤波，详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>，只不过这里是 3D Mapping，作 Raycasting 的时候采用 <a href="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/" title="What You See is What You Get">What You See is What You Get</a> 中提到的 Fast Voxel Traversal 算法。实际应用中，一般都会采用上下界限制概率值，这种限制也能提高八叉树的剪枝率。</p>
<h3 id="multi-resolution-queries">1.3. Multi-Resolution Queries</h3>
<p>　　由于八叉树的特性，OctoMap 支持低于最高分辨率的 Voxel 概率查询，即父节点是子节点的平均概率，或是子节点的最大概率: <span class="math display">\[
\bar{l}(n)=\frac{1}{8}\sum _ {i=1}^8 L (n _ i)\\
\hat{l}(n)=\max\limits _ iL(n _ i)
\tag{1}\]</span> 其中 \(l\) 是测量模型下概率的 log-odds 值。</p>
<h2 id="implementation-details-statics">2. Implementation Details &amp; Statics</h2>
<h3 id="memory-efficient-node-map-file-generation">2.1. Memory-Efficient Node &amp; Map File Generation</h3>
<p><img src="/OctoMap/save.png" width="60%" height="60%" title="图 3. Node Memory Consumption and Serialization"> 　　如图 3. 左图所示，每个节点只分配一个 float 型的数据存储以及指向子节点地址数组的地址指针(而不是直接包含子节点地址的指针)，只有存在子节点时，才会分配子节点的地址数组空间。由此在 32-bit 系统中(4 字节对齐)，每个父节点需要 40B，子节点需要 8B；在 64-bit 系统中(8 字节对齐)，每个父节点需要 80B(\(4+9\times 8\))，子节点需要 16B(\(4+8)\)。<br>
　　地图存储需要在信息量损失最小的情况下进行压缩。如图 3. 右图所示，存储序列化时，每个叶子节点总共需要 4B 概率值，不需要状态量；每个父节点总共需要 2B，表示 8 个子节点的 2bit 状态量(貌似与论文有出入，其不是最优的压缩)。在这种压缩方式下，大范围地图的存储大小一般也能接受。根据存储的地图重建地图时，只需要知道坐标原点即可。</p>
<h3 id="accessing-data-memory-consumption">2.2. Accessing Data &amp; Memory Consumption</h3>
<p><img src="/OctoMap/memusage1.png" width="60%" height="60%" title="图 4. Memory Usage VS. Scan Num."> 　　Freiburg 建图大小为 \((202\times 167\times 28) m^3\)，如图 4. 所示，随着点云扫描区域扩大，OctoMap 表示方式能有效降低建图大小。 <img src="/OctoMap/memusage2.png" width="60%" height="60%" title="图 5. Memory Usage VS. Resolution"> 　　图 5. 则说明建图大小与分辨率的关系。 <img src="/OctoMap/inserttime.png" width="60%" height="60%" title="图 6. Insert Date Time VS. Resolution"> <img src="/OctoMap/traversetime.png" width="60%" height="60%" title="图 7. Traverse Data Time VS. Depth"> 　　图 6. 显示了往图中插入一个节点所需时间，1000 个节点在毫秒级；图 7. 显示了遍历所有节点所需的时间，基本也在毫秒级。 <img src="/OctoMap/compress.png" width="60%" height="60%" title="图 8. Compression Ratio"> 　　通过限制概率上下界，可以剪枝压缩图，用 KL-diverge 来评估压缩前后图的分布相似性，图 8. 显示了压缩比与网络大小及相似性的关系。</p>
<h3 id="some-strategies">2.3. Some Strategies</h3>
<p><img src="/OctoMap/case.png" width="60%" height="60%" title="图 9. Corner Case Handle"> 　　如图 9. 所示，前后帧位姿的抖动，会导致 Occupied 持续观测的不稳定，所以需要一些领域约束策略来保证 Occupied 的稳定观测。这种类似的策略在 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 工程实现中也需要采用，因为实际的 Pose 肯定会有噪声，导致同一目标的栅格前后有一定概率不能完全命中。</p>
<h2 id="rethinking">3. ReThinking</h2>
<p>　　对于自动驾驶来说，高度方向的范围不需要很大，甚至四叉树足矣，如果采用八叉树，那么需要将高度方向的分辨率降低，从而更加紧凑的构建地图。<br>
　　此外自动驾驶肯定是需要大范围建图的，如平方千公里级别。所以切片式的地图存储与查询就显得尤为重要，换句话说，需要动态得载入局部地图，这就有两种思路：</p>
<ul>
<li>动态载入完全局部地图<br>
要求前后局部地图有一定的重叠，通过索引式的存储可以不存储重叠区域的地图信息；</li>
<li>动态载入部分局部地图<br>
随着机器人本体的运动，实时动态载入前方更远处的地图，丢掉后方远处的历史地图。这对在线地图结构的灵活性要求比较高，如果基于八叉树，那么需要作片区域剪枝及插入的操作，效率不一定高；</li>
</ul>
<p>　　在自动驾驶领域，目前用于高精度定位的栅格地图与用于 PNC 规划控制的拓扑地图(高精地图)已经比较成熟；而用于环境感知的静态语义地图还没形成大范围的共识。不管从工程实现效果及效率上，还是语义信息描述定义上，还需作很多探索与实践。比如，可以定义最底层的语义信息：地面高度，此外也可以把车道线信息打到栅格图层中去(但是可能加大对 PNC 的搜索计算量)，等等。所以可能最优的存储查询方式并不是八叉树，<strong>可能还是栅格化后并对每个栅格哈希化，牺牲一定的内存空间，然后作 \(O(1)\) 的快速插入与查询</strong>。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Hornung, Armin, et al. "OctoMap: An efficient probabilistic 3D mapping framework based on octrees." Autonomous robots 34.3 (2013): 189-206.</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>SLAM</tag>
        <tag>Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;What You See is What You Get, Exploiting Visibility for 3D Object Detection&quot;</title>
    <url>/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/</url>
    <content><![CDATA[<p>　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中较详细的阐述了 LiDAR 的测量模型，每个栅格可以标记为三个状态：UNKNOWN，FREE，OCCUPIED。传统的 Bird-View 3D Detection 没有显式得提取 UNKNOW 与 FREE 的信息(即没有提取 Visibility 信息)，而 UNKNOW 与 FREE 对数据增广及检测效果非常重要。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/visibility.png" width="90%" height="90%" title="图 1. Visibility or Freespace from LiDAR"> 　　如图 1. 所示，左图是传统的点云表示方式，无法区分红色区域是否有车，而右图则非常容易得区分哪个区域不可能有车，哪个区域可能有车。所以本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了显式提取点云 UNKNOWN 与 FREE 信息来辅助数据增广与提高目标检测精度的方法。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，本文的 3D 检测框架与传统的差不多，是 Anchor-Based 方法，主要不同点是输入网络的特征，即点云栅格化后提取出的特征不一样以及融合时序信息。并且，训练过程中，对数据增广做了精心的设计。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/fusion.png" width="90%" height="90%" title="图 3. Frusion Strategy"> 　　如图 3. 所示，点云栅格化后提取的特征不一样是指增加了 Visibility 图层。有两种融合方式，前融合是与点云栅格化后提取的特征作 Concate，然后输入到主干网络；后融合则是二者分别通过主干网络，然后再作 Concate。实验表明前融合效果较好。</p>
<h3 id="object-augmentation">1.1. Object Augmentation</h3>
<p>　　传统的数据增广关注在全帧点云的平移，旋转，翻转变换。本文则采用目标级别的数据增广。首先生成目标的点云集合，可以用 CAD 模型，也可以直接扣实际的目标点云(扣出来的点云增广能力有限)；然后将目标点云集合随机得放到全帧点云中。在放置的过程中需要模拟 LiDAR 的测量模型，也就是 Visibility 计算过程，这在第 2. 节中详细描述。实验表面能提升 ~9 个点。</p>
<h3 id="temporal-aggregation">1.2. Temporal Aggregation</h3>
<p>　　时序点云信息的利用可以有以下几种方法：</p>
<ul>
<li>将每帧点云栅格化，然后直接在 Chanel 层作 Concate，之后作 3D 卷积，或者先在 Chanel 维度作 1D 卷积，然后作 2D 卷积；</li>
<li>将点云中的点增加相对时间戳属性，然后作整体的栅格化，之后直接作传统的 2D 卷积；</li>
</ul>
<p>本文采用第二种方法，实验表明能提升 ~8 个点。</p>
<h2 id="visibility-computing">2. Visibility Computing</h2>
<p>　　<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中已经应用了 Raycasting 来计算 Visibility/Free。对于点云中的每一个点，我们不仅能获得该点探测到障碍物的这个信息，还能知道，传感器与该点之间的连线上是 Free 的。这就要求能高效得计算该连线相交 Voxel 的集合。该计算模型也用来修正 Object Augmentation 时的点云。</p>
<h3 id="efficient-voxel-traversal">2.1. Efficient Voxel traversal</h3>
<p>　　对每个点，都需要遍历传感器原点到该点所经过的 Voxel，采用 Fast Voxel Traversal<a href="#2" id="2ref"><sup>[2]</sup></a>方法来进行高效的 Voxel 遍历。</p>
<h3 id="raycasting-with-augmented-objects">2.2. Raycasting with Augmented Objects</h3>
<p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/augment.png" width="90%" height="90%" title="图 4. Rectify Object Augmentation"> 　　如图 4. 所示，本文设计了两种策略来修正物体增广：</p>
<ul>
<li>Culling，如果该物体是被遮挡的，那么直接去掉，这样会极大减少增广的物体；</li>
<li>Drilling，如果该物体是被遮挡的，那么将遮挡物去掉，即置为 Free；</li>
</ul>
<p>实验表明 Drilling 效果较好，在训练时采用该策略进行物体增广后的点云修正，作 Inference 时就直接计算 Freespace 即可。</p>
<h3 id="online-occupancy-mapping">2.3. Online Occupancy Mapping</h3>
<p>　　栅格内点云提取特征时融合了时序信息，Visibility 也需要融合时序信息，最直观的方式是将 3D Occupancy Map 进行时间维度的堆叠，获得 4D Map，这样对后续的计算量较大。本文采用 OctoMap<a href="#3" id="3ref"><sup>[3]</sup></a> 计算方式，作贝叶斯滤波，得到时序滤波的 3D Occupancy Map。原理与 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 一样，只不过这里是 3D 的。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Hu, Peiyun, et al. "What You See is What You Get: Exploiting Visibility for 3D Object Detection." arXiv preprint arXiv:1912.04986 (2019).<br>
<a id="2" href="#2ref">[2]</a> Amanatides, John, and Andrew Woo. "A fast voxel traversal algorithm for ray tracing." Eurographics. Vol. 87. No. 3. 1987.<br>
<a id="3" href="#3ref">[3]</a> Hornung, Armin, et al. "OctoMap: An efficient probabilistic 3D mapping framework based on octrees." Autonomous robots 34.3 (2013): 189-206.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Reconfigurable Voxels, A New Representation for LiDAR-Based Point Clouds&quot;</title>
    <url>/paper-reading-Reconfigurable-Voxels/</url>
    <content><![CDATA[<p>　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于<strong>点云分布的不均匀性</strong>，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布的不均匀问题，目前看到的有以下几种方法：</p>
<ol type="1">
<li>Deformable Convolution，采用可变形卷积方法，自动学习卷积核的连接范围，理论上应该能更有效得使卷积核连接到点密度较高的栅格；</li>
<li><a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a>
提出了一种极坐标栅格化方式，因为点云获取的特性，这种方法获得的栅格中点数较为均匀;</li>
<li>手动设计不同分辨率的栅格，作特征提取，然后融合。比如近处分辨率较高，远处较低的方式；</li>
<li>本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种自动选择栅格领域及分辨率，从而最大化卷积区域点数的方法；</li>
</ol>
<p><img src="/paper-reading-Reconfigurable-Voxels/reconfig.png" width="80%" height="80%" title="图 1. Reconfig Voxels"> 　　如图 1. 所示，本文提出的 Reconfigurable Voxel 方法，能自动选择领域内点数较多的栅格特征提取，进而作卷积运算，避免点数较少，从而信息量较少的栅格作特征提取操作；此外还可根据点数自动调整分辨率以获得合适的栅格点数。通过这种方法，每个栅格输入到网络前都能有效提取周围点数较多区域的特征信息。</p>
<h2 id="framework">1. Framework</h2>
<p><img src="/paper-reading-Reconfigurable-Voxels/pipeline.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. 所示，本文以检测任务为例，分三部分：Voxel/Pillar Feature Extraction，Backbone，RPN/Detection Head。后两个采用传统的方法，本文主要是改进 Voxel/Pillar Feature Extraction，这是输入到网络前的特征提取阶段。</p>
<h2 id="voxelpillar-feature-extraction">2. Voxel/Pillar Feature Extraction</h2>
<p>　　传统的输入到 2D 卷积网络的特征要么是手工提取的，要么是用 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 网络去学习每个 Voxel 的特征。由此输入到网络的特征不是最优的，因为点云的稀疏性会导致后面的 2D 卷积网络作特征提取时遇到很多“空”的 Voxel。本文提出的方法就能显式得搜索每个 Voxel 周围有点的区域作特征提取，使得之后 2D 卷积特征提取更加有效。其步骤为：</p>
<ul>
<li>点云栅格化，并存储每个 Voxel 周围 Voxel 的索引；</li>
<li>每个 Voxel 周围 Voxel 作 Biased Random Walk，去搜索有更稠密点云的 Voxel；</li>
<li>将每个 Voxel 与新搜索到的周围 Voxel 作特征提取与融合，得到该 Voxel 特征；</li>
</ul>
<h3 id="biased-random-walking-neighbors">2.1. Biased Random Walking Neighbors</h3>
<p>　　邻域 Voxel 搜索目标是：<strong>在距离较近的情况下寻找较稠密的 Voxel</strong>。由此设计几种策略：</p>
<ul>
<li>点数越少的 Voxel，有更高概率作 Random Walk，以及更多 Step 去周围相邻的 Voxel；</li>
<li>点数越多的 Voxel，有更高概率被其它 Voxel Random Walk 到；</li>
</ul>
<p>　　将以上策略数学化。设第 \(j\) 个 Voxel 有 \(N(j)\) 个点，最大点数为 \(n\)，其作 Random Walk 的概率为 \(P _ w(j)\)，步数 Step 为 \(S(j)\)，第 \(i\) 步到达的 Voxel 为 \(w _ j(i)\)，其四领域 Voxel 集合为 \(V(w _ j(i))\)，从该 Voxel 走到下一个 Voxel 的概率为 \(P(w _ j(i+1)|w _ j(i))\)。由此得到以上策略的数学描述： <span class="math display">\[P _ w(j)=\frac{1}{N(j)} \tag{1}\]</span> <span class="math display">\[S(j)=n-N(j)\tag{2}\]</span> <span class="math display">\[P\left(w _ j(i+1)|w _ j(i)\right) = \frac{N\left(w _ j(i+1)\right)}{\sum _ {v\in V(w _ j(i))}N(v)}\tag{3}\]</span> 需要注意的是，\(S(j)\) 是在开始时计算的，此后每走一步就减1。 <img src="/paper-reading-Reconfigurable-Voxels/random_walk.png" width="90%" height="90%" title="图 3. Random walk"> 　　如图 3. 所示，左边为单分辨率下 Voxel 搜索过程。</p>
<h3 id="reconfigurable-voxels-encoder">2.2. Reconfigurable Voxels Encoder</h3>
<p>　　每个 Voxel \(v _ i\) 搜索到最优的 4 领域 Voxel 集 \(V(v _ i)\) 后，需要融合得到该 Voxel 的特征： <span class="math display">\[\begin{align}
F(v _ i) &amp;= \psi\left(f _ {v _ i}, f _ {V(v _ i)}\right)\\
&amp;= \varphi _ 1\left[\varphi _ 2(f _ {v _ i}), \varphi _ 2\left(\sum _ {j=1}^4 W _ j(f _ {v _ i})f _ {V _ {j(v _ i)}}\right)\right] _ f
\tag{4}\end{align}\]</span> 其中 \(\varphi _ 1\) 为 low-level 操作，如 average pooling，\(\varphi _ 2\) 为 high-level 操作，如 MLP。</p>
<h3 id="multi-resolution-reconfigurable-voxels">2.3. Multi-resolution Reconfigurable Voxels</h3>
<p>　　图 3. 左边是单分辨率情况，Random Walking 可以拓展到多分辨率情形。当点云非常稀疏的时候，就很有必要降低栅格的分辨率。如图 3. 所示，\(P _ w\) 计算时除以 4，以维持与高分辨率的一致性；高分辨率到低分辨率搜索概率为 \(0.25P _ w\)，低分辨率到高分辨率搜索概率为 \(0.5P _ w\)。其余准则与单分辨率一致。实验结果表面多分辨率有一定提升，但是相比单分辨率提升不明显。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Wang, Tai, Xinge Zhu, and Dahua Lin. "Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds." arXiv preprint arXiv:2004.02724 (2020).</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;PolarNet&quot;</title>
    <url>/paper-reading-PolarNet/</url>
    <content><![CDATA[<p>　　Point-wise 特征提取在 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中已经有较为详细的描述，虽然 Point-wise 提取的特征更加精细，但是一般都有 KNN 构建及索引操作，计算量较大，而且实践中发现学习收敛较慢。Voxel-based 虽然理论上损失了一定的信息，但是能直接应用 2D 卷积网络，网络学习效率很高。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种在极坐标下栅格化后进行点云 Semantic Segmentation 的方法，相比传统的笛卡尔坐标系下栅格化有一定的优势。</p>
<h2 id="voxelization">1. Voxelization</h2>
<p><img src="/paper-reading-PolarNet/pts.png" width="98%" height="98%" title="图 1. Cartesian VS. Polar"> 　　如图 1. 所示，传统的笛卡尔坐标系下栅格化的栅格是矩形，而极坐标系下栅格是饼状的。激光雷达是在极坐标方式下获取点云的，所以由图可知，<strong>极坐标栅格化下，每个栅格拥有的点数更加均匀</strong>，有利于网络学习并减少计算量。此外，本文统计后显示，相比笛卡尔坐标栅格，极坐标的栅格内点属于同一目标的概率更大。</p>
<h2 id="polarnet-framework">2. PolarNet Framework</h2>
<p><img src="/paper-reading-PolarNet/framework.png" width="98%" height="98%" title="图 2. PolarNet"> 　　如图 2. 所示，点云经过 Polar 栅格化后，对每个栅格首先进行 PointNet 特征提取，然后对所有栅格作 ring-convolution 操作。<br>
　　ring-convolution 是指卷积在环形方向进行，没有边缘截断效应。实现上，将栅格从某处展开，然后边缘处用另一边对应的栅格进行 padding，即可用普通的卷积进行运算。<br>
　　网络是作 Voxel-wise 的分割，然后直接将预测的类别应用到栅格内的点云中。统计上，同一栅格内的点云属于不同类别的概率很低，所以本文并没进一步作 Point-wise 的分割。</p>
<h2 id="rethinking">3. Rethinking</h2>
<p>　　PolarNet 作 Semantic Segmentation 比其它方法提升很多。但是实际应用时，PolarNet 不能指定各个方向的范围，所以计算效率较低。比如，自动驾驶中，我们可以设定前 100m，后 60m，左右各 30m 的检测范围，笛卡尔坐标系下很容易进行栅格化，而极坐标下则没法搞。所以为了解决点云的分布不均匀问题，另一种思路是在笛卡尔坐标系下，近处打高分辨率的栅格，远处打低分辨率的栅格。具体实现，可以先用低分辨率过一遍网络，然后再对感兴趣的特定区域作高分辨率检测。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Zhang, Yang, et al. "PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation." arXiv preprint arXiv:2003.14032 (2020).</p>
]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>PointFlowNet</title>
    <url>/PointFlowNet/</url>
    <content><![CDATA[<p>　　点云的 Scene Flow 与 Semantic 一样是一个较低层的信息，通过 Point-Wise Semantic 信息可以作物体级别的检测，这种方式有很高的召回率，且超参数较少。同样，通过 Point-Wise Scene Flow 作目标级别的运动估计(当然也可作物体点级别聚类检测的线索)，也会非常鲁棒。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 将点级别/Voxel 级别的 Scene Flow 与 3D 目标检测融合在一起，作物体级别的运动估计，工作系统性较强。</p>
<h2 id="问题描述">1. 问题描述</h2>
<p>　　设 \(t\) 时刻点云 \(\mathbf{P} _ t\in\mathbb{R}^{M\times 3}\)，那么需要求解的未知量有：</p>
<ul>
<li>每个点的 Scene Flow: \(\mathbf{v} _ i\in\mathbb{R} ^3\);</li>
<li>每个点集的 Rigid Motion: \(\mathbf{R} _ i\in\mathbb{R}^{3\times 3}\)，\(\mathbf{t} _ i\in\mathbb{R}^{3}\);</li>
<li>每个物体的 3D 属性：Location，Orientation，Size，Rigid Motion;</li>
</ul>
<h2 id="算法框架">2. 算法框架</h2>
<p><img src="/PointFlowNet/framework.png" width="90%" height="90%" title="图 1. PoingFlowNet Framework"> 　　如图 1. 所示，PointFlowNet 由四部分组成，分别为：Feature Encoder，Scene Flow/Ego-motion Estimation and 3D Object Detection，Rigid Motion Estimation，Object Motion Decoder。Feature Encoder 将前后帧点云栅格化后作特征提取，然后 Context Encoder 作进一步的特征融合去提取；输出的特征第一个分支作 Voxel 级别的 Scene Flow 预测，进一步作每个点的 Rigid Motion 预测(<strong>每个点属于对应物体的 Motion 在该 Voxel 坐标系下的表示</strong>)；第二个分支作 Ego-Motion 的预测；第三个分支作 3D 目标检测，进一步作目标的 Motion Decoder。</p>
<h3 id="feature-encoder">2.1. Feature Encoder</h3>
<p>　　不同的点云特征提取方式都可采用，本文采用传统的 Bird-View Voxel 表示方式，然后作 2D/3D 卷积。同时还需要将前后帧的点云作特征融合，这里也完全可以采用 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 的特征提取形式。</p>
<h3 id="scene-flowrigid-motion-decoder">2.2. Scene Flow/Rigid Motion Decoder</h3>
<p>　　Scene Flow 是作 Bird-View 下 Voxel 级别的场景流预测，然后再预测 Rigid Motion。 <img src="/PointFlowNet/rigid-motion.png" width="80%" height="80%" title="图 2. Rigid MOtion Estimation"> 　　如图 2. 所示，世界坐标系 \(\mathbf{W}\) 下点 \(\mathbf{p}\) 的 scene flow 表示为 \(\mathbf{v}\)，刚体物体的局部坐标系从 \(\mathbf{A}\) 经过 \((\mathbf{R _ A, t _ A})\) 运动到 \(\mathbf{B}\) ，那么其 scene flow 可表示为： <span class="math display">\[\mathbf{v=[R _ A(p-o _ A)+t _ A]-(p-o _ A)} \tag{1}\]</span> 本文论证了两个定理：</p>
<ol type="1">
<li>scene flow 只能通过刚体局部坐标系的运动导出，不能直接通过世界坐标系下的刚体运动导出(除非运动无旋转量)。所以如图 1. 所示，通过 scene flow 预测出的 voxel motion 是局部坐标系下的，还需通过坐标变换到世界坐标系下。<strong>这里每个 Voxel 预测量的局部坐标系采用 Voxel 中心点</strong>。作目标运动估计时，"世界坐标系"其实可以定义为物体坐标系(Voxel 为局部坐标系)，最后再通过 Ego-motion 变换到世界坐标系。</li>
<li>不管是局部坐标系 \(\mathbf{A}\) 还是 \(\mathbf{B}\)，都能导出 scene flow。</li>
</ol>
<p>　　如图 2. 所示，实验也验证了 scene flow 不能直接学习到世界坐标系下的 translation 运动。</p>
<h3 id="ego-motion-regressor">2.3. Ego-motion Regressor</h3>
<p>　　根据前后帧的点云回归本车的运动(ego-motion)，ego-motion 建立局部坐标系与世界坐标系的联系。如果有更精准的外部模块估计的 ego-motion，则可以直接替换采用。</p>
<h3 id="d-object-detection-and-object-motion-decoder">2.4. 3D Object Detection and Object Motion Decoder</h3>
<p>　　Bird-view 下 Voxel 后的 3D 检测方法很多，可以是 Anchor-based，Anchor-free，Semantic Segmentation 等方法，其中如果采用 Semantic Segmentation + cluster 方法，那么 scene flow 的结果也可作为 cluster 的线索。<br>
　　有了 3D 目标以及目标内 Voxel 的 Rigid Motion 后，取平均或中值即可得到目标的 Motion。<br>
　　<strong>Voxel Rigid Motion 可以有两种回归方法：</strong></p>
<ol type="1">
<li>translation 真值为实际该 Voxel 的位移，rotation 为对应刚体的旋转量；</li>
<li>translation 与 rotation 均为对应刚体的位移与旋转量；</li>
</ol>
<p>我理解的本文是采用方法 1. 这种形式，这种形式的好处是回归的就是真实 Voxel 的位移，与输入的特征是 Voxel 级别对应的，但是简单的对目标内的 Voxel 取平均或中值只是目标位移的近似，实际目标的真实位移应该为旋转中心 Voxel 的位移。而方法 2. 是物体级别的回归量，均值即可反应物体的运动，只要构建物体级别的 Loss，用 Voxel 去学习物体级别的运动应该问题不大，所以可能方法 2. 更合理。</p>
<h2 id="loss-functions">3. Loss Functions</h2>
<p>　　采用 Voxel 级别的 Loss，总的 Loss 为： <span class="math display">\[\mathcal{L}=\alpha\mathcal{L} _ {flow}+\beta\mathcal{L} _ {rigmo} + \gamma\mathcal{L} _ {ego}+\mathcal{L} _ {det}\tag{2}\]</span> 这四部分具体的形式为：</p>
<ol type="1">
<li>Scene Flow Loss<br>
对于有效的 Voxel，作预测值与真值的 \(\mathcal{l} _ 1\) 误差： <span class="math display">\[\mathcal{L} _ {flow}=\frac{1}{K}\sum _ j\Vert \mathbf{v} _ j-\mathbf{v} _ j ^ * \Vert \tag{3}\]</span></li>
<li>Rigid Motion Loss<br>
对于有效的 Voxel，作预测值与真值(真值有两种形式，详见 2.4 讨论)的 \(\mathcal{l} _ 1\) 误差： <span class="math display">\[\mathcal{L} _ {rigmo} = \frac{1}{K}\sum _ j\Vert\mathbf{t} _ j-\mathbf{t} _ j^ * \Vert+\lambda\Vert\theta _ j-\theta _ j^ * \Vert\tag{4}\]</span></li>
<li>Ego-motion Loss<br>
同样的对预测值与真值作 \(\mathcal{l} _ 1\) Loss: <span class="math display">\[\mathcal{L} _ {ego}=\Vert\mathbf{t} _ {BG}-\mathbf{t} _ {BG}^ * \Vert+\lambda\Vert\theta _ {BG}-\theta _ {BG}^ * \Vert \tag{5}\]</span></li>
<li>Detection Loss<br>
不作赘述。</li>
</ol>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Behl, Aseem, et al. "Pointflownet: Learning representations for rigid motion estimation from point clouds." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
]]></content>
      <categories>
        <category>Scene Flow</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Scene Flow</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>The Normal Distributions Transform for Laser Scan Matching</title>
    <url>/paper-reading-The-Normal-Distributions-Transform/</url>
    <content><![CDATA[<p>　　机器人系统中，定位是非常重要的模块。基于 SLAM/VO/VIO 技术的算法能实时作机器人的自定位，但是这种开环下的里程计方案很容易累积绝对误差，使得定位漂移。而离线建立的地图因为有闭环检测，精度很高，所以基于地图的定位方法有很高的绝对定位精度。<br>
　　<a href="/LOAM/" title="LOAM">LOAM</a> 是一种基于点云的实时建图与定位方法，其中当前帧点云与前序建立的地图点云配准的方法，采用了提取线、面特征并建立点-线，点-面特征匹配误差函数，从而最小二乘非线性优化求解位姿。这种方案如果特征点噪声较大无匹配对，那么就会有较大的误差。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 将地图点云栅格化，每个栅格又统计点云的高斯分布，匹配的时候计算该帧点云在每个栅格的概率，从而迭代至最优匹配位姿。<br>
　　<strong>有闭环检测</strong>的 SLAM 建立的地图即可作为离线定位地图，定位的过程就是当前时刻点云与地图配准的过程，当然后续可以融合其它传感器(GPS，IMU)输出最终的绝对位姿。<strong>点云与地图配准的过程与建图时点云与局部地图或上一时刻点云配准的过程非常相似</strong>。本文介绍一种区别于 <a href="/LOAM/" title="LOAM">LOAM</a> 特征匹配的基于概率统计优化的 NDT 配准方法。</p>
<h2 id="点云配准算法过程">1. 点云配准算法过程</h2>
<p>　　考虑二维情况，本文点云配准算法过程为：</p>
<ol type="1">
<li>建立 \(t-1\) 帧点云的 NDT；</li>
<li>初始化待优化的相对位姿参数 \(T\);</li>
<li>用 \(T\) 将 \(t\) 帧点云变换到 \(t-1\) 坐标系；</li>
<li>找到变换每个变换点对应的 \(t-1\) 帧栅格的高斯分布；</li>
<li>该变换 \(T\) 的度量分数为变换点在高斯分布下的概率和；</li>
<li>用 Newton 法迭代优化 \(T\);</li>
<li>重复 3. 直到收敛；</li>
</ol>
<p>　　这里主要涉及 NDT，目标函数构建(即 \(T\) 的度量分数)，Newton 法优化三个内容。</p>
<h3 id="ndt">1.1. NDT</h3>
<p>　　NDT 是点云栅格化后一系列高斯分布的表示，其过程为：</p>
<ol type="1">
<li>将点云进行栅格化；</li>
<li>统计每个栅格的点 \(\mathbf{x} _ {i=1..n}\)；</li>
<li>计算每个栅格高斯分布的 Mean: \(\mathbf{q} = \frac{1}{n}\sum _ i\mathbf{x} _ i\);</li>
<li>计算 Covariance Matrix: \(\Sigma = \frac{1}{n}\sum _ i(\mathbf{x} _ i -\mathbf{q})(\mathbf{x} _ i-\mathbf{q})^t\)；</li>
</ol>
<p>　　由此，<strong>NDT 描述了栅格内每个位置出现点的概率</strong>，即 \(\mathbf{x}\) 有点的概率为： <span class="math display">\[ p(\mathbf{x}) \sim \mathrm{exp}\left(-\frac{(\mathbf{x-q})^t\sum ^ {-1}(\mathbf{x-q})}{2}\right) \tag{1}\]</span> 需要注意的是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 描述的是每个栅格有点的概率，NDT 描述的是每个栅格点云的概率分布。为了更准确的建模，采用重叠栅格化的设计以消除离散化的影响，以及限定 Covariance 矩阵的最小奇异值。</p>
<h3 id="目标函数构建">1.2. 目标函数构建</h3>
<p>　　考虑二维情况，需要优化的位姿参数为 \(\mathbf{p}=(t _ x, t _ y, \varphi)^t\)，第2个点云(待配准点云)中的点为 \(\mathbf{x} _ i\)，其变换到第1个点云坐标系后的表示为 \(\mathbf{x}' _ i\)，对应的第1个点云栅格的 NDT 表示为 \(\mathbf{\Sigma} _ i, \mathbf{q} _ i\)。由此可计算该变换位姿下，其度量分数为： <span class="math display">\[\mathrm{score}(\mathbf{p})=\sum _ i\mathrm{exp}\left(-\frac{(\mathbf{x}&#39; _ i-\mathbf{q} _ i)^t\sum _ i ^ {-1}(\mathbf{x}&#39; _ i-\mathbf{q} _ i)}{2}\right) \tag{2}\]</span> 最大化度量函数即可求解最优的位姿，优化过程一般都是最小化目标函数，所以设定目标函数为 \(-\mathrm{score}\)。</p>
<h3 id="newton-法优化迭代">1.3. Newton 法优化迭代</h3>
<p>　　设 \(\mathbf{q}=\mathbf{x}' _ i-\mathbf{q} _ i\)，那么目标函数为： <span class="math display">\[ s = -\mathrm{exp}\frac{-\mathbf{q^t\sum ^ {-1}q}}{2} \tag{3}\]</span> 每次迭代过程为： <span class="math display">\[\mathbf{p\gets p+\Delta p} \tag{4}\]</span> 而 \(\mathbf{\Delta p}\) 来自： <span class="math display">\[\mathbf{H\Delta p} = \mathbf{-g} \tag{5}\]</span> 其中 \(\mathbf{g}\) 是目标函数对优化参数的导数，\(\mathbf{H}\) 为目标函数的 Hessian 矩阵： <span class="math display">\[\left\{\begin{array}{l}
g _ i=\frac{\partial s}{\partial p _ i}\\
H _ {ij} = \frac{\partial s}{\partial p _ i\partial p _ j}
\end{array}\tag{6}\right.\]</span></p>
<h2 id="建图与定位">2. 建图与定位</h2>
<p>　　本文的建图是通过<strong>关键帧集合与关键帧之间的位姿变化实现的</strong>，定位的时候去找重合度最高的关键帧作点云配准。此外，当找不到重合度较高的关键帧时，可以实时更新当前帧作为关键帧添加到地图中，还可以对地图作进一步的全局，半全局优化。</p>
<h2 id="一些思考">3. 一些思考</h2>
<p>　　本文建图是关键帧的形式，更鲁棒的做法是将点云配准到一起，在世界坐标系下获得场景的稠密点云，然后再 NDT 化，这样能更准确的建模点云分布。<br>
　　<a href="/LOAM/" title="LOAM">LOAM</a> 维护的是栅格化的地图，每个栅格限制特征点的数量，所以本质上存储的是原始点云图(被选出是特征点的点云)。为了更好的描述栅格内的特征分布，可以对其作类似 NDT 近似，同时加入能描述该分布的特征，比如对于面特征，加入法向量。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Biber, Peter &amp; Straßer, Wolfgang. (2003). The Normal Distributions Transform: A New Approach to Laser Scan Matching. IEEE International Conference on Intelligent Robots and Systems. 3. 2743 - 2748 vol.3. 10.1109/IROS.2003.1249285.</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>SLAM</tag>
        <tag>Localization</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Probabilistic 3D Multi-Object Tracking for Autonomous Driving&quot;</title>
    <url>/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/</url>
    <content><![CDATA[<p>　　<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中已经较详细得阐述了 3D MOT 状态估计过程，文章末提到观测过程的协方差矩阵初始化问题可以用观测的不确定性解决，<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 就是通过贝叶斯深度神经网络来建模该不确定性。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提供了另一种简化的观测不确定性计算方法，同时估计运动模型与观测模型的不确定性，即过程噪声与测量噪声。</p>
<h2 id="kalman-filter">1. Kalman Filter</h2>
<p><img src="/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/framework.png" width="80%" height="80%" title="图 1. MOT Framework"> 　　如图 1. 所示，本文采用的卡尔曼滤波框架与传统的一样，分为预测与更新。预测阶段，根据上一时刻结果通过 Motion Model(Process Model) 预测当前时刻的状态(先验)；数据关联阶段，将预测的状态与观测的状态作目标数据关联，出 ID；更新阶段，融合预测与观测的状态，得到状态的后验估计。</p>
<h3 id="predict-step">1.1. Predict Step</h3>
<p>　　本文采用 CTRV(Constant Turn Rate and Velocity) 运动模型。不同与<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中描述的 CTRV，本文作了<strong>线性简化</strong>，其运动方程为： <span class="math display">\[\begin{align}
&amp;\begin{bmatrix}
\hat{x}\\
\hat{y}\\
\hat{z}\\
\hat{a}\\
\hat{l}\\
\hat{w}\\
\hat{h}\\
\hat{d} _ x\\
\hat{d} _ y\\
\hat{d} _ z\\
\hat{d} _ a\\
\end{bmatrix} _ {t+1}=
\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0\\
0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1\\
0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
z\\
a\\
l\\
w\\
h\\
d _ x\\
d _ y\\
d _ z\\
d _ a\\
\end{bmatrix} _ {t}  +
\begin{bmatrix}
q _ x\\
q _ y\\
q _ z\\
q _ a\\
0\\
0\\
0\\
q _ {d _ x}\\
q _ {d _ y}\\
q _ {d _ z}\\
q _ {d _ a}\\
\end{bmatrix} _ {t}\\
\Longleftrightarrow &amp; \\
&amp;\hat{\mu} _ {t+1} = \mathbf{A}\mu _ t \\
\end{align}\tag{1}
\]</span> 其中未知的线加速度与角加速度 \((q _ x, q _ y, q _ z, q _ a)\)，\((q _ {d _ x},q _ {d _ y},q _ {d _ z},q _ {d _ a})\) 符合\((0,\mathbf{Q})\)高斯分布。<br>
　　根据 Motion Model，卡尔曼的预测过程计算状态量的先验： <span class="math display">\[\begin{align}
\hat{\mu} _ {t+1} &amp;= \mathbf{A}\mu _ t \\
\hat{\Sigma} _ {t+1} &amp;= \mathbf{A}\Sigma _ t\mathbf{A}^T + \mathbf{Q}\\
\end{align}\tag{2}\]</span> 　　观测模型为每一时刻检测的结果，包括位置，朝向，目标框尺寸，即观测矩阵 \(\mathbf{H} _ {7\times 11} = [\mathbf{I}, \mathbf{0}]\)。观测噪声也符合高斯分布，由此得到预测的观测量： <span class="math display">\[\begin{align}
\hat{o} _ {t+1} &amp;= \mathbf{H}\hat{\mu} _ {t+1} \\
\mathbf{S} _ {t+1} &amp;= \mathbf{H}\hat{\Sigma} _ {t+1}\mathbf{H}^T + \mathbf{R}\\
\end{align}\tag{3}\]</span></p>
<h3 id="update-step">1.2. Update Step</h3>
<p>　　首先将预测的观测量与实际的观测量作数据关联。基本思想是将预测目标与观测目标作 Cost Matrix，然后用匈牙利/贪心算法求解最优匹配对。本文采用 Mahalanobis distance： <span class="math display">\[ m = \sqrt{(o _ {t+1}- \mathbf{H}\hat{\mu} _ {t+1})^T\mathbf{S} _ {t+1} ^{-1}(o _ {t+1}-\mathbf{H}\hat{\mu} _ {t+1})} \tag{4}\]</span> 需要注意的是，计算距离前先做角度矫正，如果两个目标框角度相差大于 90 度，那么作 180 度旋转。<br>
　　得到预测与观测的匹配对后，计算后验概率更新该目标的状态： <span class="math display">\[\begin{align}
\mathbf{K} _ {t+1} &amp;= \hat{\Sigma} _ {t+1}\mathbf{H} ^T\mathbf{S} _ {t+1}^{-1}\\
\mu _ {t+1} &amp;= \hat{\mu} _ {t+1} + \mathbf{K} _ {t+1}(o _ {t+1}-\mathbf{H}\hat{\mu} _ {t+1})\\
\Sigma _ {t+1} &amp;=(\mathbf{I}-\mathbf{K} _ {t+1}\mathbf{H})\hat{\Sigma} _ {t+1}\\
\end{align}\tag{5}\]</span> 　　以上卡尔曼过程与<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>，以及<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>完全一致。</p>
<h2 id="covariance-matrices-estimation">2. Covariance Matrices Estimation</h2>
<p>　　如何确定卡尔曼滤波过程中的 \(\Sigma _ 0, \mathbf{Q, R}\)？传统方法是直接用一个确定的经验矩阵赋值；理想的是用<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 建模处理，但是会相对较复杂；本文用更简单的基于统计方法来确定协方差矩阵。<br>
　　<strong>观测量的方差(不确定性)与目标的属性有关</strong>，如距离，遮挡，类别等。本文没有区分这些属性，只统计了一种观测量的方差，<strong>更好的处理方式是按照不同属性，统计不同的方差</strong>。而 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 是 Instance 级别的方差预测。<strong>这种统计出来的方差虽然细粒度差一点，但是非常合理，因为只要模型训练好后，模型预测的分布是与训练集分布相似的(理想情况)</strong>，所以用训练集的方差来直接代替模型预测的方差也较为合理。<br>
　　<span style="color:red"><strong>更准确的来说，不确定性与物体的属性以及标注误差有关，这里只统计了标注误差(标注误差在大多数情况下都是同分布的)，而实际上遮挡大的目标，是更难学习的(目标学习有难易之分，即预测分布与训练集分布会有偏差)，即预测结果会有额外量的不确定性，所以这种离线统计方法也有很大的局限性</strong>。</span><br>
　　设训练集的真值标签：\(\left\{\left\{x _ t^m, y _ t^m, z _ t^m, a _ t^m\right\} _ {m=1}^M\right\} _ {t = 1}^T\)。</p>
<h2 id="motionprocess-noise-model">2.1. Motion/Process Noise Model</h2>
<p>　　假设各状态量的噪声独立同分布，那么对于位置与朝向噪声，有： <span class="math display">\[\begin{align}
Q _ {xx} &amp;= \mathbf{Var}\left(\left(x _ {t+1}^m-x _ t^m\right)-\left(x _ t^m-x _ {t-1}^m\right)\right)\\
Q _ {yy} &amp;= \mathbf{Var}\left(\left(y _ {t+1}^m-y _ t^m\right)-\left(y _ t^m-y _ {t-1}^m\right)\right)\\
Q _ {zz} &amp;= \mathbf{Var}\left(\left(z _ {t+1}^m-z _ t^m\right)-\left(z _ t^m-z _ {t-1}^m\right)\right)\\
Q _ {aa} &amp;= \mathbf{Var}\left(\left(a _ {t+1}^m-a _ t^m\right)-\left(a _ t^m-a _ {t-1}^m\right)\right)\\
\end{align}\tag{6}\]</span> 　　对于线速度与角速度，因为： <span class="math display">\[\begin{align}
q _ {x _ t} &amp;\approx x _ {x+1} - x _ t - d _ {x _ t}\\
&amp; \approx (x _ {t+1}-x _ t) - (x _ t-x _ {t-1})\\
q _ {d _ {x _ t}} &amp;\approx d _ {x _ {t+1}} - d _ {x _ t}\\
&amp; \approx (x _ {t+1}-x _ t) - (x _ t-x _ {t-1})\\
\end{align}\tag{7}\]</span> 所以： <span class="math display">\[ (Q _ {d _ xd _ x}, Q _ {d _ yd _ y}, Q _ {d _ zd _ z}, Q _ {d _ ad _ a}) = (Q _ {xx}, Q _ {yy}, Q _ {zz}, Q _ {aa})\tag{8}\]</span></p>
<h2 id="observation-noise-model">2.2. Observation Noise Model</h2>
<p>　　在训练集上，找到检测与真值的匹配对 \(\left\{\left\{(D _ t^k, G _ t^k)\right\} _ {k=1}^K\right\} _ {t=1}^T\)，从而计算观测噪声： <span class="math display">\[\begin{align}
&amp;R _ {xx} = \mathbf{Var}\left(D _ {x _ t}^k-G _ {x _ t}^k\right)\\
&amp;R _ {yy} = \mathbf{Var}\left(D _ {y _ t}^k-G _ {y _ t}^k\right)\\
&amp;R _ {zz} = \mathbf{Var}\left(D _ {z _ t}^k-G _ {z _ t}^k\right)\\
&amp;R _ {aa} = \mathbf{Var}\left(D _ {a _ t}^k-G _ {a _ t}^k\right)\\
&amp;R _ {ll} = \mathbf{Var}\left(D _ {l _ t}^k-G _ {l _ t}^k\right)\\
&amp;R _ {ww} = \mathbf{Var}\left(D _ {w _ t}^k-G _ {w _ t}^k\right)\\
&amp;R _ {hh} = \mathbf{Var}\left(D _ {h _ t}^k-G _ {h _ t}^k\right)\\
\end{align}\tag{8}\]</span> 初始的状态协方差 \(\Sigma _ 0 = \mathbf{R}\)。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Chiu, Hsu-kuang, et al. "Probabilistic 3D Multi-Object Tracking for Autonomous Driving." arXiv preprint arXiv:2001.05673 (2020).</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
        <tag>Uncertainty</tag>
        <tag>MOT</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;LaserNet&quot;</title>
    <url>/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/</url>
    <content><![CDATA[<p>　　3D 目标检测中，目标定位的不确定性也很关键，<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中已经较为详细的描述了在 Bayesian Deep Networks 中如何建模异方差偶然不确定性(Aleatoric Uncertainty)。在贝叶斯深度神经网络框架下，网络不仅预测目标的位置(Mean)，还预测出该预测位置的方差(Variance)。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 延续了 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中预测 Corner 点位置方差的思路，提出了一种预测目标位置方差的方法。</p>
<h2 id="算法框架">1. 算法框架</h2>
<p><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/framework.png" width="90%" height="90%" title="图 1. LaserNet Framework"> 　　如图 1. 所示，输入为激光点云的 Sensor Range View 表示方式，输出为点级别的目标框3D属性，框顶点位置方差，以及类别概率。最后在 Bird View 下作目标框的聚类与 NMS 等后处理。</p>
<h3 id="点云输入方式">1.1. 点云输入方式</h3>
<p>　　不同于目前主流的 Bird View 点云栅格化方式，本文将点云直接根据线束在 Sensor Range View 下进行表示，高为激光线数量，宽为 HFOV 除以角度分辨率。设计 5 个 channel：距离，高度，角度，反射值，以及是否有点的标志位。<br>
　　本文认为这种点云表示方式的优点被忽视了，该视角下，点云的表达是紧促的，而且能高效得取得局部区域点，此外，能保留点云获取方式的信息。另一方面，该表达方式的缺点有，访问局部区域时，并不是空间一致的；以及需要处理物体的不同形状和遮挡问题。本文实验结果是，在 Kitti 上效果不如 Bird View 方法，但是在一个较大数据集上，能克服这些缺点。</p>
<h3 id="网络输出">1.2. 网络输出</h3>
<p>　　网络输出为点级别的预测，由三部分组成：</p>
<ol type="1">
<li><strong>类别概率</strong><br>
每个类别的概率；</li>
<li><strong>3D 框属性</strong><br>
包括相对中心距离 \((d _ x, d _ y)\)；相对朝向 \((\omega _ x, \omega _ y)=(\mathrm{cos}\omega, \mathrm{sin}\omega)\)；以及尺寸 \((l,w)\)。最终目标框中心点位置及朝向表示为： <span class="math display">\[\left\{\begin{array}{l}
\mathbf{b} _ c = [x,y]^T+\mathbf{R} _ \theta [d _ x,d _ y]^T \\
\varphi = \theta + \mathrm{atan2}(\omega _ y,\omega _ x)
\end{array}\tag{1}\right.\]</span> 其中 \(\theta\) 为该点的雷达扫描角度。由此可得到四个目标框角点坐标： <span class="math display">\[\left\{\begin{array}{l}
\mathbf{b} _ 1 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [l,w]^T\\
\mathbf{b} _ 2 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [l,-w]^T\\
\mathbf{b} _ 3 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [-l,-w]^T\\
\mathbf{b} _ 4 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [-l,w]^T
\end{array}\tag{2}\right.\]</span></li>
<li><strong>顶点位置方差</strong><br>
当观测不完全时(遮挡，远处)，目标框的概率分布是多模态的，所以如 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中所述，输出为混合高斯模型。对于每个点的每个类别，输出 \(K\) 个目标框属性：\(\{d _ {x,k}, d _ {y,k}, \omega _ {x,k}, \omega _ {y,k}, l _ k, w _ k\} _ {k=1}^K\)；对应的方差 \(\{s _ k\} _ {k=1}^K\)；以及模型权重 \(\{\alpha _ k\} _ {k=1}^K\)。</li>
</ol>
<h3 id="bird-view-后处理">1.3. Bird View 后处理</h3>
<p>　　网络其实就做了一个点级别的分割，接下来需要作聚类以得到目标框。本文采用 Mean-Shift 方法作聚类。由于是点级别的概率分布，得到目标点集后，需要用 BCN(详见 <a href="/MOT-Fusion/" title="MOT-Fusion">MOT-Fusion</a>) 转换为目标级别的概率分布： <span class="math display">\[\left\{\begin{array}{l}
\hat{\mathbf{b}} _ i = \frac{\sum _ {j\in S _ i} w _ j\mathbf{b} _ j}{\sum _ {j\in S _ i}w _ j}\\
\hat{\sigma} _ i^2 = \left(\sum _ {j\in S _ i}\frac{1}{\sigma ^2 _ j}\right)^{-1}
\end{array}\tag{3}\right.\]</span> 其中 \(w=\frac{1}{\sigma ^ 2}\)。</p>
<h2 id="loss-形式">2. Loss 形式</h2>
<p>　　分类采用 Focal Loss。对于每个点 3D 属性的回归，首先找到最靠近真值的预测模型： <span class="math display">\[k ^ * = \mathrm{arg}\min \limits _ k\Vert\hat{\mathbf{b}} _ k-\mathbf{b} ^{gt}\Vert\tag{4}\]</span> 对该预测模型作 Loss： <span class="math display">\[\mathcal{L} _ {box}=\sum _ n\frac{1}{\hat{\sigma} _ {k ^ * }} \left\vert\hat{\mathbf{b}} _ {n,k^ * }-\mathbf{b} _ n^{gt}\right\vert + \mathrm{log}\hat{\sigma} _ {k ^ * }\tag{5}\]</span> 实际回归的是 \(s:=\mathrm{log} \sigma\)。然后对混合模型的权重 \(\{\alpha _ k\} _ {k=1}^K\) 作 cross entry loss \(\mathcal{L} _ {mix}\)。最终的回归 Loss 为： <span class="math display">\[\mathcal{L} _ {reg} = \frac{1}{N}\sum _ i \frac{\mathcal{L} _ {box, i} + \lambda \mathcal{L} _ {mix,i}}{n _ i} \tag{6}\]</span></p>
<h2 id="adaptive-nms">3. Adaptive NMS</h2>
<p>　　类别概率不能反应目标框的质量，所以本文采用预测的目标框方差作为 NMS 的参考量。将目标框方差转换为目标框的质量分数：\(\alpha _ k/2\hat{\sigma} _ k\)。<br>
<img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/nms.png" width="50%" height="50%" title="图 2. Adaptive NMS"> 　　此外不同目标在 Bird-View 下 IoU 最大值有一定的限制，如图 2. 所示，最坏的情况，Bird-View 下两个框的 IoU 最大限制为设计为： <span class="math display">\[t=\left\{\begin{array}{l}
\frac{\sigma _ 1+\sigma _ 2}{2w-\sigma _ 1 - \sigma _ 2} &amp; \sigma _ 1+\sigma _ 2 &lt; w\\
1 &amp; otherwise
\end{array}\tag{7}\right.\]</span> 当两个目标框的 IoU 大于阈值时，可能的情况是：1. 目标框错误，则删除低分数的目标框；2. 方差估计错误，那么增大方差使最大阈值满足 IoU 条件。</p>
<h2 id="预测分布的分析">4. 预测分布的分析</h2>
<p><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/calibration.png" width="80%" height="80%" title="图 3. Uncertainty(Variance) Calibration"> 　　评价 Variance(Uncertainty) 预测的好坏，可以画 Calibration 图。如图 3. 所示，横坐标为预测的 Mean 与真值形成的高斯概率分布下的 CDF，而纵坐标为预测的 Variance 统计出的高斯分布下的 CDF。理想情况下，两者是 \(y=x\) 的关系，如图所示，在 ATG4D 大数据集上，预测的 Variance 效果更好。</p>
<h2 id="一些思考">5. 一些思考</h2>
<p>　　不管是 2D 检测还是 3D 检测，这种先(语义)分割后聚类出目标的思想，有很强的优势：召回率高，超参数少，自带分割信息等。本文又应用 Aleatoric Uncertainty 来建模检测的不确定性--位置方差(不确定性干嘛用，怎么用，不多说了)，有很好的借鉴意义。</p>
<h2 id="reference">6. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Meyer, Gregory P., et al. "Lasernet: An efficient probabilistic 3d object detector for autonomous driving." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Learning to See in the Dark&quot;</title>
    <url>/paper-reading-Learning-to-See-in-the-Dark/</url>
    <content><![CDATA[<p>　　无监督低光照图像增强更有应用价值，<a href="/Unsupervised-Low-Light-Image-Enhancement/" title="Unsupervised Low Light Image Enhancement">Unsupervised Low Light Image Enhancement</a> 中介绍了几种无监督方法。本文则是有监督方法，但是值得一读。在 Sensor，曝光时间，光圈，ISO 等(在线调节通过 AE 完成)确定后，图像低光照下曝光不足主要是因为 ISP 过程对图像的亮度矫正不理想。本文直接重构 ISP 过程，对 Raw 图像进行一系列操作，以增强亮度。</p>
<h2 id="算法过程">1. 算法过程</h2>
<p><img src="/paper-reading-Learning-to-See-in-the-Dark/ISP.png" width="90%" height="90%" title="图 1. Raw Image Processing Pipeline 对比"> 　　如图 1. 所示，传统 ISP 过程包括：White Balance, Demosaic, Denoise/Sharpen, Color Space Conversion, Gamma Correction(与亮度变化相关)等。L3 与 Burst 是其它 ISP pipeline 学习的方法，本文网络算法过程如图 1.b 所示，首先提取 RGB sensor 值并放大一定比例(该放大系数用来控制最终增强的曝光级别)，然后经过网络层，最终输出全尺寸的 RGB 图像。<br>
　　训练数据采集自室内静态场景，每对数据由短曝光的低光照图像与长曝光的标签图像构成，由此可进行有监督训练。</p>
<h2 id="reference">2. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Chen, Chen, et al. "Learning to see in the dark." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</p>
]]></content>
      <categories>
        <category>Low-Light Image Enhancement</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Low-Light Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title>Unsupervised Low-Light Image Enhancement</title>
    <url>/Unsupervised-Low-Light-Image-Enhancement/</url>
    <content><![CDATA[<p>　　在自动驾驶中，相机能捕捉丰富的纹理信息，是不可或缺的传感器。但是受限于相机 Sensor 及 ISP 性能，其动态范围有限，往往会出现过曝或欠曝的情况。过曝的情况还能通过 3A(AE, AF, AW) 中的 AE 调节，而欠曝的情况，AE 中要么提高增益或 ISO 但是会增加噪声，要么增加曝光时间但是撑死 50ms(按照 20Hz)，光圈则一般是固定的，不会调节。所以在低光照自动驾驶场景下，对欠曝的图像进行亮度增强则显得尤其重要（当然也可用夜视相机如红外相机等辅助)。<br>
　　基于学习的图像增强方法，由于很难获得大量的欠爆图像与对应的增强图像。所以无监督的图像增强方法就更有应用价值，本文介绍几种无监督图像增强方法。</p>
<h2 id="zero-dce1">1. Zero-DCE<a href="#1" id="1ref"><sup>[1]</sup></a></h2>
<p>　　无监督图像增强方法主要是指基于 GAN 的方法，基于 GAN 的方法还是需要选择欠爆图像及正常图像两个分布的数据集，选择不当也会导致性能下降。而 Zero-DCE 则无需选择正常图像数据集，消除了数据分布下过拟合或欠拟合的风险。<br>
　　Zero-DCE 基本思想是对每个像素作亮度变换，每个像素的变换方程为： <span class="math display">\[LE(I(\mathrm{x});\alpha) = I(\mathrm{x}) + \alpha I(\mathrm{x})(1-I(\mathrm{x})) \tag{1}\]</span> 其中 \(\alpha\in[-1,1]\) 是变换系数。对图像的每个通道每个像素分别作不同系数的迭代变换，可得： <span class="math display">\[LE _ n(\mathrm{x}) = LE _ {n-1}(\mathrm{x}) + \mathcal{A} _ n LE _ {n-1}(\mathrm{x})(1-LE _ {n-1}(\mathrm{x})) \tag{2}\]</span> 其中 \(\mathcal{A} _ n\) 是变换系数集，与图像大小一致。 <img src="/Unsupervised-Low-Light-Image-Enhancement/Zero-DCE.png" width="90%" height="90%" title="图 1. Zero-DCE Framework"> 　　如图 1. 所示，Zero-DCE 框架中，一个基本网络预测几组 \(\mathcal{A} _ n\) 集合，然后对原图每个通道进行迭代的亮度变换。LE-curves 不仅能增强暗处的曝光量，还能减弱过曝处的亮度值。<br>
　　该方法最重要的是 Loss 函数的设计，一共有以下 Loss 组成：</p>
<ol type="1">
<li><strong>Spatial Consisiency Loss</strong><br>
增强后的图像要求其与原图具有空间一致性： <span class="math display">\[ L _ {spa} = \frac{1}{K}\sum _ {i=1}^K\sum _ {j\in\Omega (i)}\left(\Vert Y _ i-Y _ j\Vert-\Vert I _ i-I _ j\Vert\right)^2 \tag{3}\]</span> 其中 \(\Omega\) 为某像素的领域集，可为四领域；\(K\) 为局部区域数量，可设定为 \(4\times 4\) 大小；\(Y,I\) 分别为增强后与原始的像素亮度值。</li>
<li><strong>Exposure Control Loss</strong><br>
曝光控制 Loss 相当于设定曝光量去监督训练每个像素亮度，实现“无监督”的效果： <span class="math display">\[ L _ {exp} = \frac{1}{M}\sum _ {k=1}^M\Vert Y _ k-E\Vert \tag{4}\]</span> 其中 \(M\) 为无重合的局部区域数量，可设定为 \(16\times 16\) 大小；\(Y _ k\) 为局部区域的平均亮度值。作者实验中，设定 \(E\in[0.4,0.7]\) 均能获得相似的较好的结果。</li>
<li><strong>Color Constancy Loss</strong><br>
根据 Gray-World color constancy 假设：rgb 每个通道的平均亮度值与 gray 灰度值一致。所以为了保证颜色不失真，构造： <span class="math display">\[ L _ {col}=\sum _ {\forall (p,q)\in \epsilon}(J^p-J^q), \epsilon=\{R,G,B\} \tag{5}\]</span> 其中 \(p,q\) 表示一对不同的颜色通道，\(J\) 表示该通道的平均亮度值。</li>
<li><strong>Illumination Smoothness Loss</strong><br>
增强的过程要求相邻亮度值是平滑的，对增强变换系数作约束： <span class="math display">\[ L _ {tv _ {\mathcal{A}}} = \frac{1}{N}\sum _ {n=1}^N\sum _ {c\in\epsilon}(\nabla _ x\mathcal{A} _ n^c+\nabla _ y\mathcal{A} _ n^c)^2, \epsilon = \{R,G,B\}\tag{6}\]</span> 其中 \(N\) 为增强迭代数；\(\nabla _ x,\nabla _ y\) 分别表示水平与垂直方向的求导操作。</li>
</ol>
<p>最终 Loss 构成为： <span class="math display">\[ L _ {total} = L _ {spa} + L _ {exp} + W _ {col}L _ {col} + W _ {tv _ {\mathcal{A}}}L _ {tv _ {\mathcal{A}}} \tag{7}\]</span></p>
<h2 id="enlightengan2">2. EnlightenGAN<a href="#2" id="2ref"><sup>[2]</sup></a></h2>
<p>　　图像增强本质上是作 domain transfer，所以能用 GAN 处理，实现无监督训练。 <img src="/Unsupervised-Low-Light-Image-Enhancement/EnlightenGAN.png" width="90%" height="90%" title="图 2. EnlightenGAN Framework"> 　　如图 2. 所示，EnlightenGAN 由 Generator 和 Discriminator 构成。Generator 是一个 attention-guided U-Net，因为我们期望欠曝的区域能增强，所以将亮度值归一化后，用 1 减去亮度值作为注意力图，与原图一起输入网络。Discriminator 由 Global Discriminator 与 Local Discriminator 组成，因为经常只需要局部区域的亮度，所以设计 Local Discriminator 就很有必要。<br>
　　Loss 的设计非常关键，EnlightenGAN 一共有以下 Loss 组成：</p>
<ol type="1">
<li><strong>Adversarial Loss</strong><br>
用于直接训练 Generator 以及 Discriminator 的 Loss，与传统的 GAN Loss 类似；</li>
<li><strong>Self Feature Preserving Loss</strong><br>
注意到，调整输入图像值的范围，对最终的高层任务影响不是很大，所以引入网络特征 Loss 来保证增强后图像的准确性。对原始图像与生成的图像，分别输入到在 ImageNet 上预训练的 VGG-16 模型，提取特征集合，将对应的特征对作 L1 Loss。</li>
</ol>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Guo, Chunle, et al. "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement." arXiv preprint arXiv:2001.06826 (2020).<br>
<a id="2" href="#2ref">[2]</a> Jiang, Yifan, et al. "Enlightengan: Deep light enhancement without paired supervision." arXiv preprint arXiv:1906.06972 (2019).</p>
]]></content>
      <categories>
        <category>Low-Light Image Enhancement</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Low-Light Image Enhancement</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN</title>
    <url>/GAN/</url>
    <content><![CDATA[<p>　　Generative Adversarial Nets(GAN) 能将某个分布的数据映射到另一组数据形成的分布空间内。这在某些领域非常有用，如：图像去噪，图像去雨雾，图像去模糊，图像低光照增强等。<strong>自动驾驶中，图像去雨雾与低光照增强非常关键，GAN 能在没有模拟器的情况下，根据有限的数据，自动生成某一分布的数据，为后续感知做准备</strong>。目前还没看到针对点云的 GAN，未来 3D GAN 可能会有大进展。<br>
　　本文介绍几个 GAN 的基础性工作。</p>
<h2 id="gan-基础网络">1. GAN 基础网络</h2>
<h3 id="generative-adversarial-nets1">1.1. Generative Adversarial Nets<a href="#1" id="1ref"><sup>[1]</sup></a></h3>
<p>　　对抗网络由生成模型和判别模型构成。生成模型输入随机噪声，输出以假乱真的图像，判别模型则对图像作分类。其优化函数为： <span class="math display">\[ \min\limits _ G \max\limits _ D V(D,G) = E _ {x\sim p _ {data}(x)}[log(D(x))] + E _ {x\sim p _ z(z)}[log(1-D(G(z)))] \tag{1}\]</span> 该优化过程有两部分组成：</p>
<ol type="1">
<li><strong>优化判别模型</strong><br>
<span class="math display">\[ \max\limits _ D V(D,G) = E _ {x\sim p _ {data}(x)}[log(D(x))] + E _ {x\sim p _ z(z)}[log(1-D(G(z)))] \tag{2}\]</span> 其中第一项表示输入为真样本时，那么判别模型输出越大越好，即越接近 1；而对于已经生成的假样本 \(G(z)\)，判别模型输出越小越好，即接近 0。</li>
<li><strong>优化生成模型</strong><br>
<span class="math display">\[ \min\limits _ GV(D,G) =E _ {x\sim p _ z(z)}[log(1-D(G(z)))] \tag{3}\]</span> 优化生成模型时，希望生成的假样本接近真样本，所以生成的假样本经过判别模型后越大越好，即\(D(G(z))\)要接近 1。由此统一成上式。</li>
</ol>
<p>　　对抗网络的优化由这两步迭代组成。</p>
<h3 id="conditional-generative-adversarial-nets2">1.2. Conditional Generative Adversarial Nets<a href="#2" id="2ref"><sup>[2]</sup></a></h3>
<p>　　条件对抗网络中的生成模型输入不在是随机噪声，而是特定的数据分布，如真值标签。其优化函数为： <span class="math display">\[ \min\limits _ G \max\limits _ D V(D,G) = E _ {x\sim p _ {data}(x)}[log(D(x|y))] + E _ {x\sim p _ z(z)}[log(1-D(G(z|y)))] \tag{4}\]</span> 　　其优化过程与 GAN 类似。</p>
<h3 id="cycle-consistent-adversarial-nets3">1.3. Cycle-Consistent Adversarial Nets<a href="#3" id="3ref"><sup>[3]</sup></a></h3>
<p>　　Cycle GAN 使得高分辨率图像的 domain-transfer 成为可能。对于两个图像分布 \(X,Y\)，设计两个映射函数(生成模型): \(G:X\to Y\) 和 \(F:Y\to X\)；设计两个判别模型: \(D _ X\) 和 \(D _ Y\)，\(D _ X\) 用于判别 \(x\) 与 \(F(y)\), \(D _ Y\) 用于判别 \(y\) 与 \(G(x)\)。为了还原高分辨率图像，设计两部分 Loss：</p>
<ol type="1">
<li><strong>Adversarial Loss</strong><br>
就是传统的对抗网络 Loss: <span class="math display">\[\begin{align}
\mathcal{L} _ {GAN}&amp;=\mathcal{L} _ {GAN}(G, D _ Y,X,Y)+\mathcal{L} _ {GAN}(F, D _ X,Y,X)\\
&amp;= E _ {y\sim p _ {data}(y)}[log(D _ Y(y))] + E _ {x\sim p _ {data}(x)}[log(1-D _ Y(G(x)))]\\
&amp;+ E _ {x\sim p _ {data}(x)}[log(D _ X(x))] + E _ {y\sim p _ {data}(y)}[log(1-D _ X(F(Y)))]
\end{align} \tag{5}\]</span></li>
<li><strong>Cycle Consistency Loss</strong><br>
为了保证映射网络的映射准确性，考虑到 \(x\to G(x)\to F(G(x))\approx x \) 以及 \(y\to F(y)\to G(F(y))\approx y \)，设计 cycle loss： <span class="math display">\[\mathcal{L} _ {cyc}(G,F)= E _ {x\sim p _ {data}(x)}\Vert F(G(x))-x\Vert + E _ {y\sim p _ {data}(y)}\Vert G(F(y))-y\Vert \tag{6}\]</span></li>
</ol>
<p>总的 Loss 为： <span class="math display">\[\mathcal{L} _ (G,F,D _ X, D _ Y)=\mathcal{L} _ {GAN}(G, D _ Y,X,Y)+\mathcal{L} _ {GAN}(F, D _ X,Y,X)+\lambda \mathcal{L} _ {cyc}(G,F) \tag{7}\]</span></p>
<h2 id="其它资料">2. 其它资料</h2>
<p>　　上面介绍了三个 GAN 基本网络，尤其是 Cycle-GAN，是高分辨率图像无监督 domain-transfer 的基础，应用相当广泛。本文介绍相对较简单，<a href="#4" id="4ref">[4]</a> 详细介绍了 GAN 的来龙去脉。代码则可以参考 <a href="#5" id="5ref">[5]</a> ，收录的 GAN 网络非常详细。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014.<br>
<a id="2" href="#2ref">[2]</a> Mirza, Mehdi, and Simon Osindero. "Conditional generative adversarial nets." arXiv preprint arXiv:1411.1784 (2014).<br>
<a id="3" href="#3ref">[3]</a> Zhu, Jun-Yan, et al. "Unpaired image-to-image translation using cycle-consistent adversarial networks." Proceedings of the IEEE international conference on computer vision. 2017.<br>
<a id="4" href="#4ref">[4]</a> http://www.gwylab.com/note-gans.html<br>
<a id="5" href="#5ref">[5]</a> https://github.com/eriklindernoren/PyTorch-GAN</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Point-based 3D Detetection</title>
    <url>/Point-based-3D-Det/</url>
    <content><![CDATA[<p>　　基于激光点云的 3D 目标检测是自动驾驶系统中的核心感知模块。由于点云的稀疏性以及空间结构的无序性，一系列 Voxel-based 3D 检测方法得以发展：<a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a>，<a href="/paperreading-Fast-and-Furious/" title="FaF">FaF</a>，<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 等。然而 Voxel-based 方法需要预定义空间栅格的分辨率，其特征提取的有效性依赖于空间分辨率。同时在点云语义分割领域，对点云的点级别特征提取方法研究较为广泛，<a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a> 中已经较详细的介绍了针对点云的点级别特征提取方法，<a href="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/" title="Grid-GCN">Grid-GCN</a> 提出了几种策略来加速特征提取。<br>
　　由此高效的 Point-based 3D 检测方法成为可能，这种方法首先提取点级别的特征(相比 Voxel-based，理论上没有信息损失)，然后用点级别的 Anchor-based 或 Anchor-free 方法作 3D 检测。</p>
<h2 id="anchor-based">1. Anchor-based</h2>
<h3 id="ipod1">1.1. IPOD<a href="#1" id="1ref"><sup>[1]</sup></a></h3>
<p><img src="/Point-based-3D-Det/ipod.png" width="90%" height="90%" title="图 1. IPOD Framework"> 　　如图 1. 所示， IPOD 与 F-PointNet 类似，只不过 IPOD 在俯视图下生成 Proposal 取点，而 F-PointNet 是直接在锥形视野的点云中作分割。IPOD 由三部分组成：</p>
<ol type="1">
<li><strong>Semantic Segmentation</strong><br>
目的是将点云中的背景点过滤掉，只生成前景点的 Anchor。作者采用图像语义分割的方法，这里也可直接用点云分割来做；</li>
<li><strong>Point-based Proposal Generation</strong><br>
生成点级别的候选框，去掉冗余的候选框；</li>
<li><strong>Head for Classification and Regression</strong><br>
根据候选框，提取特征，作分类和回归；</li>
</ol>
<p>这里的前两步是要得到少量但又能保证召回率的 Proposal，其中 Anchor 是根据每个点来设置的，然后作 NMS 操作，这里不做展开。 <img src="/Point-based-3D-Det/proposal_feat.png" width="80%" height="80%" title="图 2. Proposal Feature Generation"> 　　如图 2. 所示，每个 Proposal 提取出点云信息，然后通过 PointNet++ 直接来预测该 Proposal 的 3D 属性。这里用到了 T-Net(Spatial Transformation Network 的一种) 将点云变换到规范坐标系(Canonical coordinates)，这个套路用的也比较多。其它细节就是正常的 3D 属性回归策略，不作展开。</p>
<h3 id="std2">1.2. STD<a href="#2" id="2ref"><sup>[2]</sup></a></h3>
<p><img src="/Point-based-3D-Det/STD.png" width="80%" height="80%" title="图 3. STD Framework"> 　　如图 3. 所示，STD 模块有：</p>
<ol type="1">
<li><strong>Backbone</strong><br>
用 PointNet++ 提取点级别特征以及作点级别的 Classification；</li>
<li><strong>PGM(Proposal Generation Module)</strong><br>
根据点级别的分类结果，对目标点设计球状 Spherical Anchor；不同类别设计不同的球状 Anchor 半径。将球状 Anchor 里面的点收集起来，作坐标规范化并且 concate 点级别特征，然后用 PointNet 来预测实际的矩形 proposal：包括中心 Offsets 以及 size offsets。同时对角度进行预测，角度预测通过分类加预测 Offsets 实现。</li>
<li><strong>Proposal Feature Generation</strong><br>
有了 proposal 后，其实可以直接通过 PointNet 作进一步的预测及分类，但是作者为了加速，这时候采用了 Voxel Feature Encoding。将 proposal 里面的点都转换到中心点坐标系，然后栅格化提取特征；</li>
<li><strong>Box Prediction</strong><br>
除了通常的类别预测以及 3D Box 相关属性的 Offsets 预测，作者还加入了与真值的 IoU 预测，该 IoU 值与类别分数相乘作为最终的该预测分数(这个在 2D Detection 中已经有应用)。</li>
</ol>
<h2 id="anchor-free">2. Anchor-free</h2>
<h3 id="pointrcnn3">2.1. PointRCNN<a href="#3" id="3ref"><sup>[3]</sup></a></h3>
<p><img src="/Point-based-3D-Det/PointRCNN.png" width="80%" height="80%" title="图 4. PointRCNN Framework"> 　　如图 4. 所示，PointRCNN 是一个 two-stage 3D 检测方法，类似 Faster-RCNN，其由 Bottom-up 3D Proposal Generation 和 Canonical 3D Box Refinement 两个模块组成。</p>
<h4 id="bottom-up-3d-proposal-generation">2.1.1 Bottom-up 3D Proposal Generation</h4>
<p>　　Proposal 的生成要求是，数量少，召回率高。3D Anchor 由于要覆盖 3D 空间，所以数量会很大(如 AVOD)，本文采用目标点生成 Proposal 的方法。与 IPOD，STD 类似，首先对点云进行点级别的特征提取并作前景分割(或语义分割)，对前景的每个点用 Bin-based 方法生成 3D proposal。由此在生成尽量少的 Proposal 下，保证目标的高召回率。<br>
　　点级别的特征提取及前景分割，可以采用任意的语义分割网络，这里前景的真值即为目标框内的点云，用 Focal Loss 来平衡正负样本。<br>
<img src="/Point-based-3D-Det/bin-based.png" width="60%" height="60%" title="图 5. Bin-based Localization"> 　　如图 5. 所示，对每个前景点用 Bin-based 方法生成 proposal。将平面的 \(x,z\) (与一般的雷达坐标系不同) 方向分成若干个 bin，然后对每个前景点，预测目标中心点属于哪个 bin，以及中心点与该 bin 的 Offsets(与角度处理的方式非常像)。针对尺寸，预测该类别平均尺寸的 Residual；针对角度，还是分解成分类加回归任务进行处理。最后再作 NMS 即可得到较少的 Proposal，给到下一模块作 refine。本模块的 Loss 设计为： <span class="math display">\[\begin{align}
\mathcal{L} _ 1 &amp;= \mathcal{L} _ {seg} + \mathcal{L} _ {proposal} \\
&amp;= \mathcal{L} _ {seg} + \frac{1}{N _ {pos}} \sum _ {p\in pos} \left(\mathcal{L} _ {bin} ^ {(p)} + \mathcal{L} _ {res} ^ {(p)}\right) \\
&amp;= \mathcal{L} _ {seg} + \sum _ {u\in{\{x,z,\theta\}}} \left(\mathcal{F} _ {cls}(\widehat{bin} _ u^{(p)}, bin _ u^{(p)})+\mathcal{F} _ {reg}(\widehat{res} _ u^{(p)}, res _ u^{(p)})\right) + \sum _ {v\in\{y,h,w,l\}} \mathcal{F} _ {reg}(\widehat{res} _ v^{(p)}, res _ v^{(p)})\\
\tag{1}
\end{align}\]</span> 其中 \(\mathcal{F} _ {cls}, \mathcal{F} _ {reg}\) 分别为 cross-entropy Loss 和 smooth L1 Loss。</p>
<h4 id="canonical-3d-box-refinement">2.1.2 Canonical 3D Box Refinement</h4>
<p>　　有了 3D proposal 后，经过 Point Cloud Region Pooling 提取该 proposal 的点特征，步骤如下：先对 proposal 进行一定程度的扩大，然后提取内部点的 semantic features，foreground mask score，Point distance等。由此获得每个 proposal 的点及点特征，用来作 3D Box Refinement。<br>
<img src="/Point-based-3D-Det/canonical.png" width="60%" height="60%" title="图 6. Canonical Transformation"> 　　如图 4. 所示，为了更好的学习 proposal 的局部空间特征，增加每个 proposal 在自身 Canonical 坐标系下的空间点。Canonical 变换如图 6. 所示，因为这里每个 proposal 的位置及角度已经有了，所以直接对其内的点作变换。如果没有，那就需要 STN(T-Net) 来学习这个变换。<br>
　　Loss 也是在 Canonical 坐标系下计算的，假设 proposal：\(\mathrm{b _ i} = (x _ i,y _ i,z _ i,h _ i,w _ i,l _ i,\theta _ i)\)，真值: \(\mathrm{b} _ i^{gt} = (x _ i^{gt}, y _ i^{gt},z _ i^{gt},h _ i^{gt},w _ i^{gt},l _ i^{gt},\theta _ i^{gt})\)。那么两者变换到 Canonical 坐标系后： <span class="math display">\[\begin{align}
\mathrm{\tilde{b}} _ i &amp;=(0,0,0,h _ i,w _ i,l _ i,0) \\
\mathrm{\tilde{b}} _ i^{gt} &amp;= (x _ i^{gt}-x _ i, y _ i^{gt}-y _ i,z _ i^{gt}-z _ i,h _ i^{gt},w _ i^{gt},l _ i^{gt},\theta _ i^{gt}-\theta _ i)
\tag{2}
\end{align}\]</span> 对于中心点，还是 bin 分类加 Residual 回归，但是可以减少 bin 的尺度；对于尺寸，还是回归 Residual；对于角度，由于限定 positive 与 gt 的 IoU&gt;0.55，所以可以将回归的角度限定为 \((-\frac{\pi}{4},\frac{\pi}{4})\) 的范围，由此进行 bin 分类及 Residual 回归。最终本阶段的 Loss 为： <span class="math display">\[ \mathcal{L} _ 2= \frac{1}{N _ {pos}+ N _ {neg}} \sum _ {p\in all} \mathcal{L} _ {label} ^{(p)}+ \frac{1}{N _ {pos}} \sum _ {p\in pos} \left(\mathcal{\tilde{L}} _ {bin} ^ {(p)} + \mathcal{\tilde{L}} _ {res} ^ {(p)}\right) \tag{3}\]</span></p>
<h3 id="dssd4">2.2. 3DSSD<a href="#4" id="4ref"><sup>[4]</sup></a></h3>
<p><img src="/Point-based-3D-Det/3DSSD.png" width="100%" height="100%" title="图 7. 3DSSD Framework"> 　　如图 7. 所示，3DSSD 是 one-stage 网络，由 Backbone，Candidate Generation Layer，Head 构成。Backbone 作者提出了 Fusion Sampling 以提升前景点在采样时候的召回率。Candidate Generation Layer 中根据前景点，生成 3D box 预测的 Candidate 锚点。最后 Head 根据锚点，作 Anchor-free 的 3D Box 预测。</p>
<h4 id="fusion-sampling">2.2.1 Fusion Sampling</h4>
<p>　　为了扩大感受野提取局部特征，点云通常需要作下采样处理，一般采用 D-FPS 方法(点空间距离作为采样度量)，但是这样会使前景点大量丢失。前面几种方法不管是用图像分割还是点云分割，都会去除背景点云，保留前景点云以提高生成 Proposal 的召回率。<br>
　　这里作者提出了 Feature-FPS，加入特征间的距离作为采样的度量方式。对于地面等背景，其特征基本类似，所以很容易就去除了；而对于目标区域，其点特征都不太一样，又得以保留。如果只保留同一目标的点，也会产生冗余，所以融合点特征距离及空间距离，设计采样度量方式为： <span class="math display">\[ C(A,B) = \lambda L _ d(A,B) + L _ f(A,B) \tag{4}\]</span> 　　因为 F-FPS 去除了大量的背景点，虽然有利于回归，但是不利于分类，所以设计了融合 D-FPS 和 F-FPS 的 Fusion Sampling 方法。如图 7. 所示，最终分别输出 F-FPS 与 D-FPS 的特征点。</p>
<h4 id="candidate-generation-layer">2.2.2 Candidate Generation Layer</h4>
<p><img src="/Point-based-3D-Det/candidate_pts.png" width="60%" height="60%" title="图 8. Candidate Generation"> 　　如图 8. 所示，根据 F-FPS 采样的点，在真值框中心点的监督下，用一个 T-Net 去学习采样点与中心点的变换。变换后的点即作为 Candidate 锚点。对每个 Candidate 点提取周围一定距离的 F-FPS 与 D-FPS(大量背景点利于分类)中点集的特征(空间坐标作归一化或变换到 Candidate 坐标系，类似 Canonical 坐标系)，然后作 MaxPool 提取该 Candidate 对应区域的特征。</p>
<h4 id="prediction-head">2.2.3 Prediction Head</h4>
<p>　　对于每个 Candidate 特征，作 3D Box 属性的回归。本文采用 Anchor-free 的方法。对于中心点，直接回归 Candidate 坐标点与真值框中心点的 Offsets；对于尺寸，直接回归与该类别平均尺寸的 Residual；对于角度，还是采用 bin 分类加 Residual 回归的策略。<br>
　　这里期望的是 Candidate 点能接近目标框中心点，所以作者借鉴 FCOS(详见 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>)中的 Center-ness Loss 来选取靠近中心点的 Candidate，真值 Label 为: <span class="math display">\[l _ {ctrness}=\sqrt[3]{\frac{\mathrm{min}(f,b)}{\mathrm{max}(f,b)}+\frac{\mathrm{min}(l,r)}{\mathrm{max}(l,r)}+\frac{\mathrm{min}(t,d)}{\mathrm{max}(t,d)}} \tag{5}\]</span> 其中 \(f,b,l,r,t,d\) 分别表示前后左右上下与中心点的距离。FCOS 中，加了一个与分类平行的分支来预测 Center-ness，最终的预测 Score 是分类 Score 乘以 Center-ness 得到(与预测 IoU 套路一样，本质上都是引入与真值的距离度量)，该预测 Score 用于之后的 NMS 等处理。本文则没有显示的预测 Center-ness，其直接将真值 Center-ness 与真值类别相乘，作为类别真值，所以一个类别分支即得到最终的预测 Score。<br>
　　最终的 Loss 为： <span class="math display">\[L = \frac{1}{N _ c}\sum _ iL _ c(s _ i, u _ i) + \lambda _ 1\frac{1}{N _ p}\sum _ i[u _ i&gt;0]L _ r + \lambda _ 2\frac{1}{N _ p}L _ s \tag{5}\]</span> 其中 \(s _ i\) 为预测的类别 Score，\(u _ i\) 为经过 Center-ness 处理后的类别真值；\(L _ c\) 表示类别预测 Loss；\(L _ r\) 表示 3D Box Loss，包括中心点距离，尺寸，角度，8个角点位置；\(L _ s\) 表示生成 Candidate 点的 shift 变换 Loss。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Yang, Zetong, et al. "Ipod: Intensive point-based object detector for point cloud." arXiv preprint arXiv:1812.05276 (2018).<br>
<a id="2" href="#2ref">[2]</a> Yang, Zetong, et al. "Std: Sparse-to-dense 3d object detector for point cloud." Proceedings of the IEEE International Conference on Computer Vision. 2019.<br>
<a id="3" href="#3ref">[3]</a> Shi, Shaoshuai, Xiaogang Wang, and Hongsheng Li. "Pointrcnn: 3d object proposal generation and detection from point cloud." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="4" href="#4ref">[4]</a> Yang, Zetong, et al. "3DSSD: Point-based 3D Single Stage Object Detector." arXiv preprint arXiv:2002.10187 (2020).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>MOT Multimodal Fusion</title>
    <url>/MOT-Fusion/</url>
    <content><![CDATA[<p>　　同一传感器的目标状态估计在<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中已经有较详细的介绍。不同传感器在不同光照不同天气情况下，有不同的表现，比如相机在低光照下可靠性较差，而激光雷达能弥补这个缺陷。所以在目标状态估计中，多传感器融合非常重要，可以是<strong>数据前融合，特征级融合，目标状态后融合</strong>。本文关注目标状态后融合过程。</p>
<h2 id="问题描述">1. 问题描述</h2>
<p>　　考虑两个传感器 \(A,B\) (传感器可为相机，激光雷达，毫米波雷达等)检测输出的(也可以是经过滤波的)多目标分别为：\(A=\{A _ i\in\mathbb{R}^D|i=1,...,M\}\)，\(B=\{B _ i\in\mathbb{R}^D|i=1,...,N\}\)，其中 \(\mathbb{R}^D\) 表示目标状态的维数，如位置，速度，朝向，类别等。MOT 的多模态后融合问题即由此求解融合后结果 \(C=\{C _ i\in\mathbb{R}^D|i=1,...,L\}\)，该过程主要有三步：</p>
<ol type="1">
<li>目标匹配/数据关联：从 \(A,B\) 中找出同一目标的两个多模态观测量，设匹配数为 \(K\)；</li>
<li>目标状态的多模态融合：对匹配上的同一目标的两个多模态观测进行融合；</li>
<li>整合目标，经过滤波输出最终结果，目标数目为 \(L=M+N-K\)；</li>
</ol>
<h2 id="目标匹配">2. 目标匹配</h2>
<p>　　本质上与单传感器下目标状态估计中前后帧的数据关联问题一致，这里的关键步骤是：</p>
<ol type="1">
<li>提取每个目标的特征向量：可以是位置，速度，角度，CNN特征层等；</li>
<li>构建 cost function：对两个目标集合建立 Cost 矩阵；</li>
<li>匈牙利算法找出最优匹配；</li>
</ol>
<p>　　传统的 cost function 基本是向量的 Euclidean 距离或是 cosine 距离，<a href="#1" id="1ref">[1]</a> 提出了一种 Deep Affinity Network 来一次性解决两个目标集合的匹配问题。 <img src="/MOT-Fusion/affinity.png" width="70%" height="70%" title="图 1. Affinity Network"> 　　如图 1. 所示，两个目标集 \(A\in\mathbb{R}^{M\times D}\)，\(B\in\mathbb{R}^{N\times D}\)，扩展到维度 \(\mathbb{R}^{M\times N\times D}\)，相减后输入到网络中，预测出 affinity matrix，\(C\in\mathbb{R}^{M\times N}\)，其中 \(C _ {ij}=1\) 表示匹配上同一目标，否则认为是两个目标。这里关键是 Loss 的设计，最简单的 Loss 为： <span class="math display">\[L(A,B)=\frac{1}{MN}\sum _ {i=1} ^ {M}\sum _ {j=1}^N |C _ {ij}-G _ {ij}| \tag{1}\]</span> 其中 \(G\) 为亲和度矩阵的 groundtruth。实际对亲和度矩阵并没有 0-1 要求，最终是通过匈牙利算法找出匹配的，所以只要将同一目标的分数增大，不同目标的分数减小，最终即可选出匹配。由此设计 Loss： <span class="math display">\[L(A,B)=\sum _ {i,\,j;\,G _ {ij}=1} \left(\sum _ {k;\,G _ {ik}\neq 1}\mathrm{max}(0,C _ {ik}-C _ {ij}+m)+\sum _ {p;\,G _ {pj}\neq 1}\mathrm{max}(0,C _ {pj}-C _ {ij}+m)\right)\tag{2}\]</span> 其中 \(m\) 控制正负样本的相对大小。式(2)更容易使网络收敛。</p>
<h2 id="多模态融合">3. 多模态融合</h2>
<p>　　当多传感器检测的同一目标匹配上后，需要融合出一个最终的观测。可以采用卡尔曼滤波的方法，<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中的式(1)~(6)是时序下状态估计的迭代过程。对于多模态融合，虽然是同时获取的观测，但是融合过程类似，令测量矩阵 \(H _ k\) 为单位阵，所以可得卡尔曼增益： <span class="math display">\[K _ k=\frac{\bar{P} _ k}{\bar{P} _ k+R _ k} \tag{3}\]</span> 由此计算后验概率<a href="#2" id="2ref"><sup>[2]</sup></a>： <span class="math display">\[\begin{align}
\hat{x} _ k &amp;=\bar{x} _ k+K(z_k-\bar{x}) = \frac{\bar{P} _ kz _ k + \bar{x} _ kR _ k}{\bar{P} _ k+R _ k} \tag{4}\\
\hat{P} _ k &amp;=(I-KH _ k)\bar{P} _ k =\frac{\bar{P} _ kR _ k}{\bar{P} _ k+R _ k}\tag{5}
\end{align}\]</span> 对于多模态输入 \(A,B\)，令 \(A = \bar{x} _ k,\sigma _ A^2 = \bar{P} _ k\)，\(B=z _ k,\sigma _ B^2 =R _ k\)，可得多模态融合结果为： <span class="math display">\[\begin{align}
C &amp;= \frac{\sigma _ A^2B+\sigma _ B^2A}{\sigma _ A^2+\sigma _ B^2}\\
\sigma _ C^2 &amp;= \frac{\sigma _ A^2\sigma _ B^2}{\sigma _ A^2+\sigma _ B^2}\\
\tag{6}\end{align}\]</span> 式(6)等价于： <span class="math display">\[\begin{align}
\sigma _ C^2 &amp;= \frac{\sigma _ A^2\sigma _ B^2}{\sigma _ A^2+\sigma _ B^2}\\
C &amp;= \sigma _ C^2\left(\frac{A}{\sigma _ A^2}+\frac{B}{\sigma _ B^2}\right)\\
\tag{7}\end{align}\]</span> 这是 BCM<a href="#3" id="3ref"><sup>[3]</sup></a>！卡尔曼滤波器也是在贝叶斯概率模型下导出来的，可见两个高斯分布的同一状态的观测量，均可通过 BCM 进行融合。<br>
　　得到当前时刻多模态融合后的目标状态后，即可进一步作时序卡尔曼平滑获得最终估计的目标状态。<br>
　　另一种融合方法是在 JPDAF(Joint Probabilistic Data Association Filter)<a href="#4" id="4ref"><sup>[4]</sup></a>框架下作两次 PDA 融合<a href="#5" id="5ref"><sup>[5]</sup></a>，JPDAF 是另一种数据关联(目标匹配)的方法，这里不作展开。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Kuang, Hongwu, et al. "Multi-Modality Cascaded Fusion Technology for Autonomous Driving." arXiv preprint arXiv:2002.03138 (2020).<br>
<a id="2" href="#2ref">[2]</a> Fankhauser, Péter, et al. "Robot-centric elevation mapping with uncertainty estimates." Mobile Service Robotics. 2014. 433-440.<br>
<a id="3" href="#3ref">[3]</a> Tresp, Volker. "A Bayesian committee machine." Neural computation 12.11 (2000): 2719-2741.<br>
<a id="4" href="#4ref">[4]</a> Arya Senna Abdul Rachman, Arya. "3D-LIDAR Multi Object Tracking for Autonomous Driving: Multi-target Detection and Tracking under Urban Road Uncertainties." (2017).<br>
<a id="5" href="#5ref">[5]</a> JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>autonomous driving</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>Model Compression - &#39;Quantization&#39;</title>
    <url>/Quantization/</url>
    <content><![CDATA[<p>　　量化(Quantization)是模型压缩主要技术之一。因为模型训练后的权重及特征图基本符合高斯分布(特征图可能是混合高斯分布)，所以将 32-bit 的张量量化到低比特后也能保持模型输出的准确度。如果只量化模型的权重，那么只是减少了模型的存储及传输大小；只有同时量化权重及特征图(Weight &amp; Activation)，才能同时减少计算量。本文来详细描述下模型量化的细节。</p>
<h2 id="quantization-scheme">1. Quantization Scheme</h2>
<h3 id="fixed-point-approximation">1.1. Fixed Point Approximation</h3>
<p>　　设 Fixed Point 近似法中表示整数与小数的比特数分别为 \(\mathrm{IL,FL}\)，那么其可表达的浮点数范围为<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a>：\([-2^{\mathrm{IL-1}}, 2 ^ {\mathrm{IL-1}}-2 ^ {-\mathrm{FL}}]\)。这种方法很明显，精度较差且表达的浮点数范围有限。更进一步，可以针对不同的张量，用不同的 \(\mathrm{IL,FL}\)，即 Dynamic Fixed Point 近似法<a href="#1" id="1ref"><sup>[1]</sup></a>。综上，Fixed Point 近似法将一个浮点数表示为： <span class="math display">\[(-1)^s\cdot 2^{-\mathrm{FL}}\sum _ {i=0}^{\mathrm{IL+FL-2}}2^i\cdot x _ i \tag{1}\]</span> 其中 \(x_i\) 为第 \(i\) 比特位的值。<br>
　　对于 Dynamic Fixed Point，首先保证整数部分不溢出，所以量化张量 \(X\) 时设计： <span class="math display">\[\mathrm{IL}=\lceil\mathrm{lg} _ 2(\mathop{\max}\limits _ {S} X + 1)\rceil \tag{2}\]</span> 剩下的比特位即为符号位与小数位。<br>
　　用这种定点方式量化后，由式(1)可知，两数相乘可以转换为 bits shifts &amp; add 操作，极大提升计算效率。<br>
　　Fixed Point 近似法精度有限，尤其是当所要表示的值较大时，小数位 \(\mathrm{FL}\) 只能分到很小，所以精度必然有较大损失。</p>
<h3 id="range-based-linear-approximation">1.2. Range-Based Linear Approximation</h3>
<p>　　不同于 Fixed Point 近似中小数位有一定限制(导致精度较差)，Range-Based Linear 近似法直接将浮点数通过一个高精度的 Scale 值映射到对应量化位数中，所以能保持非常高的精度。</p>
<h4 id="asymmetric-mode">1.2.1. Asymmetric Mode</h4>
<p><img src="/Quantization/asymmetric.png" width="40%" height="40%" title="图 1. Asymmetric Quantization"> 　　如图 1. 所示，设浮点数为 \(r\)，那么 Asymmetric Linear Approximation 过程为<a href="#3" id="3ref"><sup>[3]</sup></a>： <span class="math display">\[q = round\left((r-r _ {min})\cdot\frac{2^n-1}{r _ {max}-r _ {min}}\right) = round(\frac{r}{S}-\frac{r _ {min}}{S}) \tag{3}\]</span> 等价于<a href="#4" id="4ref"><sup>[4]</sup></a>： <span class="math display">\[r = S(q-Z) \tag{4}\]</span> 其中 \(S\) 为映射的 Scale 参数，\(Z\) 表示零值被量化的值。 <img src="/Quantization/conv.png" width="40%" height="40%" title="图 2. Convolution Operator"> 　　如图 2. 所示，卷积操作可转化为矩阵相乘运算，接下来我们来推导量化后的矩阵相乘运算。假设两个 \(N\times N\) 矩阵相乘：\(r _ 3=r _ 1\cdot r _ 2\)。令 \(r _ \alpha ^{(i,j)}\) 表示矩阵 \(r _ \alpha\) 第 \((i,j)\) 个元素，\(1\leq i,j\leq N\)。矩阵张量对应的量化参数为 \(S _ \alpha,Z _ \alpha\)，对应的量化后的元素表示为 \(q _ \alpha ^{(i,j)}\)： <span class="math display">\[r _ \alpha ^{(i,j)} = S _ \alpha\left(q _ \alpha ^{(i,j)}-Z _ \alpha\right) \tag{5}\]</span> bias 量化参数设为 \(S _ b=S _ 1S _ 2,Z _ b=0\)，那么卷积运算(矩阵相乘)可表示为： <span class="math display">\[S _ 3\left(q _ 3 ^{(i,k)}-Z _ 3\right) = \sum _ {j=1} ^N S _ 1\left(q _ 1 ^{(i,j)}-Z _ 1\right)S _ 2\left(q _ 2 ^{(j,k)}-Z _ 2\right) + S _ b(q _ b^{(i)} - Z _ b)\tag{6}\]</span> 等价于： <span class="math display">\[\begin{align}
q _ 3 ^{(i,k)} &amp;= Z _ 3+M\left(\sum _ {j=1} ^N \left(q _ 1 ^{(i,j)}-Z _ 1\right)\left(q _ 2 ^{(j,k)}-Z _ 2\right)+ \frac{S _ b}{S _ 1S _ 2}q _ b^{(i)}\right) \\
&amp;= Z _ 3+M\left(NZ _ 1Z _ 2- Z _ 1\sum _ {j=1}^Nq _ 2^{(j,k)}-Z _ 2\sum _ {j=1}^Nq _ 1^{(i,j)}+\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}+ \frac{S _ b}{S _ 1S _ 2}q _ b^{(i)}\right)  \\
&amp;= Z _ 3+M\left(NZ _ 1Z _ 2- Z _ 1\sum _ {j=1}^Nq _ 2^{(j,k)}-Z _ 2\sum _ {j=1}^Nq _ 1^{(i,j)}+\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}+ q _ b^{(i)}\right)
\tag{7}
\end{align}\]</span> 其中 \(M=\frac{S _ 1S _ 2}{S _ 3}\) 可以离线计算，为上式唯一的浮点数。经验上可知 \(M\in(0,1)\)，进一步可将其表示为： <span class="math display">\[M\approx 2^{-n}M _ 0 \tag{8}\]</span> 假设 \(m\) 是能表示 \(M _ 0\) 的位数( int32 硬件下，\(m\) 可为 32)，那么有 \(2 ^ {n} M \leq 2 ^m -1\)，故： <span class="math display">\[\left\{\begin{array}{l}
n = \left\lfloor\mathrm{log} _ 2\frac{2 ^ m-1}{M}\right\rfloor \\
M _ 0 = \left\lfloor 2 ^ nM\right\rfloor
\end{array}\tag{9}\right.\]</span> 由此，乘以 \(M _ 0\) 可以用定点乘法实现，乘以 \(2 ^{-n}\) 可以用高效的位运算实现。式(7)中核心的计算为两个量化向量的乘加运算：\(\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}\)，其可通过传统的特定位数的 BLAS 库完成。<br>
　　具体的，令矩阵张量(卷积滤波器权重及特征图)量化为 8-bit，那么 8-bit 乘法需要用 32-bit 的累加器，即： <span class="math display">\[\mathrm{int32 += uint8 * uint8} \tag{10}\]</span> 所以式(7)中每一项累加时都是 32-bit 的，bias 也是量化为 32-bit 或是 rescale 到 32-bit，即 \(S _ b=S _ 1S _ 2,Z _ b=0\)。</p>
<h4 id="symmetric-mode">1.2.2. Symmetric Mode</h4>
<p><img src="/Quantization/symmetric.png" width="40%" height="40%" title="图 3. Symmetric Quantization"> 　　这种模式下最大最小值绝对值取相同值 \(R\) (该值可为任意值)，那么量化表示为： <span class="math display">\[r = Sq \tag{11}\]</span> Full Range 下 \(S = \frac{R}{(2^n-1)/2}\)(8-bit 则量化范围为 [-128,127]，Range 范围为 255)，Restricted Range 则 \(S = \frac{R}{2^{n-1}-1}\)(8-bit 量化范围为[-127,127]，Range 范围为 254)。Full Range 精度更高，PyTorch，ONNX 采用这种方式；TensorFlow，TensorRT，MKL-DNN 则采用 Restricted Range 量化方式。<br>
　　由此式(7)简化为： <span class="math display">\[q _ 3 ^{(i,k)} = M\left(\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}+ q _ b^{(i)}\right) \tag{12}\]</span> 实现更加简单。</p>
<h2 id="quantization-alogorithm">2. Quantization Alogorithm</h2>
<h3 id="post-training-quantization">2.1. Post-Training Quantization</h3>
<p>　　训练好的模型，可以直接对其权重进行量化，而对于特征的量化，则需要一个 Calibration 数据集来统计特征数值的分布，然后对其进行量化。<br>
　　量化参数的搜索，可以根据量化后的模型好坏进行 Loss 构建：</p>
<ol type="1">
<li><strong>任务级别损失函数</strong>：直接根据特定任务的指标来搜索及评价量化参数；</li>
<li><strong>张量级别损失函数</strong>：设计量化后的张量与原始张量的分布相似度，或者说信息损失度，如 KL-divergence 等度量方法；</li>
</ol>
<h3 id="quantization-aware-training">2.2. Quantization-Aware Training</h3>
<p>　　将训练好的模型直接进行量化，可能会导致对应的任务准确度下降，尤其对表达能力有限的小模型而言，以下情况会导致量化后模型准确度下降：</p>
<ol type="1">
<li>权重张量中数值差异 100 倍以上，导致小数值的量化误差较大；</li>
<li>权重张量中有 outlier 值，导致其它值的量化误差较大；</li>
</ol>
<p>而直接在训练的时候进行量化，可以保证完成模型训练也就得到了对应的高准确率的量化模型。 <img src="/Quantization/quantization-aware.png" width="80%" height="80%" title="图 4. Quantization-Aware Training Framework"> 　　如图 4. 所示，<a href="#4" id="3ref">[4]</a> 提出了一种 Quantization-Aware Training 的框架，权重和特征图均维护 float32 及 int8 数值，前向传播采用 int8 伪量化运算，反向传播更新权重的 float32 值，并作量化。 <img src="/Quantization/quantized_alg.png" width="60%" height="60%" title="图 5. Quantization-Aware Training Pipline"> 　　如图 5. 所示，<a href="#4" id="3ref">[4]</a> 基于 TensorFlow 实现了一种 Quantization-Aware Training 的算法，其步骤为：</p>
<ol type="1">
<li>建立一个浮点模型的 graph；</li>
<li>在 graph 中加入伪量化操作；</li>
<li>用伪量化的方式训练得到精度与浮点模型差不多的量化模型；</li>
<li>建立并优化量化的 Inference 模型 graph；</li>
<li>在量化引擎上作模型的 Inference；</li>
</ol>
<h4 id="simulated-quantization">2.2.1. Simulated Quantization</h4>
<p>　　这里采用 Asymmetric Linear Approximation 量化策略。对于权重，卷积运算时，先做伪量化操作，并且如果有 batch-normalization，则将其合并入卷积核权重中；对于特征图(Activations)，前向传播时都先做伪量化操作。伪量化操作如下<a href="#4" id="4ref"><sup>[4]</sup></a><a href="#5" id="5ref"><sup>[5]</sup></a>： <span class="math display">\[\begin{align}
\mathrm{clamp}(r\;;a,b) &amp;:= \mathrm{min}(\mathrm{max}(r,a),b) \\
s(a,b,n) &amp;:= \frac{b-a}{2 ^n-1} \\
q(r\;;a,b,n) &amp;:= \left\lfloor\frac{\mathrm{clamp}(r\;;a,b)-a}{s(a,b,n)}\right\rceil s(a,b,n)+a\\
\tag{13}
\end{align}\]</span> 其中 \([a,b]\) 是 被量化的浮点范围(可以是 \([r _ {min}, r _ {max}]\))，\(q(r\;;a,b,n)\) 即为浮点数 \(r\) 的伪量化表示，也是浮点数。</p>
<h4 id="learning-quantization-ranges">2.2.2. Learning Quantization Ranges</h4>
<p>　　训练时，每次迭代，权重与特征图都要作伪量化处理，所以每次要确定量化参数。对于权重，因为其服从均值为零的高斯分布，所以 \([a,b]\) 直接设为其最大值与最小值即可；对于特征图，其数值与输入相关，所以策略为：刚开始训练的时候不对其作量化处理，之后用 EMA(Exponential Moving Averages) 对量化参数进行平滑，去除特征图输出突变的影响。</p>
<h4 id="batch-normalization-folding">2.2.3. Batch Normalization Folding</h4>
<p>　　作 Inference 或者说前向传播时，BN 可以合并入卷积核权重中，所以在量化前，先要将其合并，然后权重就仅限于卷积操作中。对于每个卷积 filter，其生成特征图以及 BN 过程如下： <span class="math display">\[\begin{align}
\hat{x} _ i &amp;\gets wx _ i+b\\
\mu _ B &amp;\gets \frac{1}{m}\sum _ {i=1}^m \hat{x} _ i\\
\sigma^2 _ B &amp;\gets \frac{1}{m}\sum _ {i=1}^m(\hat{x} _ i-\mu _ B)^2\\
y _ i &amp;\gets \gamma\frac{\hat{x} _ i-\mu _ B}{\sqrt{\sigma^2 _ B+\epsilon}} + \beta\\
\tag{14}
\end{align}\]</span> 由此可得： <span class="math display">\[\begin{align}
y _ i &amp;\gets \gamma\frac{\hat{x} _ i-\mu _ B}{\sqrt{\sigma^2 _ B+\epsilon}} + \beta\\
&amp;\gets \gamma\frac{wx _ i+b-\mu _ B}{\sqrt{\sigma^2 _ B+\epsilon}} + \beta\\
&amp;\gets \frac{\gamma wx _ i}{\sqrt{\sigma^2 _ B+\epsilon}} +\frac{\gamma(b-\mu _ B)}{\sqrt{\sigma^2 _ B+\epsilon}}+ \beta\\
\tag{15}
\end{align}\]</span> 由此可知作 Inference 时，BN 参数 \(\mu _ B,\sigma^2 _ B,\gamma, \beta\) 可合并到卷积 Filter 参数中： <span class="math display">\[\left\{\begin{array}{l}
\hat{w} = \frac{\gamma w}{\sqrt{\sigma^2 _ B+\epsilon}}\\
\hat{b} = \frac{\gamma(b-\mu _ B)}{\sqrt{\sigma^2 _ B+\epsilon}}+ \beta\\
\end{array}\tag{16}\right.\]</span></p>
<h3 id="trained-quantization-thresholds">2.3. Trained Quantization Thresholds</h3>
<p>　　Post-Training Quantization 以及 Quantization-Aware Training 都是直接对张量的分析来搜索或近似求解量化参数的，Trained Quantization Thresholds 则在训练的时候同时训练得到量化参数。</p>
<h4 id="pact">2.3.1. PACT</h4>
<p>　　PACT<a href="#13" id="13ref"><sup>[13]</sup></a> 定义了激活函数输出的最大值，该最大值就是 Symmetric 量化中的激活层量化参数 Scale。具体的，改进 Relu： <span class="math display">\[ y = \mathrm{PACT}(x) = 0.5(|x|-|x-\alpha|+\alpha)=
\left\{\begin{array}{l}
0, \;\;x\in(-\infty,0)\\
x, \;\;x\in[0,\alpha]\\
\alpha, \;\;x\in[\alpha, +\infty)
\end{array}\tag{17}\right.\]</span> 对应的量化参数偏导为： <span class="math display">\[\frac{\partial y _ q(x;\,\alpha)}{\partial \alpha}=
\left\{\begin{array}{l}
0, \;\;x\in(-\infty, \alpha)\\
1, \;\;x\in[\alpha,+\infty)
\end{array}\tag{18}\right.\]</span></p>
<h4 id="tqt">2.3.2. TQT</h4>
<p>　　TQT(Trained Quantization Thresholds)<a href="#14" id="14ref"><sup>[14]</sup></a>则提出了一种同时学习权重和激活函数的量化参数的方法。为了简化，其采用 Linear Symmetric Approximation，且 Scale 参数限定为 \(s=2 ^ {-f}\)，由式(8,9)可知，消除了定点乘法运算。前向传播与式(13)并无差异，对每个权重即激活层作 scale，round，saturate，de-quant 操作。反向传播则需要对量化值 \(q(x;s)\) 求导，量化值表示为： <span class="math display">\[q(x;s)=
\left\{\begin{array}{l}
\left\lfloor\frac{x}{s}\right\rceil \cdot s, \;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\
n\cdot s, \;\;\;\;\left\lfloor\frac{x}{s}\right\rceil &lt; n\\
p\cdot s, \;\;\;\;\left\lfloor\frac{x}{s}\right\rceil &gt; p\\
\end{array}\tag{19}\right.\]</span> 其中 \(n,p\) 分别为量化值域的最小最大值。定义 \(\frac{\partial \lfloor x\rceil}{\partial x} = 1\)，那么对 Scale 的偏导为： <span class="math display">\[\nabla _ sq(x;s)=
\left\{\begin{array}{l}
\left\lfloor\frac{x}{s}\right\rceil - \frac{x}{s}, &amp;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\
n, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &lt; n\\
p, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &gt; p\\
\end{array}\tag{20}\right.\]</span> 为了稳定性，令 \(\nabla _ {(\mathrm{log} _ 2 t)} s = s\, \mathrm{In}(2)\)，则： <span class="math display">\[\nabla _ {(\mathrm{log} _ 2t)}q(x;s)= s\,\mathrm{In}(2)\cdot
\left\{\begin{array}{l}
\left\lfloor\frac{x}{s}\right\rceil - \frac{x}{s}, &amp;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\
n, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &lt; n\\
p, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &gt; p\\
\end{array}\tag{21}\right.\]</span> 对应的，对输入 \(x\) 的偏导数为： <span class="math display">\[\nabla _ xq(x;s)=
\left\{\begin{array}{l}
1,&amp;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\
0, &amp;\;otherwise\\
\end{array}\tag{22}\right.\]</span></p>
<p>　　由此可与网络权重一起训练得到量化参数。Graffitist<a href="#15" id="15ref"><sup>[15]</sup></a>基于 TensorFlow 实现了上述算法；NNCF<a href="#16" id="16ref"><sup>[16]</sup></a>基于 Pytorch 实现了类似算法。</p>
<h2 id="quantized-framework">3. Quantized Framework</h2>
<p>　　不管是 Post-Training Quantization 还是 Quantization-Aware Training，算法端都还是用伪量化操作实现的，部署时就必须用 INT8 引擎。据我所知目前 INT8 引擎有：</p>
<ol type="1">
<li>DSP/加速芯片平台<br>
目测没有开源的，大家自个玩自个的；</li>
<li>CPU 平台<br>
Google 的 TensorFlow Lite<a href="#6" id="6ref"><sup>[6]</sup></a>，Facebook 的 QNNPACK<a href="#8" id="8ref"><sup>[8]</sup></a>，Tencent 的 NCNN<a href="#9" id="9ref"><sup>[9]</sup></a>。</li>
<li>GPU 平台<br>
NVIDIA 的 TensorRT<a href="#10" id="10ref"><sup>[10]</sup></a>，TVM<a href="#11" id="11ref"><sup>[11]</sup></a>。</li>
</ol>
<p>而伪量化框架则在深度学习框架(caffe，pytorch，tensorflow)中开源的较多，如基于 pytorch 的 distiller<a href="#3" id="3ref"><sup>[3]</sup></a>，NNCF<a href="#16" id="16ref"><sup>[16]</sup></a>。<br>
　　对于 ARM 平台，INT8 引擎会通过 NEON 指令集加速；对于 x86 平台，INT8 引擎会通过 SSE 加速；对于 NVIDIA GPU 平台，则通过 dp4a<a href="#12" id="12ref"><sup>[12]</sup></a> 矩阵运算库加速。dp4a 实现了基础的 INT8 矩阵相乘操作，目前 cuDNN，cuBLAS，TensorRT 均采用该指令集。下面对 INT8 引擎作简要阐述。</p>
<h3 id="ristretto1">3.1. Ristretto<a href="#1" id="1ref"><sup>[1]</sup></a></h3>
<p>　　Ristretto 是一种基于 (Dynamix) Fixed Point Approximation, Post-Training Quantization 的量化框架，其精度有限，量化的 Inference 引擎可用 bits shifts &amp; add 操作实现，比较适合应用于 DSP 等嵌入式平台。</p>
<h3 id="tensorflow-lite6qnnpack8ncnn9">3.2. TensorFlow Lite<a href="#6" id="6ref"><sup>[6]</sup></a>/QNNPACK<a href="#8" id="8ref"><sup>[8]</sup></a>/NCNN<a href="#9" id="9ref"><sup>[9]</sup></a></h3>
<p>　　TensorFlow Lite 是 Google 基于 TensorFlow 开发的针对移动嵌入式 CPU 平台的模型(量化)加速框架，其实现在 2.2 小节中已有详细的描述，有较高精度，<a href="#4" id="4ref">[4]</a> 实现了 Quantization-Aware Training。其中 INT8 矩阵运算采用了 gemmlowp<a href="#7" id="7ref"><sup>[7]</sup></a>。<br>
　　移动端的 CPU 的量化计算引擎开源的也比较多，如 Facebook 的 QNNPACK<a href="#8" id="8ref"><sup>[8]</sup></a>，腾讯的 ncnn-int8<a href="#9" id="9ref"><sup>[9]</sup></a>。</p>
<h3 id="tensorrt10">3.3. TensorRT<a href="#10" id="10ref"><sup>[10]</sup></a></h3>
<p>　　TensorRT 是 NVIDIA 基于 GPU 平台的模型(量化)加速框架，其基于 Symmetric Linear Approximation 量化策略，并且只支持 Post-Training Quantization，其内部可能直接调用 dp4a，也可能调用 cuDNN 或 cuBLAS。TVM<a href="#11" id="11ref"><sup>[11]</sup></a> 调用 dp4a 实现了基于 python 的 INT8 引擎，对于部署来讲没有 TensorRT 高效。<br>
　　对于特征图的量化参数 \(S\) 的搜索，其使用张量级别的损失函数，最小化量化前后特征图值分布差异性的方式，KL-divergency，即两个分布的相对熵。假设连个分布 \(P,Q\)，那么两者的相对熵为： <span class="math display">\[E(P,Q) = \sum _ i P(i)\cdot\mathrm{log}\left(\frac{P(i)}{Q(i)}\right) \tag{23}\]</span> 熵越大，表示两个分布差异性越大，即量化后信息损失越大。这里也可以采用其它能描述两个分布差异性的方式，如 EMD。整个量化参数搜索过程为：</p>
<ol type="1">
<li>准备训练好的 FP32 模型，以及一个作校正(Calibration)的数据集；</li>
<li>用 FP32 模型跑数据集，统计每个特征图的值分布；</li>
<li>对不同的量化参数，根据式(17)计算量化前后的相对熵；选择最优的量化参数；</li>
<li>根据最优的量化参数量化特征图得到量化模型(权重值分布比较集中，所以可以直接用最大值作为量化参数，具体还得看 TensorRT 怎么做的)；</li>
<li>保存量化参数为 Calibration Table，载入该值即可启动 INT8 引擎作量化 Inference；</li>
</ol>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Gysel, Philipp. "Ristretto: Hardware-oriented approximation of convolutional neural networks." arXiv preprint arXiv:1605.06402 (2016).<br>
<a id="2" href="#2ref">[2]</a> Gupta, Suyog, et al. "Deep learning with limited numerical precision." International Conference on Machine Learning. 2015.<br>
<a id="3" href="#3ref">[3]</a> https://nervanasystems.github.io/distiller/index.html<br>
<a id="4" href="#4ref">[4]</a> Jacob, Benoit, et al. "Quantization and training of neural networks for efficient integer-arithmetic-only inference." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
<a id="5" href="#5ref">[5]</a> Krishnamoorthi, Raghuraman. "Quantizing deep convolutional networks for efficient inference: A whitepaper." arXiv preprint arXiv:1806.08342 (2018).<br>
<a id="6" href="#6ref">[6]</a> https://www.tensorflow.org/mobile/tflite<br>
<a id="7" href="#7ref">[7]</a> https://github.com/google/gemmlowp<br>
<a id="8" href="#8ref">[8]</a> https://github.com/pytorch/QNNPACK<br>
<a id="9" href="#9ref">[9]</a> https://github.com/Tencent/ncnn/pull/487<br>
<a id="10" href="#10ref">[10]</a> Migacz, Szymon. "8-bit inference with tensorrt." GPU technology conference. Vol. 2. No. 4. 2017.<br>
<a id="11" href="#11ref">[11]</a> https://tvm.apache.org/2019/04/29/opt-cuda-quantized<br>
<a id="12" href="#12ref">[12]</a> https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/<br>
<a id="13" href="#13ref">[13]</a> Choi, Jungwook, et al. "Pact: Parameterized clipping activation for quantized neural networks." arXiv preprint arXiv:1805.06085 (2018).<br>
<a id="14" href="#14ref">[14]</a> Jain, Sambhav R., et al. "Trained quantization thresholds for accurate and efficient neural network inference on fixed-point hardware." arXiv preprint arXiv:1903.08066 (2019).<br>
<a id="15" href="#15ref">[15]</a> https://github.com/Xilinx/graffitist<br>
<a id="16" href="#16ref">[16]</a> Kozlov, Alexander, et al. "Neural Network Compression Framework for fast model inference." arXiv preprint arXiv:2002.08679 (2020).</p>
]]></content>
      <categories>
        <category>Model Compression</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Model Compression</tag>
      </tags>
  </entry>
  <entry>
    <title>LOAM(Lidar Odometry and Mapping)</title>
    <url>/LOAM/</url>
    <content><![CDATA[<p>　　SLAM 是机器人领域非常重要的一个功能模块，而基于激光雷达的 SLAM 算法，LOAM(Lidar Odometry and Mapping)，则应用也相当广泛。本文从经典的 LOAM 出发，详细描述下激光 SLAM<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 中的一些模块细节。</p>
<h2 id="问题描述">1. 问题描述</h2>
<h3 id="scan-定义">1.1. Scan 定义</h3>
<p>　　针对旋转式机械雷达，Scan 为单个激光头旋转一周获得的点云，类似 VLP-16 旋转一周则是“几乎”同时获得了 16 个 Scan。针对棱镜旋转而激光头不旋转的雷达(Solid State LiDARs)，如大疆 Livox 系列，Scan 则可定义为一定时间下累积获得的点云。</p>
<h3 id="sweep-定义">1.2. Sweep 定义</h3>
<p>　　Sweep 定义为静止的机器人平台上激光雷达能覆盖到所有空间的点云。<br>
　　针对旋转式机械雷达，Sweep 即为旋转一周获得的由一个或多个 Scan 组成的点云。针对棱镜旋转而激光头不旋转的雷达，由于其属于非重复性扫描(Non-repetitive Scanning)结构，所以 Sweep 理论上为时间趋于无穷大时获得的点云，但是狭义上，可以认为一段较长时间下(相对于 Scan 时间)，获得的点云。<br>
<img src="/LOAM/motor_lidar.png" width="60%" height="60%" title="图 1. 3D Lidar Updated from 2D Lidar with a Motor"> 　　那么，如果给激光雷达加上一个马达呢？如图 1. 所示，<a href="#1" id="1ref">[1]</a> 中设计了一种 3D Lidar 装置，由一个只有一个激光头的 2D Lidar 和一个马达组成，激光扫描频率为 40Hz，马达转速为 180°/s。这种装置下，Scan 意义不变，Sweep 则为 1s 内该装置获得的点云(因为 1s 的时间内，该装置获得的点云可覆盖所有能覆盖的空间)。</p>
<h3 id="非重复性扫描激光雷达">1.2. 非重复性扫描激光雷达</h3>
<p><img src="/LOAM/livox.png" width="60%" height="60%" title="图 2. Livox Scanning Pattern"> 　　其实，大疆的 Livox 非重复性扫描雷达相当于把这马达移到了内部的棱镜中，而且加上非对称，所以随着时间的累积，可获得相当稠密的点云。<br>
　　Livox 这种非重复式扫描的激光雷达价格低廉，相对于传统的多线激光雷达有很多优点，但是有个致命的缺点：<strong>只能准确捕捉静态物体，无法准确捕捉动态物体；对应的，只能作 Mapping，很难作动态障碍物的估计。</strong>因为在一帧点云的扫描周期 \(T\) 内，如果目标速度为 \(v\)，那么 Livox 式雷达在扫描周期内都会扫到目标，目标的尺寸会被放大 \(Tv\)，而传统旋转的线束雷达真正扫到目标的时间为 \(t\ll T\)。当 \(T=0.1s\)，\(v=20m/s\) 时，尺寸放大为 2m，而一般小汽车车长也就几米。<strong>所以尺寸是估不准的，但是其它属性，如位置，速度，在目标加速度不是很大的情况下，可能还是有技巧可以估准的，具体就得看实验效果。另一种思路：直接对其进行物理建模，先假设已知目标速度，那么所有点即可恢复出目标的真实尺寸，然后可进一步估计速度，由此迭代至最优值</strong>。<br>
　　由于本车的状态可以通过其它方式(如 IMU)获得，所以本车运动所引起的点云畸变(即 Motion Blur，基本所有雷达都会有这个问题，详见 2.3，4.1 章节)可以很容易得到补偿，所以对于静态目标，点云是能准确捕捉到其物理属性的。</p>
<h3 id="符号定义">1.3. 符号定义</h3>
<p>　　本文首先基于图 1. 的装置进行 LOAM 算法的描述，一般的多线激光雷达或是 Livox 雷达则可以认为是图 1. 的特殊形式，算法过程很容易由此导出。<br>
　　设第 \(k\) 次 Sweep 的点云为 \(\mathcal{P} _ k\)，Lidar 坐标系定义为此次 Sweep 初始扫描(也可定义为结束扫描)时刻 \(t_k\) 时， Lidar 位置下的坐标系 \(L\)，Sweep 由 \(S\) 个 Scan 组成，或由 \(I\) 个点组成，归纳为： <span class="math display">\[\mathcal{P} _ k = \{\mathcal{P}_{(k,s)}\}_{s=1}^S = \{\mathit{X}_{(k,i)}^L\}_{i=1}^I  \tag{1}\]</span> 定义 \(\mathit{T} _ k^L(t)\) 为 Lidar 从时间 \(t_k\to t\) 的位姿变换；定义 \(\mathit{T} _ {k}^L(t_{(k,i)})\)(简写为 \(\mathit{T} _ {(k,i)}^L\)) 为 \(t_{(k,i)}\) 时刻接收到的点 \(\mathit{X} _ {(k,i)}\) 变换到坐标系 \(L\)，即 Sweep 初始时刻 Lidar 位置，的变换矩阵。<br>
　　<strong>运动补偿问题</strong>： <span class="math display">\[\{\mathit{T} _ {(k,i)}^L\} _ {i=1}^I \tag{2}\]</span> 　　<strong>里程计问题</strong>： <span class="math display">\[\mathit{T} _ K^L(t) \prod _ {k=1}^K\mathit{T} _ {k-1}^L(t _ {k}) \tag{3}\]</span></p>
<h2 id="loam-for-2d-lidar-with-motor12">2. LOAM for 2D Lidar with Motor<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a></h2>
<p><img src="/LOAM/loam.png" width="70%" height="70%" title="图 3. LOAM Software System"> 　　硬件装置如图 1. 所示，这里不再赘述，软件算法流程如图 3. 所示，\(\mathcal{\hat{P}} _ k=\{\mathcal{P} _ {(k,s)}\}\) 为累积的 Scan 点云，其都会注册到 \(L\) 坐标系，得到 \(\mathcal{P} _ k\)。Lidar Odometry 由 \(\mathcal{\hat{P}} _ k\) 注册到 \(\mathcal{P} _ {k-1}\) 生成高频低精度的位姿，并且生成运动补偿后的 Sweep 点云(这里也可以用其它的里程计实现，如 IMU 等)；Lidar Mapping 则由 \(\mathcal{P}_k\) 注册到世界坐标系 \(W\) 下的地图 \(\mathcal{P}_m\) 中，生成低频高精度的位姿和地图；Transform Integration 则插值出高精度高频的位姿。</p>
<h3 id="feature-extraction">2.1. Feature Extraction</h3>
<p>　　这里提取的特征并没有描述子，更确切的说是找出有代表性的点。定义一种描述局部平面曲率的的变量： <span class="math display">\[c = \frac{1}{\vert \mathcal{S}\vert\cdot \Vert\mathit{X} _ {(k,i)}^L\Vert} \left\Vert\sum _ {j\in\mathcal{S},j\ne i}\left(\mathit{X} _ {(k,i)}^L-\mathit{X} _ {(k,j)}^L\right)\right\Vert \tag{3}\]</span> 其中 \(\mathcal{S}\) 为点 \(\mathit{X} _ {(k,i)}^L\) 相邻的同一 Scan 的点，其前后时序上各一半。根据 \(c\) 的值，由大到小选出 Edge Points 集，由小到大选出 Planar Points 集。最终选出的点需满足以下条件：</p>
<ol type="1">
<li>为了特征点的均匀分布，将空间进行栅格化，每个栅格最多容纳特定的点数；</li>
<li>被选择的点的周围点不会被选择；</li>
<li>对于 Planar Points 集中的点，如果其平面与雷达射线接近平行，那么则不予采用；</li>
<li>对于 Edge Points 集中的点，如果其处于被遮挡的区域边缘，那么也不予采用；</li>
</ol>
<h3 id="feature-registration">2.2. Feature Registration</h3>
<p><img src="/LOAM/icp.png" width="50%" height="50%" title="图 4. Registration"> 　　如图 4. 所示，Lidar Odometry 模块的作用是将累积的 Scan 注册到上一时刻的 Sweep 中。设 \(\mathcal{\bar{P}} _ {k-1}\) 为点云 \(\mathcal{P} _ {k-1}\) 投影到 \(t _ {k}\) 的 Lidar 坐标系 \(L _ k\) 后的表示。\(\mathcal{\tilde{E}} _ k, \mathcal{\tilde{H}} _ k\) 为 \(\mathcal{\hat{P}} _ k\) 中提取的 Edge Points 与 Planar Points 集，并转换到了 \(L _ k\) 坐标系。 <img src="/LOAM/loss.png" width="50%" height="50%" title="图 4. Edge & Planar Points Correspondence"></p>
<ol type="1">
<li><strong>Point to Edge</strong><br>
对于点 \(i\in\mathcal{\tilde{E}} _ k\)，如图 4. 所示，找到其最近的点 \(j\in\mathcal{\bar{P}} _ {k-1}\)，并在点 \(j\) 前后相邻的两个 Scan 中找到与点 \(i\) 最近的点，记为 \(l\)（同一 Scan 不会打到同一 Edge 处）。通过式 (3) 进一步确认 \(j,l\) 是否满足 Edge Points 的条件，如果满足，那么直线 \((j,l)\) 则就是点 \(i\) 的对应直线，误差函数为： <span class="math display">\[d _ {\mathcal{E}} = \frac{\left\vert \left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,j)}^L\right)\times\left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right) \right\vert}{\left\vert\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\right\vert} \tag{4}\]</span></li>
<li><strong>Point to Plane</strong><br>
对于点 \(i\in\mathcal{\tilde{H}} _ k\)，如图 4. 所示，找到其最近的点 \(j\in\mathcal{\bar{P}} _ {k-1}\)，并在点 \(j\) 同一 Scan 中找到与点 \(i\) 第二近的点 \(l\)，在其前后相邻的两个 Scan 中找到与点 \(i\) 最近的点，记为 \(m\)。通过式 (3) 进一步确认 \(j,l,m\) 是否满足 Planar Points 的条件，如果满足，那么平面 \((j,l,m)\) 则就是点 \(i\) 的对应面，误差函数为： <span class="math display">\[d _ {\mathcal{H}} = \frac{\left\vert \left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,j)}^L\right)^T\cdot\left(\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\times\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,m)}^L\right)\right) \right\vert}{\left\vert\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\times\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,m)}^L\right)\right\vert} \tag{5}\]</span></li>
</ol>
<h3 id="motion-estimation">2.3. Motion Estimation</h3>
<p>　　首先进行运动补偿，即求式(2)。记 \(\mathit{T} _ k^L(t) = [\mathit{R} _ k^L(t)\; \mathit{\tau} _ k^L(t)]\)。假设 \(t_k\to t\) 雷达为匀速运动，那么根据每个点的时间戳进行运动插值: <span class="math display">\[\mathit{T} _ {(k,i)}^L = 
\begin{bmatrix}
\mathit{R} _ {(k,i)}^L &amp; \mathit{\tau} _ {(k,i)}^L
\end{bmatrix} = 
\begin{bmatrix}
e^{\hat{\omega}\theta s} &amp; s\mathit{\tau} _ k^L(t)
\end{bmatrix} = 
\begin{bmatrix}
e^{\hat{\omega}\theta \frac{t _ {(k,i)}-t _ k}{t-t _ k}} &amp; \frac{t _ {(k,i)}-t _ k}{t-t _ k}\mathit{\tau} _ k^L(t)
\end{bmatrix} =
\begin{bmatrix}
\mathbf{I} + \hat{\omega} \mathrm{sin}\left(s\theta\right) + \hat{\omega}^2\left(1-\mathrm{cos}\left(s\theta\right)\right) &amp; s\mathit{\tau} _ k^L(t)
\end{bmatrix}
\tag{6}\]</span> 其中 \(\theta, \omega\) 分别是 \(\mathit{R} _ k^L(t)\) 的幅度与旋转角，\(\hat{\omega}\) 是 \(\omega\) 的 Skew Symmetric Matrix。<br>
　　由此，对于特征点集，有如下关系： <span class="math display">\[\begin{align}
\mathit{\tilde{X}} _ {(k,i)}^L &amp;= \mathit{T} _ {(k,i)}^L\mathit{X} _ {(k,i)} \\
\tag{7}
\end{align}\]</span> 带入式(4)(5)，可简化为以下非线性最小二乘优化函数： <span class="math display">\[f(\mathit{T} _ {k}^L(t)) = \mathbf{d} \tag{8}\]</span> 其中每一行表示一个特征点及对应的误差，用非线性优化使 \(\mathbf{d}\to \mathbf{0}\)： <span class="math display">\[\mathit{T} _ {k}^L(t)\gets \mathit{T} _ {k}^L(t) - (\mathbf{J}^T\mathbf{J}+\lambda\mathrm{diag(\mathbf{J}^T\mathbf{J})})^{-1}\mathbf{J}^T\mathbf{d} \tag{9}\]</span> 其中雅克比矩阵 \(\mathbf{J}=\frac{\partial f}{\partial \mathit{T} _ {k}^L(t)}\)；\(\lambda\) 由优化方法决定，如 LM，Gaussian-Newton 等。</p>
<h3 id="lidar-odometry">2.4. Lidar Odometry</h3>
<p><img src="/LOAM/loam_alg.png" width="40%" height="40%" title="图 5. Lidar Odometry Algorithm"> 　　Lidar Odometry 模块生成 10Hz 的高频低精度雷达位姿(雷达 Scan 频率为 40Hz)，1Hz 的去畸变的点云帧，算法过程如图 5. 所示，优化时对每个特征点根据匹配距离作了权重处理。这里求取雷达位姿 \(\mathit{T} _ k^L(t)\) 是通过点云注册实现的，<strong>也完全可以采用其它里程计，如 IMU 等</strong>。</p>
<h3 id="lidar-mapping">2.5. Lidar Mapping</h3>
<p>　　Lidar Mapping 模块生成 1Hz 的低频高精度雷达位姿以及地图。式(3)后半部分表示的就是本模块要求的第 \(t_k\) 时刻在世界坐标系下的低频高精度位姿 \(\mathit{T} _ {k-1}^W(t _ k)\)。设累积到第 \(k-1\) 个 Sweep 的地图为 \(\mathcal{Q} _ {k-1}\)，第 \(k\) 次 Sweep 点云 \(\mathcal{\bar{P}} _ k\) 在世界坐标系下的表示为 \(\mathcal{\bar{Q}} _ k \)，将 \(\mathcal{\bar{Q}} _ k \) 注册到世界地图 \(\mathcal{Q} _ {k-1}\) 中，就求解出了位姿 \(\mathit{T} _ {k}^W(t _ {k+1})\)。<br>
　　算法过程与 Lidar Odometry 类似，不同的是：</p>
<ol type="1">
<li>为了提升精度，特征点数量增加了好几倍(点云量也增多了，Sweep VS. Map)；</li>
<li>由于 Map 中无法区分相邻的 Scan，所以找 Map 中对应的 Edge 或 Planar 时，采用以下方法：找到该特征点在对应 Map 中最近的点集 \(\mathcal{S'}\)，计算该点集的协方差矩阵 \(\mathbf{M}\)，其特征值与特征向量为 \(\mathbf{V,E}\)。如果该点集分布属于 Edge Line，那么有一个显著较大的特征值，对应的特征向量代表该直线的方向；如果该点集分布属于 Planar Patch，那么有两个显著较大的特征值，最小特征值对应的特征向量表示了该平面的方向。由此找到 Point-to-Edge，Point-to-Plane 匹配。</li>
</ol>
<p>　　建图时需要对 Map 进行采样，通过 Voxel-Grid Filter 保持栅格内点的密度，由此减少内存及运算量，Edge Points 的栅格应该要比 Planar Points 的小。<br>
　　得到低频高精度雷达位姿后，结合 Lidar Odometry(式(3))，即可输出高频高精度(精度相对世界坐标系而言)的雷达位姿。</p>
<h2 id="loam-for-livox3">3. LOAM for Livox<a href="#3" id="3ref"><sup>[3]</sup></a></h2>
<p>　　1.2 小节中已经阐述了 Livox 雷达的特性，这里整理如下：</p>
<ol type="a">
<li><strong>Small FoV</strong><br>
包括 MEMS 这种 Solid State LiDARs，一般都有较小的视场角，不像旋转式机械雷达可达 360°；</li>
<li><strong>Irregular Scanning Pattern</strong><br>
如图 2. 所示，雷达扫描出的 Pattern 是无规则的，这就导致有效特征提取的难度提升；</li>
<li><strong>Non-repetitive Scanning</strong><br>
非重复性扫描，有利有弊；</li>
<li><strong>Motion Blur</strong><br>
包括自身运动及目标运动所产生的点云畸变。自身运动所导致的点云畸变可以通过估计自身运动后，对点云进行运动补偿来矫正；而由于帧内周期均会扫描到目标，所以目标运动所产生的点云畸变影响较大，且基本无法消除。</li>
</ol>
<h3 id="workflow">3.1. Workflow</h3>
<p><img src="/LOAM/livox_loam.png" width="90%" height="90%" title="图 6. Livox Loam"> 　　Livox LOAM 可以认为是 LOAM 的简化版，直接从每帧的点云中提取出 Edge Points 和 Planar Points，经过线性插值的运动补偿后，在 Map 中找到对应的 Edge Line 与 Planar Patch，由此建立优化函数。相比于 LOAM，本文干掉了高频低精度的 Lidar Odometry(因为 Livox 没有前后 Scan 概念，很难做 Scan-to-Sweep 的点云注册)，直接出 20Hz 高频高精度的 Odometry 与 Map(计算平台强+软件多线程)。<br>
　　此外本文针对雷达特性还作了更细致的工程改进，包括：</p>
<ol type="1">
<li>更严格的特征点选取<br>
去除视场边缘处的特征点；去除较大或较小反射强度的点；</li>
<li>改进的特征提取<br>
为了增多提取的特征点，将周围反射率变化较大的点也列入 Edge Points；</li>
<li>Outlier Rejection<br>
在优化迭代时，先迭代两步，然后去除掉有较大误差的点，最后作进一步迭代；</li>
<li>Dynamic Objects Filtering<br>
扣除掉动态障碍物的点云，这需要动态障碍物检测模块的支持；</li>
</ol>
<h2 id="loam-for-vlp-164">4. LOAM for VLP-16<a href="#4" id="4ref"><sup>[4]</sup></a></h2>
<h3 id="motion-blur">4.1. Motion Blur</h3>
<p>　　运动导致的点云畸变主要有两种：自身运动与目标运动。对于旋转式线束雷达来说，目标运动所导致的畸变基本可考虑不计(只有目标正好处于初始扫描与结束扫描的交界处时会有影响；Mapping 时则已扣掉动态障碍物，所以不影响)，这里主要讨论自身运动所导致的点云畸变影响。<br>
　　每帧激光雷达数据(即一次 Sweep)都会标记到同一时间戳，假设标记到初始扫描的时刻。假设激光雷达旋转一周的扫描周期为 \(T\)，考虑一次 Sweep：\(t\in [0,T]\)。假设在扫描周期内自身为匀速运动，速度为 \(v\)，那么场景中点云的最大偏移畸变为 \(vT\)。考虑两次 Sweep: \(t _ 1,t _ 2\)，对应的速度为 \(v _ 1, v _ 2\)，那么两个时刻对同一物体的点云偏差量为 \(v _ 1T,v _ 2T\)。在世界坐标系下，该物体观测的点云最坏的不一致量可达到 \(|v _ 1T+v _ 2T|\)(自身运动有旋转的时候)，当然大多数情况可能是 \(|v _ 1T-v _ 2T|\)。</p>
<ol type="a">
<li><strong>单帧情况</strong><br>
当 \(T=0.1s,v=20m/s\) 时，畸变量为 2m，对于目标检测算法，虽然目标整体漂移了约 2m，不影响检测(尺寸未变)，但是直接导致观测的目标位置漂了约 2m！如果目标正好处于初始扫描和结束扫描的位置，那么目标的尺寸也会失真。</li>
<li><strong>多帧情况</strong><br>
这种情况指 Mapping 的过程。如果 \(t _ 1, t _ 2\) 时间跨度大，那么世界坐标系下同一物体的不一致性会相当高。如果是相邻 \(n\) 帧，假设自身加速度为 \(a = 5m/s^2\)，那么不一致量为 \(|v _ 1T-v _ 2T|=nTaT=0.05n\)，相邻帧可达 5cm ！</li>
</ol>
<p>由此可见，不管是单帧任务还是多帧任务，点云的运动补偿不可不做。</p>
<h3 id="other">4.2. Other</h3>
<p>　　<a href="#4" id="4ref">[4]</a> 根据代码详细描述了 LOAM 应用到旋转式多线激光雷达的诸多细节，代码中采用了 IMU 里程计作为高频低精度的位姿估计。其它内容在以上章节中都有描述，这里就不再展开了。</p>
<h2 id="reference">5. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Zhang, Ji, and Sanjiv Singh. "LOAM: Lidar Odometry and Mapping in Real-time." Robotics: Science and Systems. Vol. 2. No. 9. 2014.<br>
<a id="2" href="#2ref">[2]</a> Zhang, Ji, and Sanjiv Singh. "Low-drift and real-time lidar odometry and mapping." Autonomous Robots 41.2 (2017): 401-416.<br>
<a id="3" href="#3ref">[3]</a> Lin, Jiarong, and Fu Zhang. "Loam_livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV." arXiv preprint arXiv:1909.06700 (2019).<br>
<a id="4" href="#4ref">[4]</a> https://zhuanlan.zhihu.com/p/57351961</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>Filter Pruning</title>
    <url>/Filter-Pruning/</url>
    <content><![CDATA[<p>　　文章 <a href="/pruning/" title="pruning">pruning</a> 中详细阐述了模型压缩中 Pruning 的基本方法与理论。Pruning 可分为 Structured Pruning 与 Unstructured Pruning 两种，由于 Structured Pruning 不需要特定的芯片支持，可直接在现有 CPU/GPU 架构下进行加速，所以值得作研究及应用。而 Structured Pruning 主要指 Filter Pruning，以及伴随的 Channel Pruning。本文对近期 Filter Pruning 的进展作一个阐述及思考。<br>
　　<a href="#1" id="1ref">[1]</a> 得出结论：<strong>Pruning 的本质并不应该是选择重要的 filter/channel，而应该是确定 filter/channel 的数量，在此基础上，从零开始训练也能达到原来的性能</strong>。所以 Pruning 其实只是 AutoML/NAS 领域的一个子任务，即用 AutoML/NAS 是能解决 Pruning 问题的，但是 AutoML/NAS 方法又相对复杂且耗时，所以短期内可能传统的预定义剪枝方法更容易得到应用。本文从预定义剪枝方法和自动学习剪枝方法两大块来作归纳思考。</p>
<h2 id="问题描述">1. 问题描述</h2>
<p>　　假设预训练好的网络 \(F\)，其有 \(L\) 层卷积，所有卷积层的 Filter 表示为： <span class="math display">\[ W=\{W^i\} _ {i=1}^L= \left\{\{W^i_j\} _ {j=1}^{c_i}\in\mathbb{R}^{d_i\times c_i}\right\} _ {i=1}^L \tag{1} \]</span> 其中 \(d_i=c_{i-1}\times h_i\times w_i\)；\(c_i,h_i,w_i\) 分别是第 \(i\) 层卷积的 filter 数量，高，宽；\(W_j^i\) 是第 \(i\) 层卷积第 \(j\) 个 filter。<br>
　　目标是搜索被剪枝的网络 \(\mathcal{F}\)，剪枝后的 Filter 表示为： <span class="math display">\[ \mathcal{W}=\{\mathcal{W}^i\} _ {i=1}^L= \left\{\{\mathcal{W}^i_j\} _ {j=1}^{\tilde{c}_i}\in\mathbb{R}^{d_i\times \tilde{c} _ i}\right\} _ {i=1}^L \tag{2} \]</span> 其中 \(\tilde{c} _ i=\lfloor p_i\cdot c_i\rceil\)，\(p_i\) 为 Pruning Rate。<br>
　　Filter Pruning 会导致输出的特征 Channel 数减少，对应的下一层的每个 Filter 参数需要相应的裁剪，如 <a href="/pruning/" title="pruning">pruning</a> 中提到的三种结构下的 Pruning，尤其需要注意后两种有交点的结构，剪枝时需要作一定的约束(为了简单，交点对应的 Filter 可以选择不剪枝)。</p>
<h2 id="预定义剪枝方法">2. 预定义剪枝方法</h2>
<p>　　预定义剪枝网络方法通常预定义的是 \(P=\{p_i\} _ {i=1}^L\)，其剪枝步骤为：</p>
<ol type="1">
<li>Training<br>
根据任务训练网络；</li>
<li>Pruning<br>
设计 Filter 重要性度量准则，然后根据预定义的剪枝率，进行 Filter 剪枝；</li>
<li>Fine-tuning<br>
对剪枝好的网络，进行再训练；</li>
</ol>
<h3 id="soft-filter-pruning212">2.1. Soft Filter Pruning<a href="#2" id="2ref"><sup>[2]</sup></a><a href="#12" id="12ref"><sup>[12]</sup></a></h3>
<p><img src="/Filter-Pruning/soft_filter_pruning.png" width="50%" height="50%" title="图 1. Soft Filter Pruning"> 　　如图 1. 所示，其核心思想就是剪枝后的 Filter 在 Fine-tuning 阶段还是保持更新，由此 Pruning，Fine-tuning 迭代获得较优剪枝结果。Filter 重要性度量准则为： <span class="math display">\[\left\Vert W_j^i\right\Vert _ p = \sqrt[p]{\sum_{cc=0}^{c_{i-1}-1}\sum_{k_1=0}^{h_i-1}\sum_{k_2=0}^{w_i-1}\left\vert W_j^i(cc,k_1,k_2)\right\vert ^p} \tag{3}\]</span></p>
<h3 id="filter-sketch313">2.2. Filter Sketch<a href="#3" id="3ref"><sup>[3]</sup></a><a href="#13" id="13ref"><sup>[13]</sup></a></h3>
<p>　　选择 Filter 进行剪枝，另一种思路是，如何选择一部分 Filter，使得该 Filter 集合的信息量与原 Filter 集合信息量近似: <span class="math display">\[\Sigma_{W^i}\approx \Sigma_{\mathcal{W}^i} \tag{4}\]</span> 这里的信息量表达方式采用了协方差矩阵: <span class="math display">\[\begin{align}
\Sigma_{W^i} &amp;= \left(W^i-\bar{W}^i \right)\left(W^i-\bar{W}^i \right)^T \\
\Sigma_{\mathcal{W}^i} &amp;= \left(\mathcal{W}^i-\mathcal{\bar{W}}^i \right)\left(\mathcal{W}^i-\mathcal{\bar{W}}^i \right)^T \\
\end{align} \tag{5}\]</span> 其中 Filter 权重符合高斯分布，即 \(\bar{W}^i=\frac{1}{c_i}\sum _ {j=1}^{c _ i}W _ j ^ i\approx 0\)，\(\mathcal{\bar{W}} ^ i=\frac{1}{\tilde{c} _ i}\sum _ {j=1}^{\tilde{c} _ i}\mathcal{W} _ j^i\approx 0\)。由式(4)(5)，构建最小化目标函数： <span class="math display">\[\mathop{\arg\min}\limits_{\mathcal{W}^i}\left\Vert W^i(W^i)^T-\mathcal{W}^i(\mathcal{W}^i)^T \right\Vert \tag{6}\]</span> 将该问题转换为求取 \(W^i\) 矩阵的 Sketch 问题，则： <span class="math display">\[\left\Vert W^i(W^i)^T-\mathcal{W}^i(\mathcal{W}^i)^T \right\Vert _F \leq \epsilon\left\Vert W^i\right\Vert^2_F \tag{7}\]</span> <img src="/Filter-Pruning/sketch.png" width="50%" height="50%" title="图 2. Frequent Direction"> <img src="/Filter-Pruning/filter_sketch.png" width="50%" height="50%" title="图 3. FilterSketch"> 　　式(7)可用图 2. 所示的算法求解，最终的 Pruning 算法过程如图 3. 所示，改进的地方主要是 Filter 选择的部分，采用了 Matrix Sketch 算法。 <img src="/Filter-Pruning/pruning.png" width="60%" height="60%" title="图 4. 网络裁剪示意图"> 　　<a href="/pruning/" title="pruning">pruning</a> 中提到有分支结构的裁剪会比较麻烦，所以如图 4. 所示，本方法对分支节点的 Filter 不做裁剪处理，简化了问题。</p>
<h3 id="filter-pruning-via-geometric-median414">2.3. Filter Pruning via Geometric Median<a href="#4" id="4ref"><sup>[4]</sup></a><a href="#14" id="14ref"><sup>[14]</sup></a></h3>
<p>　　在预定义剪枝网络方法的三个步骤中，大家普遍研究步骤二中 Filter 的重要性度量设计。Filter 重要性度量基本是 Smaller-norm-less-informative 思想，<a href="#5" id="5ref">[5]</a> 中则验证了该思想并不一定正确。<strong>Smaller-norm-less-informative 假设成立的条件是</strong>：</p>
<ol type="1">
<li>Filter 权重的规范偏差(norm deviation)要大；</li>
<li>Filter 权重的最小规范要小；</li>
</ol>
<p>只有满足这两个条件，该假设才成立，即可以裁剪掉规范数较小的 Filter。 <img src="/Filter-Pruning/norm_dist.png" width="60%" height="60%" title="图 5. Filter Norm Distribution"> 　　但是，如图 5. 所示，实际 Filter 的权重分布和理想的并不一致，当 Filter 分布是绿色区域时，采用 Smaller-norm-less-informative 就不合理了，而这种情况还比较多。一般性的，前几层网络的权重规范数偏差会比较大，后几层则比较小。<br>
<img src="/Filter-Pruning/criterion.png" width="50%" height="50%" title="图 6. Criterion for Filter Pruning"> 　　由此，本方法提出一种基于 Geometric Median 的 Filter 选择方法，如图 6. 所示，基于 Smaller-norm-less-informative 的裁剪后留下的均是规范数较大的 Filter，这还存在一定的冗余性，本方法则通过物理距离测算，剪掉冗余的 Filter。<strong>另一个角度可理解为最大程度的保留 Filter 集合的大概及具体信息，其思想与 FilterSketch 类似</strong>。<br>
　　根据 Geometric Median 思想，第 \(i\) 层卷积要裁剪掉的 Filter 为： <span class="math display">\[W^i_{j^\ast}=\mathop{\arg\min}\limits_{W^i_{j^\ast}\,|\,j^\ast\in[0,c_i-1]}\sum_{j&#39;=0}^{c_i-1}\left\Vert W^i_{j^\ast}-W^i_{j&#39;}\right\Vert_2 \tag{8}\]</span> 由此裁剪掉满足条件的 \(W _ {j^*}^i\)，直至符合裁剪比率。<strong>本方法的思想非常类似于 Farthest Point Sampling 采样，留下的 Filter 即为原 Filter 集合采样的结果，且最大程度的保留了集合的信息</strong>。</p>
<h2 id="自动学习剪枝方法">3. 自动学习剪枝方法</h2>
<h3 id="abcpruner616">3.1. ABCPruner<a href="#6" id="6ref"><sup>[6]</sup></a><a href="#16" id="16ref"><sup>[16]</sup></a></h3>
<p><img src="/Filter-Pruning/ABCPruner.png" width="60%" height="60%" title="图 7. ABCPruner"> 　　出于<a href="#1" id="1ref">[1]</a>的结论：<strong>剪枝的本质应该是直接找到每层卷积最优的 Filter 数量，在此基础上从零开始训练也能达到原来的性能</strong>。ABCPruner 的目标就是搜索每层最优的 Filter 数量，如图 7. 所示，ABCPruner 步骤为：</p>
<ol type="1">
<li>初始化一系列不同 Filter 数量的网络结构；</li>
<li>每个网络结构从 pre-trained 网络中继承权重值，fine-tune 获得每个网络的 fitness(即 accuracy)；</li>
<li>用 ABC 算法更新网络结构；</li>
<li>重复迭代 2,3 步骤，获取最高的 fitness 网络作为最终网络结构；</li>
</ol>
<h3 id="metapruning717">3.2. MetaPruning<a href="#7" id="7ref"><sup>[7]</sup></a><a href="#17" id="17ref"><sup>[17]</sup></a></h3>
<p><img src="/Filter-Pruning/metapruning.png" width="50%" height="50%" title="图 8. MetaPruning"> 　　同样，本方法也是基于<a href="#1" id="1ref">[1]</a>的结论。这里设计 PruningNet 来控制裁剪，步骤为：</p>
<ol type="1">
<li>Training PruningNet<br>
PruningNet 输入为网络编码向量，即每层卷积的 Filter 数量，输出为产生网络权重的编码量，如 size reshape，crop。每次训练时随机生成网络编码量，网络编码量与 PruningNet 输出共同决定了 PrunedNet 权重，两个网络联合训练；</li>
<li>Searching for the Best Pruned Net<br>
即 Inference 过程，寻找最优的网络编码量，使得 PrunedNet 精度最高；得到最优网络后，不需要 fine-tuning。</li>
</ol>
<h3 id="generative-adversarial-learning8">3.3. Generative Adversarial Learning<a href="#8" id="8ref"><sup>[8]</sup></a></h3>
<p><img src="/Filter-Pruning/GAL.png" width="90%" height="90%" title="图 9. Generative Adversarial Learning"> 　　本方法主要思想来自知识蒸馏(Knowledge Distillation)和生成对抗网络(Generative Adversarial Network)，如图 9. 所示，Baseline 为完整的原始网络，PrunedNet 是为了学习一个 soft mask 来动态选择 block，branch，channel，最终裁剪后的网络由 soft mask 决定。<br>
　　从知识蒸馏的角度：Baseline 就是一个大容量的教师网络，Pruned Net 就是个小容量的学生网络，用大容量网络来监督小容量网络学习。从生成对抗学习的角度：Baseline 是原始网络，PrunedNet 是生成的对抗网络，用一个 Discriminator 网络来区分原始网络与生成的对抗网络的区别，使生成的对抗网络输出逼近于原始网络。</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Liu, Zhuang, et al. "Rethinking the Value of Network Pruning." International Conference on Learning Representations. 2018.<br>
<a id="2" href="#2ref">[2]</a> He, Yang, et al. "Soft filter pruning for accelerating deep convolutional neural networks." arXiv preprint arXiv:1808.06866 (2018).<br>
<a id="3" href="#3ref">[3]</a> Lin, Mingbao, et al. "Filter Sketch for Network Pruning." arXiv preprint arXiv:2001.08514 (2020).<br>
<a id="4" href="#4ref">[4]</a> He, Yang, et al. "Filter pruning via geometric median for deep convolutional neural networks acceleration." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="5" href="#5ref">[5]</a> Ye, Jianbo, et al. "Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers." arXiv preprint arXiv:1802.00124 (2018).<br>
<a id="6" href="#6ref">[6]</a> Lin, Mingbao, et al. "Channel Pruning via Automatic Structure Search." arXiv preprint arXiv:2001.08565 (2020).<br>
<a id="7" href="#7ref">[7]</a> Liu, Zechun, et al. "Metapruning: Meta learning for automatic neural network channel pruning." Proceedings of the IEEE International Conference on Computer Vision. 2019.<br>
<a id="8" href="#8ref">[8]</a> Lin, Shaohui, et al. "Towards optimal structured cnn pruning via generative adversarial learning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="9" href="#9ref">[9]</a> Singh, Pravendra, et al. "Play and prune: Adaptive filter pruning for deep model compression." arXiv preprint arXiv:1905.04446 (2019).<br>
<a id="11" href="#11ref">[11]</a> https://github.com/Eric-mingjie/rethinking-network-pruning<br>
<a id="12" href="#12ref">[12]</a> https://github.com/he-y/softfilter-pruning<br>
<a id="13" href="#13ref">[13]</a> https://github.com/lmbxmu/FilterSketch<br>
<a id="14" href="#14ref">[14]</a> https://github.com/he-y/filter-pruning-geometric-median<br>
<a id="16" href="#16ref">[16]</a> https://github.com/lmbxmu/ABCPruner<br>
<a id="17" href="#17ref">[17]</a> https://github.com/liuzechun/MetaPruning</p>
]]></content>
      <categories>
        <category>Model Compression</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Model Compression</tag>
      </tags>
  </entry>
  <entry>
    <title>Ground Segmentation with Gaussian Process</title>
    <url>/Ground-Segmentation-with-Gaussian-Process/</url>
    <content><![CDATA[<p>　　地面分割可作为自动驾驶系统的一个重要模块，本文介绍一种基于高斯过程的地面分割方法。</p>
<h2 id="算法概要">1. 算法概要</h2>
<p><img src="/Ground-Segmentation-with-Gaussian-Process/ground_seg.png" width="80%" height="80%" title="图 1. ground segmentation"> 　　为了加速，本方法<a href="#1" id="1ref"><sup>[1]</sup></a>将三维地面分割问题分解为多个一维高斯过程来求解，如图 1. 所示，其步骤为：</p>
<ol type="1">
<li><strong>Polar Grid Map</strong><br>
将点云用极坐标栅格地图表示，二维地面估计分解成射线方向的多个一维地面估计；</li>
<li><strong>Line Fitting</strong><br>
在每个一维方向，根据梯度大小，作可变数量的线段拟合；</li>
<li><strong>Seed Estimation</strong><br>
在半径 \(B\) 范围内，如果某个 Grid 绝对高度(Grid 高度定义为该 Grid 内所有点的最小高度，其绝对高度则是与本车传感器所在地面的比较)大于 \(T_s\)，那么就将其作为 Seed；</li>
<li><strong>Ground Model Estimation with Gaussian Process</strong><br>
采用高斯过程生成每个一维方向 Grid 的地面估计量，这里为了进一步加速，可以删除冗余的 Seed；根据地面估计模型，将满足模型的 Grid 加入 Seed，更新模型，迭代直至收敛，满足模型的 Seed 条件为： <span class="math display">\[\begin{align}
V[z]&amp;\leq  t_{model}\\
\frac{|z_*-\bar{z}|}{\sqrt{\sigma^2_n+V[z]}} &amp;\leq t_{data}
\end{align} \tag{0}\]</span></li>
<li><strong>Point-wise Segmentation</strong><br>
得到地面估计模型后，就得到了每个 Grid 是否为地面的标签量，对于属于地面标签量的 Grid 内的点，与 Grid 高度的相对高度小于 \(T_r\)，则认为该点属于地面。</li>
</ol>
<h2 id="高斯过程">2. 高斯过程</h2>
<p>　　步骤四中用高斯过程来估计地面模型，对于每个极射线方向的 Grids，假设有 \(n\) 个已经确定是地面的训练集：\(D=\{(r _ i,z _ i)\} _ {i=1}^n\)。根据高斯过程定义，这些样本的联合概率分布为： <span class="math display">\[p(Z|R)\sim N(f(R)+\mu,K) \tag{1}\]</span> 其中 \(R=[r_1,...,r_n]^T\) 为每个 Grid 的距离量，\(Z=[z_1,...,z_n]^T\) 为该 Grid 地面高度，\(f(\cdot)\)为高斯过程要回归的函数。\(\mu\) 设计为零，协方差矩阵 \(K\) 表示变量之间的关系，由协方差方程与噪音项构成： <span class="math display">\[K(r_i,r_j)=k(r_i,r_j)+\sigma^2_n\delta_{ij}\tag{2}\]</span> 其中当且仅当 \(i==j\) 时 \(\delta _ {ij} =1\)。<br>
　　一般的协方差方程是静态，同向的(stationary, isotropic): <span class="math display">\[k(r_i,r_j)=\sigma_f^2\mathrm{exp}\left(-\frac{(r_i-r_j)^2}{2l^2}\right) \tag{3}\]</span> 其中 \(\sigma_f^2\) 是信号协方差，\(l\) 是 length-scale。该方程假设了全空间内 length-scale 的一致性，然而实际上，<strong>越平坦的地面区域，我们需要越大的 length-scale，因为此时该区域对周围区域的概率输出能更大</strong>，所以可进一步设计协方差方程为: <span class="math display">\[k(r_i,r_j)=\sigma_f^2\left(l_i^2\right)^{\frac{1}{4}}\left(l_j^2\right)^{\frac{1}{4}}\left(\frac{l_i^2+l_j^2}{2}\right)^{-\frac{1}{2}}  \mathrm{exp}\left(-\frac{2(r_i-r_j)^2}{l_i^2+l_j^2}\right) \tag{4}\]</span> 其中 \(l_i\) 为位置 \(r_i\) 的 length-scale。\(l_i\) 由该位置距离最近的线段梯度决定(步骤二): <span class="math display">\[l_i=\left\{\begin{array}{l}
a\cdot \mathrm{log}\left(\frac{1}{|g(r_i)|}\right) \,\, if\, |g(r_i)|&gt;g_{def}\\
a\cdot \mathrm{log}\left(\frac{1}{|g_{def}|}\right) \,\, otherwise
\end{array}\tag{5}\right.\]</span> 　　高斯回归预测的过程为，对于测试集 \(T=(r_\ast,z_\ast)\)，其与训练集的联合概率分布为： <span class="math display">\[\begin{bmatrix}
Z\\
z_\ast\\
\end{bmatrix}\sim
N\left(0,
\begin{bmatrix}
K(R,R) &amp; K(R,r_\ast)\\
K(r_\ast,R) &amp; K(r_\ast,r_\ast)\\
\end{bmatrix}\right)
\tag{6}\]</span> 那么，高斯过程回归预测为： <span class="math display">\[\begin{align}
\bar{z}_\ast &amp;=K(r_\ast,R)K^{-1}Z\\
V[z_\ast] &amp;= K(r_\ast,r_\ast)-K(r_\ast,R)K^{-1}K(R,r_\ast)
\end{align} \tag{7}\]</span> 由此得到测试集的预测量，由式(0)可决定该测试量是否标记为地面，进一步迭代估计地面模型，直至收敛。<br>
　　需要注意的是，以上我们假设高斯过程的超参数 \(\theta=\{\sigma_f,a,\sigma_n\}\) 是已知的，实际应用中，可以将超参数设定为经验量，也可以基于训练集用 SGD 学习出一个最优量，这里不做展开。</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Chen, Tongtong, et al. "Gaussian-process-based real-time ground segmentation for autonomous land vehicles." Journal of Intelligent &amp; Robotic Systems 76.3-4 (2014): 563-582.</p>
]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>autonomous driving</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Grid Mapping</title>
    <url>/Grid-Mapping/</url>
    <content><![CDATA[<p>　　占据栅格地图(Occupied Grid Map)是机器人领域一种地图表示方式。可以作为 SLAM 的一个模块，但是这里讨论：<strong>在本体位姿已知的情况下，如何构建 2D Grid Map</strong>。本文介绍两种方法，贝叶斯概率模型以及高斯过程。</p>
<h2 id="贝叶斯概率模型1">1. 贝叶斯概率模型<a href="#1" id="1ref"><sup>[1]</sup></a></h2>
<p>　　设机器人位姿序列为 \(x_{1:t}\)，观测序列为 \(z_{1:t}\)，那么 Grid Map 的构建就是求解地图的后验概率：\(p(m|x_{1:t},z_{1:t})\)，其中地图由栅格构成：\(m=\{m_1,m_2,...,m_n\}\)。<strong>假设每个栅格独立同分布</strong>，那么： <span class="math display">\[p(m|x_{1:t},z_{1:t})=p(m_1,m_2,...,m_n|x_{1:n},z_{1:t}) = \prod_{i=1}^n p(m_i|x_{1:t},z_{1:t}) \tag{1}\]</span> 　　每个栅格有三种状态：被占有，空，未被观测。设被占有的概率为 \(occ(m_i) = p(m_i|x_{1:t},z_{1:t})\)，那么空的概率为 \(free(m_i)=1-occ(m_i)\)，对于未被观测的区域认为 \(occ(m_i) = free(m_i) =0.5\)。下面通过贝叶斯法则及马尔科夫性推理后验概率计算过程： <span class="math display">\[\begin{align}
occ_t(m_i) &amp;= p(m_i|x_{1:t},z_{1:t}) \\
&amp;= \frac{p(z_t|m_i,x_{1:t},z_{1:t-1})\,p(m_i|x_{1:t},z_{1:t-1})}{p(z_t|x_{1:t},z_{1:t-1})} \\
&amp;= \frac{p(z_t|m_i,x_{t})\,p(m_i|x_{1:t-1},z_{1:t-1})}{p(z_t|x_{1:t},z_{1:t-1})} \\
&amp;= \frac{p(m_i|z_t,x_{t})\,p(z_t|x_t)\,p(m_i|x_{1:t-1},z_{1:t-1})}{p(m_i|x_t)\,p(z_t|x_{1:t},z_{1:t-1})} \\
&amp;= \frac{p(m_i|z_t,x_{t})\,p(z_t|x_t)\,occ_{t-1}(m_{i})}{p(m_i)\,p(z_t|x_{1:t},z_{1:t-1})} \tag{2}
\end{align}\]</span> 对应的栅格为空的概率为： <span class="math display">\[\begin{align}
free_t(\hat{m}_i) &amp;=\frac{p(\hat{m}_i|z_t,x_{t})\,p(z_t|x_t)\,free_{t-1}(\hat{m}_{i})}{p(\hat{m}_i)\,p(z_t|x_{1:t},z_{1:t-1})} \\
&amp;= \frac{(1-p(m_i|z_t,x_{t}))\,p(z_t|x_t)\,(1-occ_{t-1}(m_{i}))}{(1-p(m_i))\,p(z_t|x_{1:t},z_{1:t-1})} \tag{3}
\end{align}\]</span> 由(2),(3)可得： <span class="math display">\[\frac{occ_t(m_i)}{1-occ_t(m_i)} = \frac{1-p(m_i)}{p(m_i)}\cdot\frac{occ_{t-1}(m_i)}{1-occ_{t-1}(m_i)}\cdot\frac{p(m_i|z_t,x_t)}{1-p(m_i|z_t,x_t)}   \tag{4}\]</span> 将上式进行对数化： <span class="math display">\[lm_i^{t} = lm_i^{t-1} + \mathrm{log}\left(\frac{p(m_i|z_t,x_t)}{1-p(m_i|z_t,x_t)}\right) - \mathrm{log}\left(\frac{p(m_i)}{1-p(m_i)}\right) \tag{5}\]</span> 其中 \(p(m_i)\) 表示未观测下其被占有的概率，\(p(m_i|z_t,x_t)\) 表示当前观测下其被占有的概率。比如，考虑到激光点云的测量噪声，我们可以假设如果该栅格有点云，那么 \(p(m_i|z_t,x_t) = 0.9\)；对于激光点光路经过的栅格区域 \(p(m_i|z_t,x_t) = 0.02\)，即 \(p(\hat{m}_i|z_t,x_t) = 0.98\)。<br>
　　该模型下，每个栅格被占有的概率可以转换为前后相加测量量的过程，实际每个栅格被占有的概率为： <span class="math display">\[occ_t(m_i) = \frac{\mathrm{exp}(lm_i^t)}{1+\mathrm{exp}(lm_i^t)} \tag{6}\]</span></p>
<h2 id="高斯过程2">2. 高斯过程<a href="#2" id="2ref"><sup>[2]</sup></a></h2>
<p>　　以上概率模型有个缺陷，其假设栅格独立。实际上栅格并不是独立的，相邻的栅格有很强的相关性。高斯过程则可以处理时域及空域的概率估计与融合问题。<br>
　　高斯过程基本理论在 <a href="/Ground-Segmentation-with-Gaussian-Process/" title="Ground Segmentation with Gaussian Process">Ground Segmentation with Gaussian Process</a> 中已经有较详细阐述，这里作简要概述。假设有训练集 \(\{X_n,y_n\}_{n=1}^N\)，那么高斯过程下其符合分布： <span class="math display">\[y_n=f(X_n)+\epsilon, \epsilon\sim \mathcal{N}(0,\sigma^2) \tag{7}\]</span> 对于测试集，则有： <span class="math display">\[f(X^\ast) = \mathcal{N}(\mu,\sigma) \tag{8}\]</span> 高斯过程对测试集的预测结果为： <span class="math display">\[\begin{align}
\mu^\ast &amp;=K(X^\ast,X)(K(X,X)+\sigma_n^2I)^{-1}y\\
\sigma^\ast &amp;=K(X^\ast,X^\ast) - K(X^\ast,X)(K(X,X)+\sigma_n^2I)^{-1}K(X,X^\ast)
\end{align} \tag{9}\]</span> <img src="/Grid-Mapping/GPOM.png" width="60%" height="60%" title="图 1. GPOM"> 　　高斯过程占据栅格地图(Gaussian Process Occupancy Maps, GPOM)算法过程如图 1. 所示。\(\mathrm{p,r}\) 分别为机器人位姿以及观测量。基本思想就是根据当前时刻的观测数据，提取出正负样本训练集，然后构建高斯模型，对于未观测到的区域，用高斯模型进行预测；每个栅格的信息通过 BCM<a href="#3" id="3ref"><sup>[3]</sup></a> 进行时序的融合，最终采用 logistic 回归得到每个栅格被占据的概率(贝叶斯概率模型中，代替 BCM 及 logistic 的是 log 函数累加融合并求取概率，这里应该也可以用这种方式实现)。<br>
　　可见，高斯过程来求解占据栅格地图，<strong>能融合时序及空间信息</strong>，但是效率会比较低，不过除了高斯过程中的矩阵求逆操作，其它操作基本可以并行化处理。代码可参考<a href="#4" id="4ref">[4]</a>。</p>
<h2 id="reference">3. reference</h2>
<p><a id="1" href="#1ref">[1]</a> Thrun, Sebastian. "Probabilistic robotics." Communications of the ACM 45.3 (2002): 52-57.<br>
<a id="2" href="#2ref">[2]</a> Yuan, Yijun, Haofei Kuang, and Sören Schwertfeger. "Fast Gaussian Process Occupancy Maps." 2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV). IEEE, 2018.<br>
<a id="3" href="#3ref">[3]</a> Tresp, Volker. "A Bayesian committee machine." Neural computation 12.11 (2000): 2719-2741.<br>
<a id="4" href="#4ref">[4]</a> https://github.com/STAR-Center/fastGPOM</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>SLAM</tag>
        <tag>Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Grid-GCN for Fast and Scalable Point Cloud Learning&quot;</title>
    <url>/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/</url>
    <content><![CDATA[<p>　　目前点云特征学习在学术界还处于各种探索阶段，<a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中将点云特征提取分为三维物理空间操作以及映射空间操作两大类，其中对直接在三维空间中提取特征的操作进行了较详细的分析。由于变换到映射空间的操作会相对比较复杂，目前为了实时应用，本人还是比较倾向于直接在三维空间进行操作。<br>
　　类比图像特征提取，直接在三维空间进行点云特征提取的基本操作有：</p>
<ul>
<li><strong>局部点云特征提取</strong>：对目标点的周围点特征进行融合，从而得到该目标点特征；</li>
<li><strong>上采样/下采样</strong>：采样以扩大感受野，进一步提取局部/全局信息；</li>
</ul>
<p>　　<a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 主要描述了已知周围点位置后，局部点云特征的提取方式，考虑的是特征提取的有效性，重写该问题为：针对待提取特征的坐标点 \(\mathcal{x} _ c\)，融合其周围 \(K\) 个点的操作： <span class="math display">\[ \tilde{f_c} = \mathcal{A}\left(\{e(\mathcal{x_i,x_c},f_c, f_i)\ast \mathcal{M}(f_i)\}, i\in1,...,K \right) \tag{1}\]</span> 其中 \(f_i\) 为点 \(\mathcal{x_i}\) 的特征，\(\mathcal{M}\) 为多层感知机；\(e,\mathcal{A}\) 分别为周围点特征权重函数以及特征聚合函数，大致对应 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中的 \(h_\theta\) 以及 \(\Box\)。本文则思考这两个基本操作如何计算加速以能实时应用。具体来看，耗时操作主要是：</p>
<ul>
<li>Sampling</li>
<li>Points Querying</li>
</ul>
<p>　　<a href="#1" id="1ref">[1]</a> 提出了一种基于 Voxel 的快速采样方法，并依赖 Voxel 做近似而快速的 Points Querying，以下作详细分析。</p>
<h2 id="overview">1. Overview</h2>
<p><img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/grid-gcn.png" width="50%" height="50%" title="图 1. Grid-GCN Model"> 　　如图 1. 所示，Grid-GCN 模型目标是提取点级别的特征，从而可以作 semantic segmentation 等任务。基本模块为 GridConv，该模块又包括数据的构建-Coverage-aware Grid Query(CAGQ)，以及图卷积-Grid Context Aggregation(GCA)。 <img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/feature.png" width="80%" height="80%" title="图 2. Grid Context Aggregation"> 　　GCA 操作如图 2. 所示，与 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中介绍的方法都大同小异，当信息量累加到一定程度后，基本只有一两个点的 mAP 差异，这里不作展开。<br>
　　CAGQ 则包含 sampling 与 points querying 两个核心且又最耗时的操作，CAGQ 能极大提升这两个操作的速度。首先定义三维 voxel 大小 \((v_x,v_y,v_z)\)，那么对于点 \(x,y,z\)，其 voxel 索引为 \(Vid(u,v,w)=floor\left(\frac{x}{v_x},\frac{y}{v_y},\frac{z}{v_z}\right)\)，每个 voxel 限制点数量为 \(n_v\)。假设 \(O_v\) 为非空的 voxel 集合，采样 \(M\) 个 voxel \(O_c\subseteq O_v\)。对于每个 voxel \(v_i\)，定义其周围的 voxel 集合为 \(\pi(v_i)\)，该集合中的点则构成 context points。由此可知要解决的问题：</p>
<ul>
<li><strong>Sampling</strong>：采样 voxel 集合 \(O_c\subseteq O_v\)；</li>
<li><strong>Points Querying</strong>：从 Context Points 中选取 K 个点；</li>
</ul>
<h2 id="sampling">2. Sampling</h2>
<p>　　<a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 中大致阐述过几种采样方法，信息保留度较高的方法是 FPS，但是速度较慢。 <img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/sample2query.png" width="80%" height="80%" title="图 3. Sampling and Points Querying"> 　　如图 3. 所示，本文提出了两种基于 voxel 的采样方法:</p>
<ul>
<li><strong>Random Voxel Sampling(RVS)</strong><br>
对每个 voxel 进行随机采样，相比对每个点进行随机采样(Random Point Sampling)，RVS 有更少的信息损失，更广的空间信息覆盖率。</li>
<li><strong>Coverage-Aware Sampling(CAS)</strong><br>
在 RVS 基础上，CAS 有更广的信息覆盖率，其步骤为：
<ol type="1">
<li>随机采样 \(M\) 个 voxel，即执行 RVS；</li>
<li>对未被采样到的 voxel \(v_c\)，计算如果加入这个 voxel，空间覆盖率增益： <span class="math display">\[ H_{add} = \sum_{v\in \pi(v_c)}\delta(C_v) - \beta\frac{C_v}{\lambda} \tag{2}\]</span> 对采样集里面的 voxel \(v_i\)，计算如果去掉这个 voxel，空间覆盖率减少量： <span class="math display">\[ H_{rmv} = \sum_{v\in \pi(v_i)}\delta(C_v-1) \tag{3}\]</span></li>
<li>如果 \(H_{add} &gt; H_{rmv}\)，则进行替换；</li>
<li>迭代 2,3 步骤；</li>
</ol></li>
</ul>
<p>其中 \(\delta(x)=1,if x=0,else\,0\)。\(\lambda\) 为周围 voxel 个数，\(C_v\) 是采样集覆盖该 voxel 的个数。</p>
<h2 id="points-querying">3. Points Querying</h2>
<p>　　传统的 Points Querying 一般是在所有点中建立 KD-Tree 或 Ball Query 形式来找某点的邻近点。本文在 voxel 基础上来快速寻找邻近点，提供了两种方法：</p>
<ul>
<li><strong>Cube Query</strong><br>
这是一种近似法，直接在 Context Points 中随机采样 \(K\) 个点作为最近邻点。从物理意义上将，最近邻的区域的点特征应该都是相似的，所以这种近似法应该会很有效。</li>
<li><strong>K-Nearest Neighbors</strong><br>
在 Context Points 中寻找 K-NN，相比在全点云中找 K-NN，这种方法搜索速度会非常快。</li>
</ul>
<h2 id="experiments">4. Experiments</h2>
<p><img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/complexity.png" width="60%" height="60%" title="图 4. 时间复杂度"> <img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/time-eval.png" width="70%" height="70%" title="图 5. 空间覆盖率与耗时"> 　　如图 4. 与图 5. 所示，比较了 RPS，FPS，RVS，CAS 等采样算法的时间复杂度与空间覆盖率，以及 Ball Query，Cube Query，K-NN 等 Points Query 算法的时间复杂度。由此可见，本文提出的 Sample 及 Points Query 算法非常高效。</p>
<h2 id="reference">5. reference</h2>
<p><a id="1" href="#1ref">[1]</a> Xu, Qiangeng. "Grid-GCN for Fast and Scalable Point Cloud Learning." arXiv preprint arXiv:1912.02984 (2019).</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Epistemic Uncertainty for Active Learning</title>
    <url>/Epistemic-Uncertainty-for-Active-Learning/</url>
    <content><![CDATA[<p>　　<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中详细讨论了 Aleatoric Uncertainty 的建模以及应用。本文讨论 Epistemic Uncertainty 的建模，以及在 Active Learning 中的应用。Epistemic Uncertainty 描述了模型因为缺少训练数据而存在的不确定性，所以其可应用于 Active Learning。应用场景有：</p>
<ul>
<li><strong>减少训练时间</strong>：在大数据集下，训练时挑选当前模型认知困难的样本，减少训练数据从而减少训练时间；</li>
<li><strong>减少无效标注</strong>：只挑选当前模型认知困难的样本进行标注、迭代模型；</li>
</ul>
<p><img src="/Epistemic-Uncertainty-for-Active-Learning/active_learning.png" width="50%" height="50%" title="图 1. active learning 工作流"> 　　<a href="#1" id="1ref">[1]</a>中提到的一种 Active Learning 工作流如图 1. 所示，重要环节有 Estimating Uncertainty 以及 Querying Data。该工作流假设了<strong>一个完美的图像检测器(至少有个完美的召回率)</strong>，图像检测器提供目标 proposal，3D 检测对 proposal 作 uncertainty 估计，从而确定是否标注。 Estimating Uncertainty 指的是 Epistemic Uncertainty 的建模；Querying Data 则设计一种策略，其能通过估计的 Uncertainty 来选择模型认知困难的样本。<br>
　　由于 Epistemic Uncertainty 只能通过 Monte-Carlo 等方法近似得到，这些方法都是基于模型预测的目标进行 Uncertainty 估计的，所以对于漏检的目标，其 Uncertainty 是无法有效获取的。换句话说，本文讨论的 Epistemic Uncertainty 只能抓取预测的正样本(TP)置信度不高，以及误检(FP)的 Uncertainty 信息，无法获得TP置信度非常低的样本 Uncertainty，即完全没见过的目标。<strong>所以基于 Epistemic Uncertainty 的 Active Learning，理论上只能使正样本置信度提高，以及消除误检；对于漏检，需要加入一定的随机性，让模型先“见到”这种类型的目标。</strong></p>
<h2 id="estimating-epistemic-uncertainty">1. Estimating Epistemic Uncertainty</h2>
<p>　　针对一批训练数据集\(\{\mathbf{X,Y}\}\)，训练模型 \(\mathbf{y=f^W(x)}\)，在贝叶斯框架下，预测量的后验分布为<a href="#3" id="3ref"><sup>[3]</sup></a>： <span class="math display">\[p\left(\mathbf{y\vert x,X,Y}\right) = \int p\left(\mathbf{y\,|\,f^W(x)}\right) p\left(\mathbf{W\,|\,X,Y}\right)d\mathbf{W} \tag{1}\]</span> 其中 \(p(\mathbf{W\,|\,X,Y})\) 为模型参数的后验分布，描述了模型的不确定性，即 Epistemic Uncertainty；\(p\left(\mathbf{y\,|\,f^W(x)}\right)\) 为观测似然，描述了观测不确定性，即Aleatoric Uncertainty。接下来讨论如何计算 Epistemic Uncertainty。</p>
<h3 id="分类问题">1.1. 分类问题</h3>
<p><img src="/Epistemic-Uncertainty-for-Active-Learning/softmax.png" width="60%" height="60%" title="图 2. softmax for unseen data"> 　　如图 2. 所示<a href="#2" id="2ref"><sup>[2]</sup></a>，softmax 可能会对没见过的目标产生较高的概率输出(如误检)。所以不能直接使用分类的概率输出作为 Uncertainty 估计。</p>
<ul>
<li><strong>Monte-Carlo Dropout</strong><br>
<a href="#2" id="2ref">[2]</a>中提出了 Monte-Carlo 近似求解 Epistemic Uncertainty 的方法，其指出：在训练阶段，Dropout 等价于优化网络权重 \(W\) 的 Bernoulli 分布；在测试阶段，使用 Dropout 对样本进行多次测试，能得到模型权重的后验分布，即 Epistemic Uncertainty。由此得到： <span class="math display">\[p(\mathbf{y|x}) \approx \frac{1}{T}\sum^T_{t=1} p(\mathbf{y|x,W}_t) = \frac{1}{T}\sum^T_{t=1}softmax_{(\mathbf{W}_t)}(\mathbf{x}) \tag{2}\]</span> 其中 \(\mathbf{W}_t\) 为第 \(t\) 次 Inference 网络权重。</li>
<li><strong>Deep Ensembles</strong><br>
Deep Ensemble 则是一种非贝叶斯的方法，该方法用不同的初始化方法训练一系列网络 \(\{\mathbf{M} _ e\} _ {e=1}^E\)。那么： <span class="math display">\[p(\mathbf{y|x}) \approx \frac{1}{E}\sum^E_{e=1} p(\mathbf{y|x,M}_e) = \frac{1}{E}\sum^E_{e=1}softmax_{(\mathbf{M}_e)}(\mathbf{x}) \tag{3}\]</span></li>
</ul>
<p>　　有了预测的概率后，可用 Shannon Entropy 或者 Mutual Information 来计算目标的信息量，即 Uncertainty。</p>
<ul>
<li><strong>Shannon Entropy(SE)</strong><br>
SE 计算公式为: <span class="math display">\[\mathcal{H}[\mathbf{y|x}] = -\sum^C_{c=1}p(y=c|\mathbf{x})\,\mathrm{log}\,p(y=c|\mathbf{x}) \tag{4}\]</span></li>
<li><strong>Mutual Information(MI)</strong><br>
由于 Monte-Carlo 以及 Deep Ensembles 获取的是概率分布，以 Monte-Carlo 为例，由此可计算 MI： <span class="math display">\[\mathcal{I}[\mathbf{y;W}] = \mathcal{H}[\mathbf{y|x}] - \mathbb{E}\mathcal{H}[\mathbf{y|x,W}] \approx \mathcal{H}[\mathbf{y|x}] + \frac{1}{T}\sum_{t=1}^T\sum_{c=1}^Cp(y=c|\mathbf{x,W}_t)\,\mathrm{log}\,p(y=c|\mathbf{x,W}_t) \tag{5}\]</span></li>
</ul>
<p>　　SE 测量的是预测 Uncertainty，MI 测量的是模型对该数据的 Uncertainty。根据该 Uncertainty，即可挑选样本进行标注。Uncertainty 越高，代表该样本对模型的信息量更大，所以需要进一步标注来训练模型。</p>
<h3 id="回归问题">1.2. 回归问题</h3>
<p>　　Monte-Carlo 采样下，假设获得的回归量为 \(\{\mathbf{v}\}_{t=1}^T\)。那么其均值和方差为： <span class="math display">\[\left\{\begin{array}{l}
\mathcal{M}_{\mathbf{v}} \approx \frac{1}{T}\sum_{t=1}^T\mathbf{v}_t \\
\mathcal{C}_{\mathbf{v}} = \frac{1}{T}\sum_{t=1}^T\mathbf{v}_t\mathbf{v}_t^T-\mathcal{M}_{\mathbf{v}}\mathcal{M}_{\mathbf{v}}^T
\end{array}\tag{6}\right.\]</span> 由此得到回归量的 Uncertainty： <span class="math display">\[TV_{\mathbf{v}} = trace\left(\mathcal{C}_{\mathbf{v}} \right) \tag{7}\]</span> 该 Uncertainty 越大，说明该数据对模型的信息也越多，所以可进一步标注训练。</p>
<h2 id="metrics">2. Metrics</h2>
<p>TODO</p>
<h2 id="reference">3. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. "Deep active learning for efficient training of a lidar 3d object detector." arXiv preprint arXiv:1901.10609 (2019).<br>
<a id="2" href="#2ref">[2]</a> Gal, Yarin. Uncertainty in deep learning. Diss. PhD thesis, University of Cambridge, 2016.<br>
<a id="3" href="#1ref">[3]</a> Feng, Di, Lars Rosenbaum, and Klaus Dietmayer. "Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection." 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018.</p>
]]></content>
      <categories>
        <category>Uncertainty</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>Heteroscedastic Aleatoric Uncertainty</title>
    <url>/Heteroscedastic-Aleatoric-Uncertainty/</url>
    <content><![CDATA[<p>　　<a href="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/" title="Multi-task Learning Using Uncertainty to Weigh Losses">Multi-task Learning Using Uncertainty to Weigh Losses</a> 已经详细描述了贝叶斯模型中几种可建模的不确定性(uncertainty)，并应用了<strong>任务依赖/同方差不确定性(Task-dependent or Homoscedastic Aleatoric Uncertainty)</strong>来自动学习多任务中的 Loss 权重。本文讨论同为偶然不确定性(Aleatoric Uncertainty)的<strong>数据依赖/异方差不确定性(Data-dependent or Heteroscedastic Aleatoric Uncertainty)</strong>。需要注意的是，偶然不确定性(Aleatoric Uncertainty)描述的是数据不能解释的信息，只能通过提高数据的精度来消除；而认知不确定性(Epistemic Uncertainty)描述的是模型因为缺少训练数据而存在的未知，可通过增加训练数据解决。<br>
　　为什么要建模 Heteroscedastic Aleatoric Uncertainty？Learning 算法一个比较致命的问题是，网络能输出预测量，但是网络不知道其预测的不确定性，如目标状态估计中，需要获得观测的协方差矩阵<strong>(检测作为观测模块，理论上需要出检测的 Uncertainty，包括 Aleatoric 与 Epistemic Uncertainty，但是 Epistemic Uncertainty 只能通过多次采样近似得到，不能实时应用，所以一般只考虑 Aleatoric Uncertainty 作为观测的不确定性)</strong>。尤其在自动驾驶领域，<strong>我们不仅关注模型知道什么，更要关注模型不知道什么</strong>。<br>
　　本文通过贝叶斯神经网络来建模 Aleatoric Uncertainty，并分析其应用效果。</p>
<h2 id="aleatoric-uncertainty-建模">1. Aleatoric Uncertainty 建模</h2>
<p>　　针对一批训练数据集\(\{\mathbf{X,Y}\}\)，训练模型 \(\mathbf{y=f^W(x)}\)，在贝叶斯框架下，预测量的后验分布为： <span class="math display">\[p\left(\mathbf{y\vert x,X,Y}\right) = \int p\left(\mathbf{y\,|\,f^W(x)}\right) p\left(\mathbf{W\,|\,X,Y}\right)d\mathbf{W} \tag{0}\]</span> 其中 \(p(\mathbf{W\,|\,X,Y})\) 为模型参数的后验分布，描述了模型的不确定性，即 Epistemic Uncertainty；\(p\left(\mathbf{y\,|\,f^W(x)}\right)\) 为观测似然，描述了观测不确定性，即Aleatoric Uncertainty。Epistemic Uncertainty 只能通过近似推断获得，本文不作讨论。<br>
　　<a href="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/" title="Multi-task Learning Using Uncertainty to Weigh Losses">Multi-task Learning Using Uncertainty to Weigh Losses</a> 已经详细推导了 Aleatoric Uncertainty 的建模过程，这里摘抄如下：</p>
<p><span class="math display">\[\mathcal{L}(\mathbf{W}, s_1, s_2) = \frac{1}{2}\mathrm{exp}(-s_1)\mathcal{L}_1(\mathbf{W}) + \mathrm{exp}(-s_2)\mathcal{L}_2(\mathbf{W}) + \mathrm{exp}(\frac{1}{2}s_1) + \mathrm{exp}(\frac{1}{2}s_2) \tag{1}\]</span> 其中 \(\mathcal{L}(\mathbf{W},s_1)\) 为回归项，\(\mathcal{L}(\mathbf{W},s_2)\) 为分类项。<br>
　　<a href="#1" id="1ref">[1]</a><a href="#2" id="2ref">[2]</a><a href="#3" id="3ref">[3]</a> 中建模的回归项 loss uncertainty 与式(1)有细微出入(可以认为是 Uncertainty 的正则项不同，但是效果类似)，其负log似然为： <span class="math display">\[-\mathrm{log}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) \propto \frac{1}{2\sigma ^2} \Vert \mathbf{y-f^W(x)} \Vert ^2 + \frac{1}{2}\mathrm{log}\sigma^2 \tag{2}\]</span> 所以其回归项 loss 为： <span class="math display">\[\mathcal{L}(\mathbf{W}, s_1) = \frac{1}{2}\mathrm{exp}(-s_1)\mathcal{L}_1(\mathbf{W}) + \frac{1}{2}s_1 \tag{3}\]</span></p>
<h3 id="d-object-detection-by-regressing-corners2">1.1. 3D Object Detection by regressing corners<a href="#2" id="2ref"><sup>[2]</sup></a></h3>
<p>　　该方案是在俯视图下回归 3D 框的 8 个角点，总共 24 个参数。假设观测为多变量的高斯分布，即： <span class="math display">\[\left\{\begin{array}{l}
p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \mathcal{N}\left(\mathbf{f^W(x)}, \Sigma(\mathbf{x}) \right) \\
\Sigma(\mathbf{x}) = diag(\sigma _ {\mathbf{x}}^2)
\end{array}\tag{4}\right.\]</span> 其中 \(\mathbf{y}\) 是预测的目标框参数，\(\sigma _ {\mathbf{x}}^2\) 是 24 维的向量，表示了观测数据的噪声水平，由式(3)可知，噪声越大，其对 Loss 的作用越小。</p>
<p><img src="/Heteroscedastic-Aleatoric-Uncertainty/Aleatoric.png" width="60%" height="60%" title="图 1. Aleatoric Uncertainty 与 3D corner 关系"> 　　如图 1. 所示，同一目标，靠近本车的 corner 点，其 Aleatoric Uncertainty 越小；距离越远，目标被遮挡的越严重，其 Aleatoric Uncertainty 越高。</p>
<h3 id="d-object-detection-by-regressing-location-and-orientation-3">1.2. 3D Object Detection by regressing location and orientation <a href="#3" id="3ref"><sup>[3]</sup></a></h3>
<p><img src="/Heteroscedastic-Aleatoric-Uncertainty/regression_uncert.png" width="80%" height="80%" title="图 2. network arch"> 　　如图 2. 所示，网络结构比较简单，这里建模了三种 uncertainty: RPN bbox regression \(\sigma^2_{\mathbf{t_r}}\)；Head 中的 location \(\sigma^2_{\mathbf{t_v}}\)；Head 中的 orientation \(\sigma^2_{\mathbf{r_v}}\)。最终的 Loss 由三项式(3) 以及两项分类 loss 构成。<br>
<img src="/Heteroscedastic-Aleatoric-Uncertainty/Aleatoric_Uncert.png" width="80%" height="80%" title="图 3. Aleatoric Uncertainty 与目标状态关系"> 　　如图 3. 所示，TV(Total Variance) 与目标状态的关系。对于距离越远，遮挡越严重的目标，其 Aleatoric Uncertainty 会越高，因为其观测到的点云会比较少。</p>
<h3 id="semantic-segmentation-1">1.3. Semantic Segmentation <a href="#1" id="1ref"><sup>[1]</sup></a></h3>
<p><img src="/Heteroscedastic-Aleatoric-Uncertainty/Aleatoric_Epistemic.png" width="60%" height="60%" title="图 4. Aleatoric Uncertainty 在语义分割中的关系"> 　　如图 4. 所示，Aleatoric Uncertainty 在远处，边缘处较大；而 Epistemic Uncertainty 对没见过的数据/区域较大。</p>
<h2 id="aleatoric-uncertainty-预测">2. Aleatoric Uncertainty 预测</h2>
<p>　　<a href="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/" title="Multi-task Learning Using Uncertainty to Weigh Losses">Multi-task Learning Using Uncertainty to Weigh Losses</a> 中 Uncertainty 不需要作为预测输出，可将其设计为网络的 weights，且每个任务都设计为单变量高斯分布的形式。<a href="#2" id="2ref">[2]</a><a href="#3" id="3ref">[3]</a> 中则将 Uncertainty 设计为网络的输出，且是多变量高斯分布。更一般的，假设模型输出为混合高斯分布： <span class="math display">\[\left\{\begin{array}{l}
p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \sum_k \alpha_k \mathcal{N}\left(\mathbf{f^W(x)}_{(k)}, \Sigma(\mathbf{x})_{(k)} \right)\\
\sum_k \alpha_k = 1
\end{array}\tag{5}\right.\]</span> 　　对于 3D Detection 问题，网络输出的 3D 框参数为 \(\mathbf{y}=(x,y,z,l,h,w,\theta)\)，当输出满足 \(K\) 个混合高斯分布时，网络的输出量有：</p>
<ul>
<li>\(K\) 组目标框参数预测量 \(\{\mathbf{y}_k\}\)；</li>
<li>\(K\) 个对数方差 \(\{s_k\}\)；</li>
<li>\(K\) 个混合高斯模型权重参数 \(\{\alpha_k\}\)；</li>
</ul>
<p>　　训练时，找出与真值分布最近的一组预测量，混合高斯模型权重用 softmax 回归并用 cross-entropy loss，找到最相似的分布后，将该分布的方差用式(3)作用于回归的 Loss 项；测试时，找到混合高斯模型最大的权重项，对应的高斯分布，即作为最终的输出分布。这里只考虑了输出 3D 框的一个整体的方差，也可以输出定位方差+尺寸方差+角度方差，只要将该方差作用于对应的 Loss 项即可。当 \(K=1\) 时，就是多变量单高斯模型，一般也够用。</p>
<h2 id="metrics">3. Metrics</h2>
<p>TODO</p>
<h2 id="reference">4. Reference</h2>
<p><a id="1" href="#1ref">[1]</a> Kendall, Alex, and Yarin Gal. "What uncertainties do we need in bayesian deep learning for computer vision?." Advances in neural information processing systems. 2017.<br>
<a id="2" href="#2ref">[2]</a> Feng, Di, Lars Rosenbaum, and Klaus Dietmayer. "Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection." 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018.<br>
<a id="3" href="#3ref">[3]</a> Feng, Di, et al. "Leveraging heteroscedastic aleatoric uncertainties for robust real-time lidar 3d object detection." 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019.</p>
]]></content>
      <categories>
        <category>Uncertainty</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>Object Registration with Point Cloud</title>
    <url>/Object-Registration-with-Point-Cloud/</url>
    <content><![CDATA[<p>　　<a href="/ADH-Tracker/" title="ADH Tracker">ADH Tracker</a> 通过 ADH 方法有效得在两目标点云的 T 变换的解空间中搜索出高概率解集，并用简单的运动模型，在贝叶斯概率框架下进行目标状态(位置，速度)的估计。这其中关键的环节还是两目标点云之间变换关系 \((R,T)\) 的求解，即 Object Registration。<br>
　　求解两点云之间的位姿关系，传统的做法是 ICP。以 ICP 为代表的方法大多数都是迭代法，本文介绍两种 learning-based 点云注册方法。</p>
<h2 id="deep-closet-point1">1. Deep Closet Point<a href="#1" id="1ref"><sup>[1]</sup></a></h2>
<h3 id="icp-描述">1.1. ICP 描述</h3>
<p>　　假设两个点云集：\(\mathcal{X}=\{x _ 1,...,x _ i,...,x _ N\}\in\mathbb{R}^3\)，\(\mathcal{Y}=\{y _ 1,...,y _ j,...y _ M\}\in\mathbb{R}^3\)。两个点集之间的变换为 \(R,t\)，定义点集匹配的误差函数： <span class="math display">\[ E(R,t) = \frac{1}{N}\sum_i^N\Vert Rx_i+t-y _ {m(x_i)}\Vert \tag{1}\]</span> 其中 \(y_{m(x_i)}\) 为 \(x_i\) 经过变换后匹配上的最近点，即： <span class="math display">\[ m(x_i,\mathcal{Y}) = \mathop{\arg\min}_j\Vert Rx_i+t-y_j\Vert \tag{2}\]</span> 定义点云重心：\(\bar{x}=\frac{1}{N}\sum _ {i=1}^Nx _ i\)，\(\bar{y}=\frac{1}{M}\sum _ {j=1}^Ny _ j\)。计算 Cross-covariance 矩阵： <span class="math display">\[ H = \sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y}) \tag{3}\]</span> \(R,t\) 变换可通过 \(H=USV^T\) 最小化误差函数 \(E(R,t)\) 实现： <span class="math display">\[\left\{\begin{array}{l}
R= VU^T\\
t= -R\bar{x}+\bar{y}
\end{array}\tag{4}\right.\]</span> ICP 算法就是迭代得求解式(2)与式(1)的过程。</p>
<h3 id="网络结构">1.2. 网络结构</h3>
<p><img src="/Object-Registration-with-Point-Cloud/DCP.png" width="80%" height="80%" title="图 1. DCP"> 　　如图 1. 所示，DCP 网络结构由三部分组成：</p>
<ul>
<li><strong>Embedding Module</strong><br>
特征提取层，可以用 PointNet，也可以用 DGCNN 网络(<a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a>)，DGCNN 能更有效的提取局部特征。</li>
<li><strong>Transformer</strong><br>
该模块基于 Attention 机制，详情可参考<a href="#3" id="3ref">[3]</a><a href="#4" id="4ref">[4]</a>。</li>
<li><strong>Head</strong><br>
该模块用于预测 \((R,t)\)，可以简单的用 MLP 回归，也可以用 SVD 层来预测，因为 Transformer 会输出 \(x_i\) 在 \(\mathcal{Y}\) 中的匹配点。</li>
</ul>
<h3 id="loss">1.3. Loss</h3>
<p>　　Loss 比较简单，也是基于有监督的学习： <span class="math display">\[ Loss = \Vert R^TR_g-I\Vert ^2 + \Vert t-t_g\Vert ^2 + \lambda \Vert\theta\Vert ^2\]</span></p>
<h2 id="alignnet-3d2">2. AlignNet-3D<a href="#2" id="2ref"><sup>[2]</sup></a></h2>
<h3 id="网络结构-1">2.1. 网络结构</h3>
<p><img src="/Object-Registration-with-Point-Cloud/AlignNet.png" width="60%" height="60%" title="图 2. AlignNet"> 　　如图 2. 所示，AlignNet 由两个网络组成：</p>
<ul>
<li><strong>CanonicalNet</strong><br>
CanonicalNet 作用是预测点集目标3D框的中心点坐标系，从而将点集坐标转换到中心点坐标系。预测点集目标3D框的中心点坐标系通过 coarse-to-fine 方式实现，stage1(T-CoarseNet) 只粗略预测中心点的位置信息，stage2(T-FineNet) 预测中心点位置相对 Stage1 的残差，以及中心点坐标系的旋转量。参考以前的方法，旋转量通过角度区域分类＋残差实现。通过该网络，每个点集的坐标均在各自目标框中心点坐标系下，能直观的反应目标的形状。</li>
<li><strong>Head</strong><br>
Head(stage3) 则将两个点集特征聚合，预测各中心点坐标系下两个点集的相对位姿。<br>
设点集 \(s_1\) 经过 CanonicalNet 预测的变换为 \(T_1\)，\(s_2\) 对应的变换为 \(T_2\)，stage3 预测的两者的变换为 \(T_f\)，那么最终得到的两个点集的变换为 \(T_1T_fT_2^{-1}\)。</li>
</ul>
<h3 id="loss-1">2.2. Loss</h3>
<p>　　stage1 预测了 translation，stage2/stage3 预测了 translation 和 rotation，总的 Loss 为： <span class="math display">\[\begin{align}
L &amp;= L_{trans,overall}+\lambda_2\cdot L_{angle,overall}\\
  &amp;= \lambda_1(L_{trans,s1}+L_{trans,s2}) + L_{trans,s3} + \lambda_2(\lambda_1L_{angle,s2}+L_{angle,s3})
\end{align}\]</span> stage1/stage2 预测的目标框中心点坐标系(包括中心点坐标及目标框的朝向)真值由点云所构成的目标框提供。</p>
<h3 id="不足点">2.3. 不足点</h3>
<p>　　这种级联式的方法，思想是非常好的，将两个点集的相对位姿分解为两大部来求解，即先将点集转换到中心点坐标系，然后再求解点集剩下位姿残差，coarse-to-fine，能较好回归且收敛。<br>
　　但是存在一些问题。我们假设两个点集作为同一刚性目标，其3D框没有偏差(标注非常准)，那么 CanonicalNet 出来结果，已经可以作为相对位姿结果。但是标注肯定会有抖动(除非是生成的数据)，可以认为是高斯分布，以及获取点云的传感器的测量噪音，这样的话，看起来 stage3 就是只用来拟合这种均值为 0 的高斯分布了。<br>
　　所以本方法对生成的数据与真实的数据，存在一定的偏差，因为目标框真值的抖动分布不一致。这样的话在生成的数据上训练的网络，直接迁移到真实数据中，可能性能会下降比较明显，反之可能还好。</p>
<h2 id="参考文献">3. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Wang, Yue, and Justin M. Solomon. "Deep Closest Point: Learning Representations for Point Cloud Registration." arXiv preprint arXiv:1905.03304 (2019).<br>
<a id="2" href="#2ref">[2]</a> Groß, Johannes, Aljoša Ošep, and Bastian Leibe. "AlignNet-3D: Fast Point Cloud Registration of Partially Observed Objects." 2019 International Conference on 3D Vision (3DV). IEEE, 2019.<br>
<a id="3" href="#3ref">[3]</a> Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.<br>
<a id="4" href="#4ref">[4]</a> https://zhuanlan.zhihu.com/p/48508221</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>ICP</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>ADH(Annealed Dynamic Histograms) Tracker</title>
    <url>/ADH-Tracker/</url>
    <content><![CDATA[<p>　　<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>中详细推导了卡尔曼滤波及其扩展卡尔曼滤波基于贝叶斯的推导过程。由贝叶斯法则式(7)，<strong>状态估计问题可定义为：已知似然及先验概率，最大化后验概率的过程</strong>。其中先验即为“运动学模型(motion model)”，似然即为“观测”，后验概率即为待估计的状态量。对于卡尔曼滤波，对应了式(1)的运动方程及测量方程。<br>
　　用扩展卡尔曼滤波来估计目标状态的原理可见<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>。该文重点讨论基于质点的一系列运动学模型，以及基于刚体的前转向车模型；测量模型则没做深入研究，默认是目标重心级别的测量量。比如，观测量如果是三维框，那么自然可得到目标的位置，相减就得到速度的观测量。<br>
　　但是基于点云的目标检测中，目标的观测量更准确的应该是点集(cluster)。<strong>如何在贝叶斯框架下，定义点集的运动学模型及观测模型</strong>，对提高目标状态的估计显得尤其重要。ADH Tracker<a href="#1" id="1ref"><sup>[1]</sup></a> 就是一种点集状态估计方法，其描述了一种可跟踪目标表面形状特性的概率模型，本文主要阐述 ADH Tracker 的原理及实现细节。</p>
<h2 id="点集状态估计的概率模型">1. 点集状态估计的概率模型</h2>
<h3 id="贝叶斯框架">1.1. 贝叶斯框架</h3>
<p><img src="/ADH-Tracker/bayesian.png" width="50%" height="50%" title="图 1. 点集状态估计的贝叶斯概率模型"> 　　如图 1. 所示，状态量为 \(x_t\)，点集状态为 \(s_t\)，测量/观测量为 \(z_t\)，\(s_t\) 表示为从目标点集中采样的点集。 <img src="/ADH-Tracker/gaussian.png" width="50%" height="50%" title="图 2. 传感器噪声"> 　　如图 2. 所示，由于传感器的噪声 \(\Sigma_e\)，实际的目标上的点集 \(s_t\) 需要加上传感器噪声，以及目标的当前位置，才是最终的观测量点集 \(z_t\): <span class="math display">\[z_{t,j} \sim \mathcal{N}(s_{t,j},\Sigma_e) + x_{t,p}  \tag{1}\]</span> 注意坐标系是在前一时刻目标的中心，状态量中的位置是相对位置，所以前一时刻目标点服从分布： <span class="math display">\[z_{t-1,i} \sim \mathcal{N}(s_{t-1,i},\Sigma_e)  \tag{2}\]</span> 图 1. 的贝叶斯模型下： <span class="math display">\[p(z_{t-1}|x_t,s_{t-1}) = p(z_{t-1}|s_{t-1}) \tag{3}\]</span> 由于目标的遮挡等位置变换，目标上的点集 \(s_t\) 又是随时间变化的，假设 \(p(V)\) 表示当前时刻点集从前一时刻点集采样的先验概率，那么当前时刻每个点从前一时刻采样的概率为： <span class="math display">\[p(s_{t,j}|s_{t-1}) = p(V)p(s_{t,j}|s_{t-1},V) + p(\neg V)p(s_{t,j}|s_{t-1},\neg V) \tag{4}\]</span> 假设当前点在前一时刻不可见的均为被遮挡的情况，那么： <span class="math display">\[p(s_{t,j}|s_{t-1},\neg V) = k_1(k_2-(s_{t,j}|s_{t-1},V))\]</span> 合并可得： <span class="math display">\[p(s_{t,j}|s_{t-1}) = \eta(p(s_{t,j}|s_{t-1},V) +k) \tag{5}\]</span></p>
<h3 id="状态估计问题">1.2. 状态估计问题</h3>
<p>　　式(1)~(5)描述了该贝叶斯网络下各变量之间的关系，状态估计求解的目标是：在所有观测量的基础上估计当前状态，即\(p(x_t|z_1...z_t)\)。根据贝叶斯法则： <span class="math display">\[p(x_t|z_1...z_t)=\eta\; p(z_t|x_t,z_1...z_{t-1}) p(x_t|z_1...z_{t-1}) \tag{6}\]</span> 其中 \(\eta\) 为归一化常数，<strong>第一项是观测模型，第二项是运动模型</strong>。如果依据条件独立，观测模型则可简化为： <span class="math display">\[p(z_t|x_t,z_1...z_{t-1}) = p(z_t|x_t)\]</span> 但是这里考虑到 \(s_t\) 均是从同一目标采样的，所以条件独立性不成立，将观测模型简化近似为： <span class="math display">\[p(z_t|x_t,z_1...z_{t-1}) \approx p(z_t|x_t,z_{t-1}) \tag{7}\]</span> 直观上理解为，当前观测不仅依赖当前状态，还依赖上一时刻的观测量。</p>
<h2 id="adh-tracker-观测模型">2. ADH Tracker 观测模型</h2>
<p>　　观测模型式(7)可重写为： <span class="math display">\[\begin{align}
p(z_t|x_t,z_{t-1}) &amp;= \int p(z_t,s_t|x_t,z_{t-1})ds_t \\
&amp;= \int p(z_t|s_t,x_t)p(s_t|x_t,z_{t-1})ds_t \\
&amp;= \int p(z_t|s_t,x_t)\left(\int p(s_t,s_{t-1}|x_t,z_{t-1})ds_{t-1}\right)ds_t \\
&amp;= \int p(z_t|s_t,x_t)\left(\int p(s_t|s_{t-1})p(s_{t-1}|x_t,z_{t-1})ds_{t-1}\right)ds_t \\
&amp;= \int p(z_t|s_t,x_t)\left(\int \eta\;p(s_t|s_{t-1})p(z_{t-1}|x_t,s_{t-1})p(s_{t-1})ds_{t-1}\right)ds_t \\
&amp;= \int p(z_t|s_t,x_t)\left(\int \eta\;p(s_t|s_{t-1})p(z_{t-1}|s_{t-1})p(s_{t-1})ds_{t-1}\right)ds_t
\tag{8}
\end{align}\]</span> 式(1)(2)(5)可得高斯模型: <span class="math display">\[\left\{\begin{array}{l}
p(z_t|s_t,x_t) = \mathcal{N}(z_t;s_t+x_{t,p},\Sigma_e) \\
p(z_{t-1}|s_{t-1}) = \mathcal{N}(z_{t-1};s_{t-1},\Sigma_e) \\ 
p(s_t|s_{t-1}) = \eta\left(\mathcal{N}(s_{t};s_{t-1},\Sigma_r)+k \right) \\ 
\end{array}\tag{9}\right.\]</span> 其中 \(\Sigma_e \) 为传感器噪声方差，\(\Sigma_r\) 为传感器不同距离的分辨率。因为两个高斯分布相乘还是高斯分布，所以由式(8)(9-2)(9-3)，可得： <span class="math display">\[ p(s_t|x_t,z_{t-1}) = \eta (\mathcal{N}(s_t;z_{t-1},\Sigma_r+\Sigma_e)+k) \tag{10}\]</span> 进一步由式(8)(9-1)(10)可得： <span class="math display">\[p(z_t|x_t,z_{t-1}) = \eta \left(\mathcal{N}(z_t;z_{t-1}+x_{t,p},\Sigma_r+2\Sigma_e)+k \right) \tag{11}\]</span> 　　观测模型实际计算中，令 \(\bar{z} _ {t-1}\) 为点集 \(z_{t-1}\) 经过状态量变换后的点集，即 \(\bar{z} _ {t-1}=z _ {t-1}+x _ {t,p}\)；对于 \(z _ j\in z _ t\)，令 \(\bar{z} _ i \) 为 \(z _ j\) 在点集 \(\bar{z}_ { t-1}\) 中的最近点。那么: <span class="math display">\[ p(z_t|x_t,z_{t-1}) = \eta \left(\prod_{z_j\in z_t} \mathrm{exp}\left(-\frac{1}{2}(z_j-\bar{z_i})^T\Sigma^{-1}(z_j-\bar{z}_i)\right)+k\right) \tag{12}\]</span> 其中 \(\Sigma=2\Sigma_e+\Sigma_r\)。</p>
<h2 id="adh-tracker-运动模型">3. ADH Tracker 运动模型</h2>
<p>　　这里使用的是质点匀速模型，因为在 \((R,t)\) 搜索空间中得到了一组不同概率的解，所以可用多变量高斯分布去拟合这组解： <span class="math display">\[\left\{\begin{array}{l}
\mu_t=\sum_i p(x_{t,i}|z_i...z_t)x_{t,i}\\
\Sigma_t = \sum_i p(x_{t,i}|z_1...z_t)(x_{t,i}-\mu_t)(x_{t,i}-\mu_t)^T
\end{array}\tag{13}\right.\]</span> 其中 \(x_{t,i}\) 为第 \(i\) 组解对应的状态量。得到该状态量的高斯分布后，就可以用匀速运动模型预测下一时刻的状态。<br>
　　同时针对每一组解空间中的候选解，还可计算其匀速模型下的速度概率项，叠加到观测概率中。</p>
<h2 id="adh-算法">4. ADH 算法</h2>
<p><img src="/ADH-Tracker/adh.png" width="60%" height="60%" title="图 3. ADH 原理"> 　　对 \((R,t)\) 解空间进行有效搜索直接决定求解速度，如图 3. 所示，将解空间(state space)分割成一系列搜索区域，每个区域基于后验概率 \(p(x_t|z_1...z_t)\) 计算区域离散概率： <span class="math display">\[\begin{align}
p(c_i) &amp;= p(c_i\cap R) \\
&amp;= p(c_i|R)p(R) \\
&amp;= \frac{p(x_i|z_1...z_t)\vert c_i\vert}{\sum_{j\in R}p(x_j|z_1...z_t)\vert c_i\vert} p(R) \\
&amp;= \eta p(x_i|z_1...z_t)p(R)
\tag{14}
\end{align}\]</span> 其中 \(R\) 为待细分的区域集合(cells)，其被划分为子区域 \(c_i\in R\)，所以区域概率满足 \(\sum_{i\in R}p(c_i) = p(R)\)。对拥有较大离散概率的区域，进一步细分搜索区域，进行迭代搜索。初始化时，\(p(R)=1\)。<br>
　　这里需要制定区域细分的策略，考虑最大化划分前后区域概率分布的 KL-divergence，即 KL-divergence 能描述划分后，后验概率与真实分布的相似性，越接近真实分布，前后区域离散概率分布的 KL-divergence 会越小。而为了提高搜索效率，要求前后离散概率分布的 KL-divergence 要最大，最终收敛到真实分布。<br>
　　假设 \(R\) 区域的离散概率分布为 \(P_i\)，需要划分 \(k\) 个区域。那么划分前，可以认为其概率分布为每个 cell 概率为 \(P_i/k\)；划分后，其概率分布为：\(\sum_{j=1}^kp_j=P_i\)。这两个分布的 KL-divergence 为： <span class="math display">\[ D_{KL}(A\Vert B)=\sum_{j=1}^k p_j \mathrm{In}\left(\frac{p_j}{P_i/k}\right) \tag{15} \]</span> 当某个细分区域 \(p_{j'} = P_i\) 时： <span class="math display">\[ D_{KL}(A\Vert B)=P_i \mathrm{In}k  \tag{16}\]</span> 如果每个 cell 后验概率计算需要时间 \(t\) 秒，那么每秒能获得最大的 DL-divergence 为 \(P_i\mathrm{In}k/(kt)\)，由此可以选择策略：</p>
<ul>
<li>对 \(P_i\) 大于一定阈值的区域进行划分；</li>
<li>每个搜索维度划分的区域个数设定为 \(k=3\)。因为该函数在 \(k=e\) 时取得最大值。</li>
</ul>
<p><img src="/ADH-Tracker/adh_alg2.png" width="80%" height="80%" title="图 4. ADH Tracker"> 　　图 4. 为 ADH Tracker 算法的伪代码。</p>
<h2 id="adh-tracker-实现细节2">5. ADH Tracker 实现细节<a href="#2" id="2ref"><sup>[2]</sup></a></h2>
<h3 id="kalman-部分">5.1. Kalman 部分</h3>
<p>　　ADH 代码中 centroid-based kalman 的运动模型为质点匀速模型，较为简单。 其设置为：状态量 \(x=[v_x,v_y,v_z]\)，测量量 \(z=\frac{1}{\delta t}[p_x,p_y,p_z]\)。状态转移矩阵 \(A\) 以及观测矩阵 \(C\) 均为单位阵。过程噪声为高斯分布，其协方差矩阵为 \(Q_k = diag(\sigma_a,\sigma_a,\sigma_{a_z})\cdot \delta ^2 t\)，测量噪声的协方差矩阵为 \(R_k = diag(\sigma_m,\sigma_m,\sigma_m)\)。由此可方便的计算 kalman 预测及更新两个过程。</p>
<h3 id="adh-部分">5.2. ADH 部分</h3>
<p>　　ADH 算法中，每个采样分辨率下需要多次计算解空间中各 \((R,t)\) 下的观测模型，而观测模型计算中，每次需要通过 KD-Tree 寻找两个点集的匹配点，再通过式(12)计算观测概率模型。这样会非常耗时，因为观测模型本质上就是求解两个点集相似度，所以代码实现中，作者采用的策略为：首先将被匹配的点集进行栅格化，然后将点集中每个点以稠密度(density)高斯概率分布的形式拓展一定栅格范围，每个栅格取拓展到该栅格的点的高斯概率值的最大值。之后任何一个点集需要与之计算观测模型(相似度)，只要直接统计索引这个点集在该栅格下的概率值即可。</p>
<h2 id="参考文献">6. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Held, David, et al. "Robust real-time tracking combining 3D shape, color, and motion." The International Journal of Robotics Research 35.1-3 (2016): 30-49.<br>
<a id="2" href="#2ref">[2]</a> https://github.com/davheld/precision-tracking</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>Point Cloud</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>PointCloud Feature Extraction</title>
    <url>/PointCloud-Feature-Extraction/</url>
    <content><![CDATA[<p>　　机器学习中，特征提取是非常重要的一个环节（认为是最重要的一环也不为过）。对图像数据的特征提取操作已经较为成熟，如卷积；而点云数据由于无序性，所以对其进行高效的特征提取则比较困难。 一个好的点云特征提取操作需要具备以下特征：</p>
<ul>
<li>能提取点云的<strong>局部以及全局特征</strong>；</li>
<li>计算高效；</li>
</ul>
<p>　　目前已知的点云特征提取方法可分为两大类：Voxel-level，以及 Point-level。Voxel-Level 的特征提取也已经相当成熟，基本思路是将点云空间网格化，每个网格进行手工特征填充或者 Point-level 的特征提取，然后就可以应用标准的 2D/3D 卷积操作进行局部及全局特征提取。这种方法提取的特征细粒度取决于空间栅格化的分辨率，针对点级别的任务（如 semantic segmentation，Scene flow等），其特征的细粒度还是不够的。<br>
　　本文主要介绍 Point-level 的方法，这种方法能提取点级别的局部、全局特征信息，是处理点云的有效手段。这种方法首先要将无序的点云进行一定的结构化组织，由此可分为若干方法，如下阐述。</p>
<h2 id="基于原始三维空间操作">1. 基于原始三维空间操作</h2>
<p>　　在三维空间下进行点的局部特征提取，需要快速找到每个点周围的点，所以需要对点云构建 Kd-tree(或 Ball-tree)，来加速邻近点的快速查询。Kd-tree 的算法复杂度为：</p>
<ul>
<li>构建：\(\mathcal{O}(\mathrm{log}^2n)\)</li>
<li>插入：\(\mathcal{O}(\mathrm{log}n)\)</li>
<li>删除：\(\mathcal{O}(\mathrm{log}n)\)</li>
<li>查询：\(\mathcal{O}(n^{1-\frac{1}{k}}+m)\)，其中 \(m\) 为要查询的最近点个数</li>
</ul>
<h3 id="问题描述">1.1. 问题描述</h3>
<p>　　设点云集合：\(P=\{p_1,...,p_n\}\in R^{F}\)，每个点有 \(F\) 维的特征，以及每个点的三维坐标为：\(p_i=(x_i,y_i,z_i)\)（注意，坐标也可作为特征包含于 \(F\) 维中）。因为点云的无序性，定义点云集合的最近邻图(k-nearest neighbor graph) \(\mathcal{G=(V,E)}\)，其中 \(\mathcal{V}\) 表示点云中的点，\(\mathcal{E}\) 表示点 \(p_i\) 与最近的 \(k\) 个点 \(P_i^k=\{p_j ^ {i1},...,p_j ^ {ik}\}\) 所构成的有向边集合 \(\{(i,j_{i1}),...,(i,j_{ik})\}\)。由此定义<strong>点级别特征提取操作</strong>： <span class="math display">\[ p_i&#39; = \displaystyle\Box_{j:(i,j)\in\mathcal{E}} h_\Theta(p_i,p_j) \tag{1}\]</span> 其中 \(h _ {\Theta}\) 表示非线性映射函数，将特征空间：\(\mathbb{R} ^ F \times \mathbb{R} ^ F \to \mathbb{R} ^ {F'}\)；\(\Box\) 为用于特征聚合的对称函数。该操作类似图像二维卷积操作，将输入的点云集合：\(P=\{p_1,...,p_n\}\in R^{F}\) 映射到相同点数的：\(P'=\{p_1',...,p_n'\}\in R^{F'}\)。</p>
<h3 id="hbox-的选择">1.2. \(h,\Box\) 的选择</h3>
<h4 id="euclidean-conv">1.2.1. Euclidean Conv</h4>
<p>　　设计 \(h_{\Theta}(p_i,p_j)=\theta_jp_j\)，\(\Box=\sum\)，得到传统的 Euclidean convolution： <span class="math display">\[ p_i&#39; = \displaystyle\sum_{j:(i,j)\in\mathcal{E}}(\theta_jp_j) \tag{2}\]</span> 其中 \(\Theta=(\theta_i,...,\theta_k)\) 为滤波器的权重。</p>
<h4 id="pointnet1">1.2.2. PointNet<a href="#1" id="1ref"><sup>[1]</sup></a></h4>
<p>　　设计 \(h_{\Theta}(p_i,p_j)=h_{\Theta}(p_i) = \mathrm{MLP}(p_i)\)，\(\Box=\mathrm{MAX} 或 \sum\)，得到 PointNet 中的操作： <span class="math display">\[ p_i&#39; = \displaystyle\left\{ \sum|\mathrm{MAX}\right\}(\theta_ip_i) = \displaystyle\left\{\sum|\mathrm{MAX}\right\}\, \mathrm{MLP}(p_i) \tag{3}\]</span> 感知机的权重可以共享。</p>
<h4 id="deep-parametric-continuous-convoluion2">1.2.3. Deep Parametric Continuous Convoluion<a href="#2" id="2ref"><sup>[2]</sup></a></h4>
<p>　　设计 \(h_{\Theta}(p_i,p_j)=\mathrm{MLP}(p_j^{xyz}-p _ i^{xyz})\cdot p _ j^{\mathrm{exclude}\,xyz}\)，\(\Box=\mathrm{\sum}\)，得到 Deep Parametric Continuous Convolution 操作： <span class="math display">\[ p_i&#39; = \displaystyle\sum_{j:(i,j)\in\mathcal{E}}\left(\mathrm{MLP}(p_j^{xyz}-p _ i^{xyz})\cdot p _ j^{\mathrm{exclude}\,xyz}\right) \tag{4}\]</span> 根据邻近点的距离，显示的来学习其对中心点的特征贡献。Continuous Fusion Layer 中证明没必要显示的学习，直接将相对距离 Concate 到特征上，隐式的学习同样有效。</p>
<h4 id="pointnet3flownet3d4continuous-fusion-layer5">1.2.4. PointNet++<a href="#3" id="3ref"><sup>[3]</sup></a>/FlowNet3D<a href="#4" id="4ref"><sup>[4]</sup></a>/Continuous Fusion Layer<a href="#5" id="5ref"><sup>[5]</sup></a></h4>
<p>　　设计 \(h _ {\Theta}(p_i,p_j)=\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus (p _ j ^ {xyz}-p _ i^{xyz})\right)\)，\(\Box=\left\{\mathrm{MAX}|\sum\right\}\)，得到 PointNet++/FlowNet3D/Continuous Fusion Layer(前两者是 \(\mathrm{MAX}\)，后者是 \(\sum\) 操作) 中的操作： <span class="math display">\[ p_i&#39; = \displaystyle\left\{\mathrm{MAX}|\sum\right\}_{j:(i,j)\in\mathcal{E}}\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus (p _ j ^ {xyz}-p _ i^{xyz})\right) \tag{5}\]</span> 将点 \(p_j\) 中的坐标都转换到以中心点 \(p_i\) 为参考的局部坐标。这样能更好的提取局部信息，但是丢失了点的绝对坐标信息。</p>
<h4 id="edgeconvdgcnn6">1.2.5. EdgeConv(DGCNN)<a href="#6" id="6ref"><sup>[6]</sup></a></h4>
<p>　　设计 \(h _ {\Theta}(p_i,p_j)=\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ i^{xyz}\right)\)(这里只是猜测是这么做的，EdgeConv paper 中没有具体说怎么做的)，\(\Box=\mathrm{MAX}\)，得到 EdgeConv 中的操作： <span class="math display">\[ p_i&#39; = \displaystyle\mathrm{MAX}_{j:(i,j)\in\mathcal{E}}\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus( p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ i^{xyz}\right) \tag{6}\]</span> 额外加上点　\(p_i\) 的世界坐标，保留点的全局信息。 <img src="/PointCloud-Feature-Extraction/DGCNN.png" width="80%" height="80%" title="图 1. DGCNN"> 　　如图 1. 所示，DGCNN 网络结构与 PointNet 网络差不多，区别就在核心的点特征提取操作。<br>
　　代码实现可参考<a href="#14" id="14ref">[14]</a>, <a href="#15" id="15ref">[15]</a>，其中 <a href="#15" id="15ref">[15]</a>是完整的 DGCNN，每次卷积操作都是要在该点的新特征下取寻找 \(k\) 个最近邻，而 <a href="#14" id="14ref">[14]</a> 是简化版，最近邻点是固定的，分析代码可知其步骤：</p>
<ol type="1">
<li>针对每个点 \(p_i\)，首先找到该点最近的 \(k\) 个点及对应的特征，得到 tensor 维度：\(B\times N\times k\times F\);</li>
<li>然后将本点 \(p_i\) 的特征 concate 到对应的 \(k\) 个点特征，得到 tensor 维度： \(B\times N\times k\times 2F\)；</li>
<li>不同层 conv，bn，relu 的作用，得到多个 tensor，其维度：\(B\times N\times k\times \{F'|F'_1,...,F'_s\}\)；</li>
<li>对 \(k\) 个点作最大化聚合，得到各 tensor 维度：\(B\times N\times \{F'|F_1',...,F_s'\}\)</li>
<li>每个点的特征进行 concate，然后作 conv，bn，relu 操作，最终得到点的特征 tensor，维度为 \(B\times N\times F^{final}\)；</li>
</ol>
<p>该实现与式 (6) 有点出入，该实现没有显示计算本点坐标与对应的 \(k\) 个点坐标的差值。但是总体思想一致。</p>
<h4 id="randla-net7">1.2.6. RandLA-Net<a href="#7" id="7ref"><sup>[7]</sup></a></h4>
<p>　　设计 \(h _ {\Theta}(p_i,p_j)=\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus\left\Vert p _ j^{xyz}-p _ i^{xyz}\right\Vert\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ j^{xyz}\oplus p _ i^{xyz}\right)\)，\(\Box=\sum \mathrm{softmax\,MLP}(h_{\Theta}(p_i,p_j))\)，得到 RandLA-Net 中的操作(详见 <a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>)： <span class="math display">\[ p_i&#39; = \displaystyle\sum_{j:(i,j)\in\mathcal{E}}\left(\mathrm{softmax\,MLP&#39;}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus\left\Vert p _ j^{xyz}-p _ i^{xyz}\right\Vert\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ j^{xyz}\oplus p _ i^{xyz}\right)\right)\cdot \left(\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus\left\Vert p _ j^{xyz}-p _ i^{xyz}\right\Vert\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ j^{xyz}\oplus p _ i^{xyz}\right)\right) \tag{7}\]</span> 这里的 \(\Box\) 函数称为 Attention Pooling，即将特征维度进行加权求和。</p>
<h4 id="tanet12">1.2.7. TANet<a href="#12" id="12ref"><sup>[12]</sup></a></h4>
<p><img src="/PointCloud-Feature-Extraction/TANet.png" width="40%" height="40%" title="图 2. TANet"> 　　如图 2. 所示，TANet 中提出了 TA Module，该模块包含三种注意力机制：point-wise，channel-wise，voxel-wise。其中前两种注意力可用于任意点的特征提取。对应的前两种注意力构成了 \(h _ {\Theta}(p_i,p_j)\) 函数： <span class="math display">\[h_{\Theta} = \left(\mathrm{MLP_1}(\mathrm{MaxPool_{feats}}\,P_i^k) \times \mathrm{MLP_2}(\mathrm{MaxPool_{points}}\, P_i^k)\right) \cdot P_i^k \tag{8}\]</span> 其中 point-wise attention 为 \(\mathrm{MLP_1}(\mathrm{MaxPool_{feats}}\,P_i^k) = S \in \mathbb{R}^{K\times 1}\)；channel-wise attention 为 \(\mathrm{MLP_2}(\mathrm{MaxPool_{points}}\,P_i^k) = T \in \mathbb{R}^{F\times 1}\)；由此构成 \(M=S\times T\in\mathbb{R}^{K\times F}\)，作为权重作用于 \(P_i^k\)，最后用 \(\sum |\mathrm{MAX}\) 操作对点维度进行特征聚合。注意，这里的 point-wise attention 是与点的顺序有关的，看起来这里经过训练，可以消除点顺序的影响。</p>
<h4 id="pointconv13">1.2.8. PointConv<a href="#13" id="13ref"><sup>[13]</sup></a></h4>
<p><img src="/PointCloud-Feature-Extraction/PointConv.png" width="60%" height="60%" title="图 3. PointConv"> <img src="/PointCloud-Feature-Extraction/PointConv2.png" width="60%" height="60%" title="图 4. Efficient PointConv"> 　　如图 3. 以及 4. 所示，PointConv 设计的 \(h_{\Theta}\) 有两部分组成。一是根据 \(P_i^k\) 点集计算权重矩阵 \(W\)；二是用核密度函数(Kernel Density Estimation)计算点的密度，然后根据密度计算权重。这里加入基于点密度的权重，是因为，点密度高的区域，需要显式地降低其特征权重，避免最终特征学不到稀疏点的特征。图 4. 是高效版本。</p>
<h2 id="基于映射空间操作">2. 基于映射空间操作</h2>
<p>　　基于原始三维空间的点特征提取操作，<strong>其算法复杂度直接依赖点数</strong>；而如果将其映射到高维空间，则点数只会影响映射与反映射的过程，核心特征提取操作将不受点的个数影响。<br>
　　三维空间下点云无法有序组织，将点云映射到更高维空间，在高维空间下进行结构化组织后，即可应用传统的卷积操作进行特征提取。</p>
<h3 id="bilateral-convolutional-layerbcl8splatnet9hplflownet10">2.1. Bilateral Convolutional Layer(BCL<a href="#8" id="8ref"><sup>[8]</sup></a>)(SPLATNet<a href="#9" id="9ref"><sup>[9]</sup></a>/HPLFlowNet<a href="#10" id="10ref"><sup>[10]</sup></a>)</h3>
<p><img src="/PointCloud-Feature-Extraction/BCL.png" width="60%" height="60%" title="图 3. BCL"> 　　如图 3. 所示，BCL 操作有三部分组成：</p>
<ul>
<li><strong>Splat</strong><br>
将三维空间的点 \(p_i^{xyz}\) 投影到高维空间，实际操作中直接乘以一个预定义的 \(4\times 3\) 矩阵。4 维空间的晶格顶点聚合晶格内映射点的信息，聚合过程中以映射点与格点的距离作为权重；</li>
<li><strong>Convolve</strong><br>
因为晶格空间内空间是栅格化的，所以直接进行传统的 2D 卷积操作；</li>
<li><strong>Slice</strong><br>
卷积得到的是晶格空间的特征图，反映射到三维空间，即得到点级别的包含周围信息的特征向量；</li>
</ul>
<p>　　映射与反映射的操作实现上需要建立哈希表作点的快速查询，需要记录的辅助信息也比较多。后期有时间再对着代码分析。</p>
<h2 id="参考文献">3. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Qi, Charles R., et al. "Pointnet: Deep learning on point sets for 3d classification and segmentation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br>
<a id="2" href="#2ref">[2]</a> Wang, S., Suo, S., Ma, W.C., Urtasun, R.: Deep parameteric convolutional neural networks. In: CVPR (2018)<br>
<a id="3" href="#3ref">[3]</a> Qi, Charles Ruizhongtai, et al. "Pointnet++: Deep hierarchical feature learning on point sets in a metric space." Advances in neural information processing systems. 2017.<br>
<a id="4" href="#4ref">[4]</a> Liu, Xingyu, Charles R. Qi, and Leonidas J. Guibas. "Flownet3d: Learning scene flow in 3d point clouds." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="5" href="#5ref">[5]</a> Liang, Ming, et al. "Deep continuous fusion for multi-sensor 3d object detection." Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br>
<a id="6" href="#6ref">[6]</a> Wang, Yue, et al. "Dynamic graph cnn for learning on point clouds." ACM Transactions on Graphics (TOG) 38.5 (2019): 146.<br>
<a id="7" href="#7ref">[7]</a> Hu, Qingyong, et al. "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds." arXiv preprint arXiv:1911.11236 (2019).<br>
<a id="8" href="#8ref">[8]</a> Kiefel, Martin, Varun Jampani, and Peter V. Gehler. "Permutohedral lattice cnns." arXiv preprint arXiv:1412.6618 (2014).<br>
<a id="9" href="#9ref">[9]</a> Su, Hang, et al. "Splatnet: Sparse lattice networks for point cloud processing." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
<a id="10" href="#10ref">[10]</a> Gu, Xiuye, et al. "Hplflownet: Hierarchical permutohedral lattice flownet for scene flow estimation on large-scale point clouds." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="11" href="#11ref">[11]</a> Xie, Liang, et al. "PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module." arXiv preprint arXiv:1911.06084 (2019).<br>
<a id="12" href="#12ref">[12]</a> Liu, Zhe, et al. "TANet: Robust 3D Object Detection from Point Clouds with Triple Attention." arXiv preprint arXiv:1912.05163 (2019).<br>
<a id="13" href="#13ref">[13]</a> Wu, Wenxuan, Zhongang Qi, and Li Fuxin. "Pointconv: Deep convolutional networks on 3d point clouds." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="14" href="#14ref">[14]</a> https://github.com/WangYueFt/dcp/blob/master/model.py<br>
<a id="15" href="#15ref">[15]</a> https://github.com/WangYueFt/dgcnn/blob/master/pytorch/model.py</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;RandLA-Net&quot;</title>
    <url>/paper-reading-RandLA-Net/</url>
    <content><![CDATA[<p>　　不同与点云 3D 检测，可以 Voxel 化牺牲一定的分辨率，点云语义分割则要求点级别的分辨率，所以栅格化做点云分割信息会有一定的损失。但是直接对所有点进行特征提取，计算量又相当巨大，为了平衡效率与性能，一般也不得不对点云进行采样处理。这种点云级别的处理方式有 <a href="/PointNet-系列论文详读/" title="PointNet++">PointNet++</a>， <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 等。 <img src="/paper-reading-RandLA-Net/arch2.png" width="90%" height="90%" title="图 1. RandLA-Net"> 　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出的方法主要为了解决大尺度点云集下，如何高效提取点云局部特征的问题。针对大尺度点云集，作者对比了不同采样算法，得出随机采样最简单高效的结论；针对随机采样丟失信息的问题，以及为了提高局部特征提取能力，本文提出了局部特征聚合(Local Feature Aggregation)模块，该模块包含 Local Spatial Encoding，Attentive Pooling，以及 Dilated Residual Block。<br>
　　如图 1. 所示，LFA 作为基本模块用于特征提取，下采样采用随机采用，上采样过程类似图像中的 dconv，包含向上插值以及 MLP 过程。</p>
<h2 id="sampling">1. Sampling</h2>
<p>　　关于点云采样，在 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 中有简单介绍。本文将采样算法分为两大类：</p>
<ul>
<li>Heuristic Sampling
<ol type="1">
<li>Farthest Point Sampling(FPS)， <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 中有介绍，是一种均匀采样方法。其算法复杂度为 \(\mathcal{O}(N^2)\)。</li>
<li>Inverse Density Importance Sampling(IDIS)，计算每个点的密度属性，根据属性选取 K 个点，其复杂度为 \(\mathcal{O}(N)\)。</li>
<li>Random Sampling(RS)，随机采样，复杂度为 \(\mathcal{O}(1)\)。</li>
</ol></li>
<li>Learning-based Sampling<br>
...</li>
</ul>
<p>　　本文作者认为随机采样复杂度最低，其它采样复杂度太高。我认为也不能这么说，在一定策略及加速下，其它采样算法效率也可以很高。比如栅格化后在采样，可以高效的并行加速，并且使得稀疏区域保留更多信息。</p>
<h2 id="local-feature-aggregation">2. Local Feature Aggregation</h2>
<p><img src="/paper-reading-RandLA-Net/arch1.png" width="90%" height="90%" title="图 2. RandLA-Net"> 　　特征提取非常关键，尤其在本文采用随机采样后，稀疏区域信息丢失比较严重的情况下。如图 2. 所示，本文提出了局部特征聚合(Local Feature Aggregation)模块，包含 Local Spatial Encoding，Attentive Pooling，以及 Dilated Residual Block。</p>
<h3 id="local-spatial-encoding">2.1. Local Spatial Encoding</h3>
<p>　　在原始点云中提取每个点的局部特征，类似 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a>(PointNet++) 中的 set conv 层，这里多了手工特征信息，其步骤为：</p>
<ol type="1">
<li>针对每个点 \(p_i\)，用 KNN 找到与其最近的 K 个点: \(\{p _ i^1,...p _ i^k,...p _ i^K\}\)；</li>
<li>针对最近邻的每个点 \(p_i^k\)，设计其相对位置的特征： <span class="math display">\[ \mathrm{r}_i^k = \mathrm{MLP}\left(p_i\oplus p_i^k\oplus (p_i-p_i^k)\oplus ||p_i-p_i^k||\right) \tag{1}\]</span></li>
<li>针对最近领的每个点 \(p_i^k\)，其本来的特征为 \(\mathrm{f}_i^k\)，叠加相对位置特征 \(\mathrm{r}_i^k\) 后得到每个点的特征为 \(\mathrm{\hat{f}}_i^k\)。由此最近领点集的特征为： \(\mathrm{\hat{F}}_i=\{\hat{\mathrm{f}}_i^1,...\hat{\mathrm{f}}_i^k,...\hat{\mathrm{f}}_i^K\}\)。</li>
</ol>
<h3 id="attentive-pooling">2.2. Attentive Pooling</h3>
<p>　　该模块的作用是聚合 \(p_i\) 的最近邻点集特征 \(\hat{\mathrm{F}}_i\)。PointNet 的 SA 层(FlowNet3D 中的 set conv 层)直接用 Max/Mean 这种对称函数聚合，本文采用一种更有效的基于注意力机制的 pooling 方式，其步骤为：</p>
<ol type="1">
<li>计算注意力分数，对每个特征设计分数计算方式为： <span class="math display">\[ \mathrm{s}_i^k = \mathrm{g}\left(\hat{\mathrm{f}}_i^k, W\right) \tag{2}\]</span> 其中 \(\mathrm{g}\) 表示一个感知机 MLP(W 为其权重) 以及一个 softmax 函数。</li>
<li>聚合，根据注意力分数，权重求和，得到 \(p_i\) 点的特征： <span class="math display">\[ \bar{\mathrm{f}}_i = \sum_{k=1}^K \left(\hat{\mathrm{f}}_i^k \cdot \mathrm{s}_i^k \right) \tag{3}\]</span></li>
</ol>
<h3 id="dilated-residual-block">2.3.  Dilated Residual Block</h3>
<p><img src="/paper-reading-RandLA-Net/LA.png" width="60%" height="60%" title="图 3. LA Module"> 　　如图 2. 及 3. 所示，连续堆叠多个 LA 模块，能起到增加感受野的效果，然后引入 residual 思想，图 2. 下图就构成了一个 LFA 的基础模块。</p>
<p><a id="1" href="#1ref">[1]</a> Hu, Qingyong, et al. "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds." arXiv preprint arXiv:1911.11236 (2019).</p>
]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>Model Compression - &#39;Pruning&#39;</title>
    <url>/pruning/</url>
    <content><![CDATA[<p>　　模型压缩技术主要有：Pruning，Regularization，Quantization，KnowLedge Distillation，Comditional Computation等。本文主要讨论剪枝技术(Pruning)。复杂模型存在存储空间大，计算量大等问题，对其进行剪枝使网络中的权重及特征层稀疏化(Regularization 也是稀疏化的过程)，能获得以下效益：</p>
<ul>
<li><strong>模型更小</strong><br>
稀疏化的模型含有大量的零值，称为稀疏表达(Sparse Representation)，通过稀疏矩阵压缩技术进行编码压缩后得到压缩表达(Compressed Representation)。片内内存(On-chip Mem)与片外内存(Off-chip Mem)数据的传输可用压缩表达，使实际传输中的模型内存更小，而计算时，可通过反编码算法得到稀疏表达，从而进行正常的矩阵运算；也可以直接用压缩表达进行矩阵运算，这需要特殊的硬件支持，并且稀疏化的过程一般是结构化剪枝(Structured Pruning)或是正则。</li>
<li><strong>速度更快</strong><br>
目前大部分矩阵运算芯片，性能瓶颈都在片内片外内存的带宽，稀疏化后能有效压缩矩阵单元，降低模型传输内存；另一方面，通过结构化的剪枝，在特定硬件下，能直接减少零值运算量。</li>
<li><strong>能效更高</strong><br>
片外内存访问所花费的能量大概比片内内存多两个数量级，所以降低模型的传输内存，甚至将模型及中间计算量(如特征层)直接塞到片内内存，减少与片外内存的交互，能有效提高能效。</li>
</ul>
<p>　　剪枝的过程主要是：根据剪枝类型选用对应的稀疏性定义方式；剪枝前模型的敏感度分析；应用剪枝算法及策略。以下根据 Distiller<a href="#1" id="1ref"><sup>[1]</sup></a> 库分别对这三部分进行详细阐述。</p>
<h2 id="稀疏性定义">1. 稀疏性定义</h2>
<p>　　剪枝大致可分为 element-wise 剪枝以及 Structured 剪枝，element-wise 剪枝只需要定义每个张量的稀疏性，即 Element-wise Sparsity，而 Structured 剪枝需要定义不同结构的稀疏性，有 Filter-wise Sparsity，Channel-wise Sparsity，Kernel-wise Sparsity，Block-wise Sparsity，Column-wise Sparsity，Row-wise Sparsity。<br>
　　设输入特征层 IFM(Input Feature Map)\(\in\mathbb{R}^{N\times C_1\times H_1\times W_1}\)，卷积核 Filter\(\in\mathbb{R}^{C_2\times C_1\times K\times K}\)，则输出特征层 OFM(Output Feature Map)\(\in\mathbb{R}^{N\times C_2\times H_2\times W_2}\)。</p>
<h3 id="element-wise-sparsity">1.1. Element-wise Sparsity</h3>
<p>　　张量元素的稀疏性，设 \(X\in\mathbb{R}^{N\times C\times H\times W}\)： <span class="math display">\[\Vert X\Vert_{element-wise} = \frac{l_0(X)}{N\times C\times H\times W} = \frac{\sum_{n=1}^{N}\sum_{c=1}^{C}\sum_{h=1}^{H}\sum_{w=1}^{W}\left\vert X_{n,c,h,w} \right\vert ^0}{N\times C\times H\times W} \tag{1}\]</span> 其中 \(l_0\) 正则根据元素是否为 0，确定输出 0/1。</p>
<h3 id="filter-wise-sparsity">1.2. Filter-wise Sparsity</h3>
<p>　　对于有 \(C_2\) 个卷积核的 Filter\(\in\mathbb{R}^{C_2\times C_1\times K\times K}\)，其 Filter-wise 的稀疏性可表示为： <span class="math display">\[\Vert X\Vert_{filter-wise} = \frac{\sum_{c_2=1}^{C_2}\left\vert\sum_{c_1=1}^{C_1}\sum_{k_1=1}^{K}\sum_{k_2=1}^{K}\vert X_{c_2,c_1,k_1,k_2}\vert \right\vert ^0}{C_2} \tag{2}\]</span></p>
<h3 id="kernel-wise-sparsity">1.3. Kernel-wise Sparsity</h3>
<p>　　卷积核 Filter\(\in\mathbb{R}^{C_2\times C_1\times K\times K}\) 拥有 \(C_2\times C_1\) 个 \(K\times K\) 大小的 Kernel，其 Kernel-wise 的稀疏性可表示为： <span class="math display">\[\Vert X\Vert_{kernel-wise} = \frac{\sum_{c_2=1}^{C_2}\sum_{c_1=1}^{C_1}\left\vert\sum_{k_1=1}^{K}\sum_{k_2=1}^{K}\vert X_{c_2,c_1,k_1,k_2}\vert \right\vert ^0}{C_2\times C_1} \tag{3}\]</span></p>
<h3 id="channel-wise-sparsity">1.4. Channel-wise Sparsity</h3>
<p>　　对于张量单元 \(X\in\mathbb{R}^{N\times C\times H\times W}\)： <span class="math display">\[\Vert X\Vert_{channel-wise} = \frac{\sum_{c=1}^{C}\left\vert\sum_{n=1}^{N}\sum_{h=1}^{H}\sum_{w=1}^{W}\vert X_{n,c,h,w}\vert \right\vert ^0}{C} \tag{4}\]</span></p>
<h3 id="column-wise-sparsity">1.5. Column-wise Sparsity</h3>
<p>　　对于张量单元 \(X\in\mathbb{R}^{H\times W}\)： <span class="math display">\[\Vert X\Vert_{column-wise} = \frac{\sum_{h=1}^{H}\left\vert\sum_{w=1}^{W}\vert X_{h,w}\vert \right\vert ^0}{H} \tag{5}\]</span></p>
<h3 id="row-wise-sparsity">1.6. Row-wise Sparsity。</h3>
<p>　　对于张量单元 \(X\in\mathbb{R}^{H\times W}\)： <span class="math display">\[\Vert X\Vert_{row-wise} = \frac{\sum_{w=1}^{W}\left\vert\sum_{h=1}^{H}\vert X_{h,w}\vert \right\vert ^0}{W} \tag{6}\]</span></p>
<h3 id="block-wise-sparsity">1.7. Block-wise Sparsity</h3>
<p>　　对于张量单元 \(X\in\mathbb{R}^{N\times C\times H\times W}\)，设定 block\(\in\mathbb{R}^{repetitions\times depth\times 1\times1}\)，由此将 \(X\) 划分为 \(\frac{N\times C}{repetitions\times depth}\times (repetitions\times depth)\times (H\times W)=N'\times B\times K\)。block-sparsity 定义为： <span class="math display">\[\Vert X\Vert_{block-wise} = \frac{\sum_{n=1}^{N&#39;}\sum_{k=1}^K\left\vert\sum_{b=1}^{B}\vert X_{n,b,k}\vert \right\vert ^0}{N&#39;\times K} \tag{7}\]</span></p>
<h2 id="模型敏感度分析sensitivity-analysis">2. 模型敏感度分析(Sensitivity Analysis)</h2>
<p>　　在剪枝前，我们首先要确定减哪几层，每层减多少(即剪枝阈值或剪枝程度)。这就涉及到模型中每层网络对模型输出的敏感度分析(Sensitivity Analysis)。<a href="#2" id="2ref">[2]</a> 提出了一种有效的方法来确定每层的敏感度。在一个已训练模型下，分别对每一层进行不同程度的剪枝，得到对应的网络输出精度，绘制敏感度曲线。<br>
<img src="/pruning/sensitivity.png" width="70%" height="70%" title="图 1. 敏感度分析"> 　　如图 1. 所示，AlexNet 网络各层对 element-wise 剪枝的敏感度曲线显示，越深的网络层对输出越不敏感，尤其是全连接层，所以剪枝程度可以更高。而对于非常敏感的浅层网络，则需要降低剪枝程度，甚至不剪枝。</p>
<h2 id="剪枝算法">3. 剪枝算法</h2>
<h3 id="magnitude-pruner">3.1. Magnitude Pruner</h3>
<p>　　这是最基本的剪枝方法，对于要剪枝的对象，判断其绝对值是否大于阈值 \(\lambda\)，如果小于阈值，则将该对象置为零。该对象可以是 element-wise，也可以是其它结构化的对象，如 filter，Kernel 等。<br>
　　该方法需要直接设定阈值，而阈值的设定是比较困难的。</p>
<h3 id="sensitivity-pruner">3.2. Sensitivity Pruner</h3>
<p>　　卷积网络每层的权重值为高斯分布，由高斯分布的性质可知，在标准差 \(\sigma\) 内，有 68% 的元素，所以阈值可设定为 \(\lambda=s\times \sigma\)，其表示了 \(s\times 68\%\) 的元素被剪枝掉。</p>
<h3 id="level-pruner">3.3. Level Pruner</h3>
<p>　　Level Pruner 直接设定需要剪枝的比例，即直接设定剪枝后的稀疏性，这比前两种方法更加稳定。具体做法就是对每个对象进行排序，然后以此裁剪，直到裁剪到设定的比例。</p>
<h3 id="automated-gradual-pruneragp">3.4. Automated Gradual Pruner(AGP)</h3>
<p>　　<a href="#3" id="3ref">[3]</a>提出了一种训练剪枝的方法，在 Level Pruner 基础上，随着训练的过程，设计剪枝的稀疏性从初始的 \(s_i\) 增加到目标 \(s_f\)，其数学表示为： <span class="math display">\[ s_t = s_f+(s_i-s_f)\left(1-\frac{t-t_0}{n\Delta t}\right)^3 \; \mathrm{for} \, t\in \{t_0, t_0+\Delta t,...,t_0+n\Delta t\} \tag{8}\]</span> 实现的效果是，初始阶段，剪枝比较厉害，越到最后，剪枝的量越少，直到达到目标剪枝值。</p>
<h3 id="structure-pruners">3.5. Structure Pruners</h3>
<p>　　这里讨论结构化剪枝中 Filter 以及 Channel 的剪枝<a href="#4" id="4ref"><sup>[4]</sup></a>，对应的需要用到前面提到的 Filter-wise 以及 Channel-wise 的稀疏性。不同于 element-wise 剪枝，结构化剪枝由于网络的连接性会更复杂，这里考虑三种链接情况。</p>
<h4 id="连接结构1">3.5.1. 连接结构1</h4>
<p><img src="/pruning/filter1.png" width="70%" height="70%" title="图 2. 连接结构1"> 　　如图 2. 所示，设第\(i\)层特征 \(X_i\in\mathbb{R}^{C_i\times H_i\times W_i}\)，经过卷积核 \(\mathcal{F}\in\mathbb{R}^{C_{i+1}\,\times\, C_i\,\times\,K\,\times\,K}\)后得到第 \(i+1\)层特征层 \(X_{i+1}\in\mathbb{R}^{C_{i+1}\,\times\, H_{i+1}\,\times\, W_{i+1}}\)。图中绿色及黄色代表剪枝掉的 Filter，对应的输出少了这两个卷积计算得到的 channel 维度的两个特征图，再往后就是去除 BN 里面对应 channel 层的 scale 以及 shift 信息(Distiller 中自动删除)，最后再次应用的卷积核需要去除对应的 channel，即类似做 channel-wise 剪枝。由此可见，结构化剪枝会影响后面的网络结构，需要根据网络信息流作网络调整。<br>
　　第 \(i\) 卷积层运算量 MAC 为 \(C_{i+1}C_iK^2H_{i+1}W_{i+1}\)，如果剪枝掉 \(m\) 个卷积核，那么第 i 层卷积减少的运算量为 \(mC_iK^2H_{i+1}W_{i+1}\)，下一层 \(i+1\) 卷积层减少的运算量为 \(C_{i+2}mK^2H_{i+2}W_{i+2}\)。所以在第 \(i\) 层剪枝掉 \(m\) 个卷积核，会使第 \(i,i+1\) 层的运算量各减少 \(m/C_{i+1}\)。</p>
<h4 id="连接结构2">3.5.2. 连接结构2</h4>
<p><img src="/pruning/filter2.png" width="60%" height="60%" title="图 3. 连接结构2"> 　　如图 3. 所示，与被剪枝的特征图直连的卷积核均需要作 channel 维度的裁剪，这一步在 Distiller 中自动进行。</p>
<h4 id="连接结构3">3.5.3. 连接结构3</h4>
<p><img src="/pruning/filter3.png" width="60%" height="60%" title="图 4. 连接结构3"> 　　如图 4. 所示，如果两个卷积层的输出要做 element-wise 相加操作，那么就要求两个卷积层的输出大小要一样。这就要求对这两个卷积层的卷积核裁剪过程要一样，包括裁剪的卷积数量以及卷积位置。这需要在 Distiller 中显示的指定。</p>
<h2 id="参考文献">4. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> https://nervanasystems.github.io/distiller/index.html<br>
<a id="2" href="#2ref">[2]</a> Han, Song, et al. "Learning both weights and connections for efficient neural network." Advances in neural information processing systems. 2015.<br>
<a id="3" href="#3ref">[3]</a> Zhu, Michael, and Suyog Gupta. "To prune, or not to prune: exploring the efficacy of pruning for model compression." arXiv preprint arXiv:1710.01878 (2017).<br>
<a id="4" href="#4ref">[4]</a> Li, Hao, et al. "Pruning filters for efficient convnets." arXiv preprint arXiv:1608.08710 (2016).</p>
]]></content>
      <categories>
        <category>Model Compression</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Model Compression</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;3D-LaneNet End-to-End 3D Multiple Lane Detection&quot;</title>
    <url>/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/</url>
    <content><![CDATA[<p>　　在上一篇 paper reading <a href="/paper-reading-Deep-Multi-Sensor-Lane-Detection/" title="Deep Multi-Sensor Lane Detection">Deep Multi-Sensor Lane Detection</a> 中，最后我提到一个思考点：借鉴 STN 的思路，用前视图直接去回归 IPM 变换需要的矩阵参数。本文<a href="#1" id="1ref"><sup>[1]</sup></a>就是采用了这种思路！ <img src="/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/res.png" width="60%" height="60%" title="图 1. 方法概图"> 如图 1. 所示，车道线检测还是在俯视图下来做的，车道线输出是三维曲线，一定程度上估计出了地面高度。</p>
<h2 id="网络结构">1. 网络结构</h2>
<p><img src="/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/arch.png" width="90%" height="90%" title="图 2. 网络结构"> 　　如图 2. 所示，网络有两部分组成：</p>
<ul>
<li>Image-view 通路<br>
输入为前视图图像，输出相机 pitch 角度 \(\theta\) 以及相机高度 \(H\)，这里假设相机坐标系相对地面坐标系没有 roll，yaw 偏转，由此可得到相机外参矩阵，用于 IPM 变换；</li>
<li>Top-view 通路<br>
输入为前视图某个特征层经过 Projective Transformation Layer 变换后的特征，之后的特征层叠加来自经过变换的前视图特征层，最后输出车道线检测；</li>
</ul>
<h3 id="projective-transformation-layer">1.1. Projective Transformation Layer</h3>
<p>　　<a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a> 中较详细得阐述了 IPM 原理，<a href="/paper-reading-Deep-Multi-Sensor-Lane-Detection/" title="Deep Multi-Sensor Lane Detection">Deep Multi-Sensor Lane Detection</a> 则阐述了 STN 的原理。Projective Transformation Layer 类似 STN 的结构，输入相机内外参后，沿用 STN 中的 Grid Generator 以及 Sampler 模块，Grid Generator 就是 IPM 的过程。此外，Projective Transformation Layer 还增加一个卷积层，将前视图的 C 维特征卷积为 C/2 维特征与俯视图的特征层进行叠加。<br>
　　该层不仅从前视图特征层上产生了俯视图特征，还融合了前视图与俯视图特征层，融合前视图特征有两大好处：</p>
<ul>
<li>瘦高型物体，如栅栏，行人，在俯视图下信息量很小，而前视图能有效提取丰富特征；</li>
<li>远距离时，俯视图下的信息会比较稀疏(类似点云)，而前视图信息会比较密集，能有效提取远距离下的信息特征；</li>
</ul>
<h3 id="anchor-based-lane-prediction">1.2. Anchor-Based Lane Prediction</h3>
<p><img src="/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/anchor.png" width="60%" height="60%" title="图 3. Anchor-Based Lane Prediction"> 　　如图 3. 所示，作者提出了一种 Anchor-Based 车道线检测方法，其实这和目标检测中的 Anchor-Based 还是不太一样，这里的 Anchor 指的是几条线。设定 \(y\) 方向的 anchor 线段：\(\{X_A^i\} _ {i=1}^N\)，\(y\) 坐标上的预定义位置：\(\{y_j\} _ {j=1}^K\)。对于每个 anchor 线段，分类上以 \(Y_{ref}\) 为基准，输出三种类别(距离 \(Y_{ref}\) 最近的线的类型)，两种车道中心线，一种车道线，即 \(\{c_1,c_2,d\}\)；回归上每种类别都输出 2K 个 Offsets：\(\{(x_j ^ i,z_j ^ i)\} _ {j=1}^K\)，对应的第 \(i\) 个 anchor，在第 \(j\) 位置上的 3D 点表示为 \((x_j ^ i+X_A ^ i,y_j,z_j ^ i)\in\mathbb{R}^3\)。综上网络输出 \(N\times(3(2K+1))\) 维的向量，最后经过 1D NMS 处理后，每个 anchor 上的 3D 点通过样条插值出 3D 线条。</p>
<h2 id="loss">2. Loss</h2>
<p>　　训练阶段，真值如何匹配 anchor 很重要，过程如下：</p>
<ol type="1">
<li>将所有车道线以及车道中心线通过 IPM 投影到俯视图下；</li>
<li>在 \(Y_{ref}\) 位置上将每条线匹配给 \(x\) 方向距离最近的 anchor 线段；</li>
<li>对于每个 anchor 上匹配到的线，将最左边的车道线与中心线赋为 \(d,c_1\)，如果还有其它中心线，则赋为 \(c_2\)；</li>
</ol>
<p>对于没有穿过 \(Y_{ref}\) 的车道线，则予以忽略，中心线理论上都会穿过 \(Y_{ref}\)。所以理论上，本文预测的中心线是全的，而车道线会不全，前方的岔路口，一部分车道线不会被预测出来。<br>
　　Loss 项有四部分组成，分别为车道线分类，车道线锚点 Offsets 回归，相机外参 pitch 角 \(\theta\) 以及高度 \(h_{cam}\) 的回归，如下： <span class="math display">\[\begin{align}
\mathcal{L} =&amp; - \sum_{t\in\{c_1,c_2,d\}} \sum_{i=1}^N\left(\hat{p}_t^i\mathrm{log}p_t^i + \left(1-\hat{p}_t^i\right)\mathrm{log}\left(1-p_t^i\right)\right) \\
&amp;+ \sum _ {t\in\{c_1,c_2,d\}}\sum_{i=1}^N \hat{p}_t^i\left(\left\Vert x_t^i-\hat{x}_t^i\right\Vert+\left\Vert z_t^i-\hat{z}_t^i\right\Vert\right) \\
&amp;+ \left|\theta-\hat{\theta}\right| + \left|h_{cam}-\hat{h}_{cam}\right| \tag{1}
\end{align}\]</span></p>
<h2 id="参考文献">3. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Garnett, Noa, et al. "3D-LaneNet: end-to-end 3D multiple lane detection." Proceedings of the IEEE International Conference on Computer Vision. 2019.<br>
<a id="2" href="#2ref">[2]</a> <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a><br>
<a id="3" href="#3ref">[3]</a> <a href="/paper-reading-Deep-Multi-Sensor-Lane-Detection/" title="Deep Multi-Sensor Lane Detection">Deep Multi-Sensor Lane Detection</a></p>
]]></content>
      <categories>
        <category>Lane Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Lane Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Deep Multi-Sensor Lane Detection&quot;</title>
    <url>/paper-reading-Deep-Multi-Sensor-Lane-Detection/</url>
    <content><![CDATA[<p>　　前文 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a> 已经较详细得阐述了俯视图下作车道线检测的逆透视原理，提到传统 IPM 有个较强的假设：地面是平坦的。对于 L4 自动驾驶，在这个假设下车道线检测不管是精度还是可靠性，都远远不够。如果有高精度地图，那么这些问题都有方法来消除。当然，如果有高精度地图，且自定位准确，也就不需要车道线检测了，所以这里讨论，在无高精度地图下，本文<a href="#1" id="1ref"><sup>[1]</sup></a>如何通过激光点云数据学习的方法解决上述问题。</p>
<h2 id="网络结构">1. 网络结构</h2>
<p><img src="/paper-reading-Deep-Multi-Sensor-Lane-Detection/lane_det.png" width="90%" height="90%" title="图 1. Multi-Sensor Lane Detection"> 　　如图 1. 所示，整个算法有两个网络组成：</p>
<ul>
<li><strong>地面估计(Ground Height Estimation)网络</strong><br>
输入是俯视图下历史 N 帧的栅格点云，输出的是俯视图下地面高度；</li>
<li><strong>车道线检测(Lane Prediction)网络</strong><br>
输入是俯视图下历史 N 帧的栅格点云，并且叠加前视图图像逆透视变换到俯视图后的图像，输出为像素级别的车道线检测结果；</li>
</ul>
<p>历史 N 帧点云需要经过 ego-motion 补偿到当前本车位置，补偿后的点云只对运动物体会存在变形，而网络正好需要忽视运动物体。通过地面估计得到了俯视图下稠密的地面估计后，就可以将前视图的图像投影到俯视图下了。具体的过程为：取地面估计的三维点(高度+像素坐标经过分辨率变换后的物理坐标)，投影到图像上，然后双线性插值取得图像像素值，填充至俯视图上。这种透视变换是借助 3D 点信息完成的，原理可详见 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a>。</p>
<h2 id="differentiable-warping-function">2. Differentiable Warping Function</h2>
<p>　　其实这里估计出来的地面高度就是个简陋的高精度地图，所以这种方案理论上就能消除上述问题。并且，投影的过程采用了可求导的映射方程(differentiable warping function)，所以整个算法可以端到端的训练。 <img src="/paper-reading-Deep-Multi-Sensor-Lane-Detection/STN.png" width="90%" height="90%" title="图 2. Spatial Transformer Networks"> 　　关于可求导的映射方程，这里借鉴了 DeepMind 的 Spatial Transformer Networks<a href="#2" id="2ref"><sup>[2]</sup></a> 的思想。传统卷积网络只对较小的位移有位移不变性，而 STN 引入 2D/3D 仿射/透视变换，显示得将特征层变换到有利于分类的形态，这样整个网络就具有了仿射甚至透视(位移，旋转，裁剪，尺度，歪斜)不变性。如图 2. 所示，STN 有三部分构成：</p>
<ol type="1">
<li><strong>Localisation Net</strong>，对于 2D 仿射，回归预测出仿射变换矩阵 \(\theta \in \mathbb{R}_{2\times 3}\);</li>
<li><strong>Grid Generator</strong>，根据仿射变换矩阵及仿射变换前后特征图的大小，建立仿射前后坐标映射关系；</li>
<li><strong>Sampler</strong>，根据坐标映射关系设计可求导的插值采样方法(如双线性)，从输入特征中采样出特征值填入仿射后的特征图中；</li>
</ol>
<p>　　本文则是一个透视变换矩阵 \(P\)，但是 \(P\) 不需要网络预测，其完全由激光雷达与相机的内外参决定，这个需要提前标定好。预测的地面高度通过 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a> 中的式 (3) 即可与图像坐标系建立联系，作为 Grid Generator。最后采用可求导的 Sampler，这个模块就可以嵌入到网络中，进行端到端的训练。</p>
<h2 id="loss">3. Loss</h2>
<p>　　Loss 采用 SmoothL1 Loss，其有两种构成：</p>
<ul>
<li>地面估计项<br>
<span class="math display">\[ L_{gnd} = \sum_{p\in Output Image} \Vert z_{p,gt}-z_{p,pred}\Vert \tag{1}\]</span></li>
<li>车道线检测项<br>
<span class="math display">\[ L_{lane} = \sum_{p\in Output Image} \left\Vert \left(\tau-\mathrm{min}\{d_{p,gt}, \tau\}\right)-d_{p,pred}\right\Vert \tag{2}\]</span> 其中 \(\tau\) 是车道线真值标签的衰减像素区域，高速场景设为 30，城市道路设为 20。</li>
</ul>
<h2 id="其它思考">4. 其它思考</h2>
<p>　　既然 STN 专门是用来作仿射/透视变换的，那么是否可以在不借助激光点云的情况下，用前视图图像直接回归出透视变换到俯视图的透视矩阵 \(P\) ？理论上是可行的，但是训练过程不一定能收敛，需要精心设计训练过程，以及针对斜坡还会有一定的距离误差。</p>
<h2 id="参考文献">5. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Bai, Min, et al. "Deep Multi-Sensor Lane Detection." 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018.<br>
<a id="2" href="#2ref">[2]</a> Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. "Spatial transformer networks." Advances in neural information processing systems. 2015.</p>
]]></content>
      <categories>
        <category>Lane Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Lane Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Anchor-Free Detection</title>
    <url>/Anchor-Free-Detection/</url>
    <content><![CDATA[<p>　　3D 目标检测的技术思路大多数源自 2D 目标检测，所以图像 2D 检测的技术更迭极有可能在将来影响 3D 检测的发展。目前 3D 检测基本还是 Anchor-Based 方法(也称为 Top-Down 方法)，而今年以来，Anchor-Free(也称为 bottom-Up 法) 的 2D 检测已经达到了 SOTA，所以本文来探讨下 Anchor-Free 的目标检测方法发展历程。<br>
<img src="/Anchor-Free-Detection/history.jpg" width="90%" height="90%" title="图 1. 目标检测发展历程"> 　　如图 1. 所示(图片出自<a href="https://zhuanlan.zhihu.com/p/82491218" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/82491218</a>)，每种技术思路的发展都是为了解决目标检测中的一些痛点，这些技术思路又交相互用，才推动目标检测往更简单、更高性能方向发展。列举一些主要的痛点：</p>
<ul>
<li><strong>正负样本不均衡及无法区分困难样本导致网络学习困难</strong>，two-stage;</li>
<li><strong>网络及后处理复杂</strong>，one-stage，包含 Anchor-Free 方法；</li>
<li><strong>尺度问题很难同时检测大小目标</strong>，pyramid-scale；</li>
<li><strong>框与特征的对齐问题导致提取出的目标特征有偏差</strong>，deformable；</li>
</ul>
<p>　　本文包含两大块，一块是 Anchor-Free 方法的概括总结，另一块是代表算法的详细分析。</p>
<h2 id="归纳总结">1. 归纳总结</h2>
<p>　　首先推荐下乃爷写的文章——<a href="https://zhuanlan.zhihu.com/p/68291859" target="_blank" rel="noopener">聊聊 Anchor 的“前世今生”</a>，高屋建瓴。本节也是打算聊聊 Anchor-Free 方法的来龙去脉，以及归纳总结下各算法的思路。<br>
　　由之前讨论的，其中一个比较大的问题是，目标检测中正负样本严重不平衡。这会导致网络学习时很难针对性的学习困难样本，而 two-stage 相比 one-stage 多了一级正样本的删选，所以在没有额外困难样本选择策略的情况下，two-stage 普遍比 one-stage 效果好。可以想象的是，更多 stage 这种级联结构效果会更好，但是网络会变得相当复杂。这个痛点极大地阻碍了 one-stage 以及 Anchor-Free(负样本更多) 方法的发展，OHEM 困难样本学习当然是种有效的方法，但是还不够，直到 RetinaNet 中 Focal Loss 的提出，有效解决了正负样本严重不均衡所导致的学习困难问题。由此不仅 Anchor-based one-stage 方法性能达到了 two-stage 高度，甚至 Anchor-Free 方法性能也达到了 SOTA。<br>
　　回顾 Anchor-Free 检测，最早的应该是 YOLO-v1<a href="#1" id="1ref"><sup>[1]</sup></a>，DenseBox<a href="#2" id="2ref"><sup>[2]</sup></a>，而 RetinaNet<a href="#3" id="3ref"><sup>[3]</sup></a> 中 Focal Loss 的提出，使得 Anchor-Free 方法引来爆发式发展。大体上可分为两种：</p>
<ol type="1">
<li><strong>回归目标角点，后处理需要匹配角点以生成目标框</strong>，以 CornerNet<a href="#4" id="4ref"><sup>[4]</sup></a> 为代表的一系列改进方法 CornerNet-Lite<a href="#5" id="5ref"><sup>[5]</sup></a>，CenterNet(KeyPoint Triplets)<a href="#6" id="6ref"><sup>[6]</sup></a>，ExtremeNet<a href="#7" id="7ref"><sup>[7]</sup></a>等；</li>
<li><strong>像素级别预测目标框的不同编码量</strong>，后处理很容易生成目标框，有 CenterNet(Objects as Points)<a href="#8" id="8ref"><sup>[8]</sup></a>，FCOS<a href="#9" id="9ref"><sup>[9]</sup></a>，FoveaBox<a href="#10" id="10ref"><sup>[10]</sup></a>，FSAF<a href="#11" id="11ref"><sup>[11]</sup></a>等；</li>
</ol>
<p>回归角点的方法继承了人体姿态估计的很多策略，backbone 都使用 Hourglass<a href="#14" id="14ref"><sup>[14]</sup></a> 网络，在单尺度上能提取有效的特征；而像素级别预测目标框的不同编码量，引入了 FPN<a href="#15" id="15ref"><sup>[15]</sup></a> 网络进行多尺度检测，解决大小框在同一中心点或有相同角点的情况(CenterNet 还是使用了 Hourglass 网络，因为单尺度能很容易融合 3D 检测，人体姿态估计等任务)。此外，RepPoints<a href="#12" id="12ref"><sup>[12]</sup></a> 延续了 Deformable Conv 的工作，去掉了角点的框约束，使得角点一定贴合目标的边缘，本质上基本解决了以上所列的问题，其思想很值得借鉴。</p>
<h2 id="cornernet4-cornernet-lite5">2. CornerNet<a href="#4" id="4ref"><sup>[4]</sup></a>, CornerNet-Lite<a href="#5" id="5ref"><sup>[5]</sup></a></h2>
<h3 id="网络结构">2.1. 网络结构</h3>
<p><img src="/Anchor-Free-Detection/CornerNet-arch.png" width="80%" height="80%" title="图 2.1. CornerNet 框架"> <img src="/Anchor-Free-Detection/CornerNet-arch2.png" width="80%" height="80%" title="图 2.2. CornerNet 网络结构"> 　　如图 2.1 与 2.2 所示，CornerNet 的 backbone 采用了人体关键点检测中常用的 Hourglass 网络，这种沙漏网络类似多层 FPN，能有效提取细节信息；网络最终输出的是 Top-Left Corners Heatmaps，Bottom-Right Corners Heatmaps，以及对应的 Embeddings，Offsets。这里以 Top-Left Corners 分支为例，说明其网络计算过程。 <img src="/Anchor-Free-Detection/CornerNet-block.png" width="80%" height="80%" title="图 2.3. CornerNet"> 　　如图 2.3. 所示，这里引入 Corner Pooling Module，该模块能提取角点的上下文信息，其计算过程是行最大值与列最大值的叠加。网络输出的:</p>
<ul>
<li>Score Heatmaps \(\in\mathbb{R}^{C\times H\times W}\)，每个 Channel 的监督项是个二值图，代表了是否是该类别下的角点；</li>
<li>Embeddings \(\in\mathbb{R}^{C\times H\times W}\)，每个角点都会预测一个 Embedding 值(度量空间下的值)，用来对 top-left 与 bottom-right 角点的配对；</li>
<li>Offsets \(\in\mathbb{R}^{2\times H\times W}\)，由于 \(H\times W\) 可能是原图的下采样，所以变换到原图的角点坐标会有离散偏差，需要预测 Offsets 修正，类别无关或者类别有关都可以；</li>
</ul>
<p>Inference 阶段，得到这三个输出后，还需要进行后处理才能得到目标检测框。后处理过程为：</p>
<ol type="1">
<li>对 Heatmaps 采用点 NMS 处理(可通过 \(3\times 3\) max-pooling 实现)得到分数最高的前 100 个 top-left 角点以及前 100 个 bottom-right 角点；</li>
<li>类内计算 top-left 角点与 bottom-right 角点的 Embedding L1 距离，删除大于 0.5 的配对；</li>
<li>通过 Offsets 调整配对的角点值；</li>
<li>计算配对的角点值的平均分数，作为该目标框的分数；</li>
</ol>
<p>相比 Anchor-based 方法，整个后处理还是相对较为简单，没有框之间的 IoU 计算。</p>
<h3 id="loss">2.2. Loss</h3>
<p>　　网络训练的 Loss 表示为： <span class="math display">\[ L= L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off} \tag{2.1}\]</span> 其中 \(L_{det}\) 是角点检测的 Loss 项，\(L_{pull}, L_{push}\)是 Embedding 距离监督项，\(L_{off}\)是 Offsets 的 Loss 项；\(\alpha,\beta,\gamma\)是权重。</p>
<ul>
<li>\(L_{det}\)<br>
角点检测是 pixel-level 的检测，每个角点虽然只有一个真值，但是靠近角点的像素点作为角点而构成的目标框与真值框重合度也会较高，所以在真值角点处设计高斯函数 \(e^ {-\frac{x^ 2+y^ 2}{2(r/3) ^ 2}}\) 作为标签衰减函数，\(r\) 值等于真值角点周围定义的圆的半径。圆半径由以下准则确定：四个角点为中心构成四个圆，在这区域内构成的目标框与真值框的 IoU 要小于 \(t\)(文中设为 0.3)。所以这里引入超参数 \(t\)。由此得到检测的 Loss 项： <span class="math display">\[ L_{det} = -\frac{1}{N}\sum_{c=1}^C\sum_{i=1}^H\sum_{j=1}^W
\left\{\begin{array}{l}
(1-p_{cij})^{\alpha} \mathrm{log}(p_{cij}) &amp; \mathrm{if} \; y_{cij}=1\\
(1-y_{cij})^{\beta} (p_{cij})^{\alpha} \mathrm{log}(1-p_{cij}) &amp; \mathrm{otherwise}
\end{array}\tag{2.2}\right.\]</span> 其中 \(p_{cij}\) 代表 Heatmaps 中 \(c\) 类别的 \((i,j)\) 位置预测的角点分数，\(y_{cij}\) 表示经过高斯衰减后的真值标签值。可以看出这是 Focal Loss 的变种，对平衡正负样本及学习困难样本有重要作用。</li>
<li>\(L_{off}\)<br>
原图点 \((x,y)\) 经过网络下采样后变换到 \((\lfloor\frac{x}{n}\rfloor,\lfloor\frac{y}{n}\rfloor)\)，与真值的 Offset 可表示为 \(\mathbf{o}_k=(\frac{x_k}{n}-\lfloor\frac{x_k}{n}\rfloor\, \frac{y_k}{n}-\lfloor\frac{y_k}{n}\rfloor)\)，由此可得 Offsets 的 Loss 项： <span class="math display">\[ L_{off}=\frac{1}{N}\sum_{k=1}^N\mathrm{SmoothL1Loss}(\mathbf{o}_k,\mathbf{\hat{o}}_k) \tag{2.3}\]</span></li>
<li>\(L_{pull}, L_{push}\)<br>
每个角点都会预测一个 Embedding 值，期望的是，同一个目标框的 top-left 角点与 bottom-right 角点的 Embedding 值要相近，不同框的角点的 Embedding 值差异要大，由此设计： <span class="math display">\[\left\{\begin{array}{l}
L_{pull} = \frac{1}{N}\sum_{k=1}^N\left[(e_{t_k}-e_k)^2+(e_{b_k}-e_k)^2\right] \\
L_{push} = \frac{1}{N(N-1)}\sum_{k=1}^N\sum_{j=1, j\not=k}^N\mathrm{max}(0,\Delta-|e_k-e_j|)
\end{array}\tag{2.4}\right.\]</span> 其中 \(e_{t_k}, e_{b_k}\) 分别表示 top-left 角点与 bottom-right 角点的 Embedding 值，\(e_k\) 是二者的平均值，\(\Delta\) 设定为 1。与 Offsets 一样，该 Loss 项也只作用于真值角点。</li>
</ul>
<h2 id="centernet-keypoint-triplets6">3. CenterNet: KeyPoint Triplets<a href="#6" id="6ref"><sup>[6]</sup></a></h2>
<h3 id="网络结构-1">3.1. 网络结构</h3>
<p><img src="/Anchor-Free-Detection/CenterNetKey-arch.png" width="80%" height="80%" title="图 3.1. CenterNet 框架"> 　　CenterNet 的 Motivation是：<strong>CornerNet 的 corner pooling 对目标框内的特征提取能力有限，以及角点匹配得到的目标框在没有其它约束下有时候检测结果会出错。</strong>由此，如图 3.1 所示，CenterNet 在 CornerNet 基础上增加了 Center 的预测分支，并引入 center pooling 以及 cascade corner pooling 模块。<br>
　　Inference 处理时，预测的 Center 点用于删除不合理的框。具体的，取 top-left 角点与 bottom-right 角点匹配后得到的目标框中心点，在中心点附近检测是否有 Center 点，如没有，则删除该匹配；否则，保留该目标框，并用这三个点的平均分数代表该目标框的分数。 <img src="/Anchor-Free-Detection/CenterNetKey-pool2.png" width="60%" height="60%" title="图 3.2. CenterNet pooling Module"> 　　如图 3.2 所示，CenterNet 引入 center pooling 并升级了 cascade corner pooling，这两个模块极大的提升了目标框内的特征提取融合能力，类似 ROI-pooling 的作用。具体的：</p>
<ul>
<li><strong>Center Pooling</strong>，叠加了水平和垂直方向上的最大值；</li>
<li><strong>Cascade Corner Pooling</strong>，不同于 Corner Pooling 只在角点所在的目标框边缘处取最大值，它还在目标框的内部取得最大值；</li>
</ul>
<p><img src="/Anchor-Free-Detection/CenterNetKey-pool.png" width="60%" height="60%" title="图 3.3. CenterNet pooling Module"> 　　如图 3.3 所示，这两个模块可通过不同方向的 corner pooling 组合而成，实现也较为简单。</p>
<h3 id="loss-1">3.2. Loss</h3>
<p>　　相比 CornerNet，增加了 center Heatmaps 的 Loss 项，其它都一样： <span class="math display">\[ L= L_{det}^{co} + L_{det}^{ce} + \alpha L_{pull}^{co} + \beta L_{push}^{co} + \gamma \left(L_{off}^{co}+L_{off}^{ce}\right) \tag{3.1}\]</span></p>
<h2 id="extremenet7">4. ExtremeNet<a href="#7" id="7ref"><sup>[7]</sup></a></h2>
<p><img src="/Anchor-Free-Detection/ExtremeNet-arch.png" width="80%" height="80%" title="图 4.1. ExtremeNet 框架"> 　　如图 4.1 所示，ExtremeNet 继承了 CornerNet(CenterNet) 主干，所不同的是，ExtremeNet 预测了目标的上下左右四个点，这四个点都是在目标上的，而传统的目标框上的左上及右下点则离目标有一定距离。所以输出上，角点的 Heatmaps \(\in\mathbb{I}^{4\times C\times H\times W}\)，Center 点 Heatmaps \(\in\mathbb{I}^{C\times H\times W}\)，只对角点预测 Offsets \(\in\mathbb{R}^{4\times 2\times H\times W}\)，去掉了 Embedding 的预测。</p>
<p><img src="/Anchor-Free-Detection/ExtremeNet-post.png" width="40%" height="40%" title="图 4.2. ExtremeNet 后处理"> 　　CornerNet 与 CenterNet 因为预测的角点是目标框的左上及右下点，所以 Embedding 能较好的用于角点配对，而 ExtremeNet 预测的角点可能在目标框的任意位置，所以作者采用暴力穷举匹配的方法，实验表面效果也更好。如图 4.2 所示，最后判断是否是一个匹配到的角点，与 CenterNet 类似，也是判断待匹配角点的中心角点上是否有较强的 Center 响应。</p>
<h2 id="centernet-objects-as-points8">5. CenterNet: Objects as Points<a href="#8" id="8ref"><sup>[8]</sup></a></h2>
<p><img src="/Anchor-Free-Detection/CenterNetObj-arch.png" width="80%" height="80%" title="图 5.1. CenterNet 网络结构"> 　　如图 5.1 所示，CenterNet 网络大体上还是继承了 CornerNet，在 2D 检测上，CenterNet 预测目标框的中心点 Center \(\in\mathbb{I}^{C\times H\times W}\)，中心点 Offsets \(\in\mathbb{R}^{2\times H\times W}\)，以及目标框的尺寸 size \(\in\mathbb{R}^{2\times C\times H\times W}\)。其 Loss 为： <span class="math display">\[ L_{det}=L_k + \lambda_{size}L_{size}+\lambda_{off}L_{off} \tag{5.1} \]</span> 　　Inference 的后处理只需要对 Center Heatmaps 作 3x3 的 max-pooling，<strong>不需要对目标框作 NMS</strong>！</p>
<p><img src="/Anchor-Free-Detection/CenterNetObj-tasks.png" width="40%" height="40%" title="图 5.2. CenterNet 多任务输出"> 　　此外，这种 pixel-level 的预测容易将其它任务也包含进来，如图 5.2 所示，作者还融入了 3D 检测，人体姿态估计。<br>
　　3D 检测任务中，预测项为:</p>
<ul>
<li>目标距离编码量 \(\sigma(\hat{d}_k)\in\mathrm{(0,1)}^{3\times C\times H\times W}\)，由于直接回归距离比较困难，实际距离的回归量为 \(\frac{1}{\sigma(\hat{d}_k)}-1\);</li>
<li>三围尺寸 \(\hat{\gamma}_k\in\mathbb{R}^{3\times C\times H\times W}\)，包括长，宽，高；</li>
<li>角度 \(\hat{\theta}_k\in\mathrm{[-\pi/2,\pi/2]}^{C\times H\times W}\)，直接回归比较困难，借鉴目前用的比较多的分类+回归的思想，设计编码量 \(\hat{\alpha}_k\in\mathbb{R}^{8\times C\times H\times W}\)，将角度划分为两个 bin，\(B_1=\left[-\frac{7\pi}{6},\frac{\pi}{6}\right]\)，\(B_2=\left[-\frac{\pi}{6},\frac{7\pi}{6}\right]\)，每个 bin 有四个预测量，其中两个预测量用来作 softmax 分类，另外两个预测量作相对于 bin 中心点 \(m_i\) 的 sin，cos 的 Offsets 量；</li>
</ul>
<p>综上，3D 检测的 Loss 为： <span class="math display">\[\left\{\begin{array}{l}
L_{dep} = \frac{1}{N}\sum_{k=1}^N\left\vert\frac{1}{\sigma(\hat{d}_k)}-1-d_k\right\vert \\
L_{dim} = \frac{1}{N}\sum_{k=1}^N\left\vert\hat{\gamma}_k-\gamma_k\right\vert \\
L_{ori} = \frac{1}{N}\sum_{k=1}^N\sum_{i=1}^2\left(softmax\left(\hat{b}_i,c_i\right)+c_i\left\vert \hat{a}_i-a_i\right\vert\right)
\end{array}\tag{5.2}\right.\]</span> 其中 \(c_i=\mathbb{1}(\theta\in B_i)\)，\(a_i=\left(\mathrm{sin}(\theta-m_i),\mathrm{cos}(\theta-m_i)\right)\)，预测的角度可解码为 \(\hat{\theta}=arctan2\left(\hat{a}_{i1},\hat{a}_{i2}\right)+m_i\)。</p>
<h2 id="fcos9">6. FCOS<a href="#9" id="9ref"><sup>[9]</sup></a></h2>
<h3 id="网络结构-2">6.1. 网络结构</h3>
<p><img src="/Anchor-Free-Detection/FCOS-res.png" width="40%" height="40%" title="图 6.1. FCOS 目标框定义方式"> 　　如图 6.1 所示，FCOS 提出了另一种目标框的表示方式，“参考点”+\((l,t,r,b)\)，当“参考点”是中心点时，就退化为中心点+尺寸的方式了。这种方式弱化了中心点的重要性，一定程度上“更有可能”回归出准确的目标框。 <img src="/Anchor-Free-Detection/FCOS-arch.png" width="80%" height="80%" title="图 6.2. FCOS 网络结构"> 　　如图 6.2 所示，FCOS 继承了 RetinaNet 主体网络，采用 FPN 形式，在不同尺度的特征层上进行目标检测。HourGlass 设计之初就是用于 pixel-level 的预测的，而 FPN 多尺度检测一定程度上更有利于框检测，<strong>不同尺度上检测不同大小的框能有效解决两个大小框中心点重合的情况</strong>，HourGlass 则无法解决，虽然这种情况很少。网络预测量有：</p>
<ul>
<li>Score Heatmaps \(\in\mathbb{R}^{C\times H\times W}\)，每个 Channel 的监督项是个二值图，代表了是否是该类别下的角点；</li>
<li>Regression \(\in\mathbb{R}^{4\times H\times W}\)，“参考点” 上的 \((l,t,r,b)\)；</li>
<li>Center-ness \(\in\mathbb{R}^{1\times H\times W}\)，监督“参考点”趋向于中心点，因为接近目标框边缘的“参考点”效果会比较差；</li>
</ul>
<h3 id="多尺度检测">6.2. 多尺度检测</h3>
<p>　　不同于 Hourglass 网络只在一个尺度上进行预测，FPN 在多尺度上对真值框的划分会比较复杂，基本准则是：<strong>不同尺度要检测不同尺寸的目标框，尺度越大(特征层越小)要检测的目标框尺寸越大</strong>。所以在真值框监督的划分上，具体的，如图 6.2 所示，多尺度特征表示为 \(\{P_i|i=3,4,5,6,7\}\)，对应每个特征层能回归的最大像素距离设定为 \(\{m_i|i=2,3,4,5,6,7\} = \{0,64,128,256,512,\infty\}\)。监督第 \(i\) 特征层学习的正样本真值框需满足： <span class="math display">\[ m_{i-1}&lt;\mathrm{max}(l^{gt},t^{gt},r^{gt},b^{gt})\le m_i \tag{6.1}\]</span></p>
<h3 id="loss-2">6.3. Loss</h3>
<p>　　Loss 由三部分组成：</p>
<ul>
<li><p>类别分类<br>
目标框内的所有点都作为正样本，所以直接采用 Focal Loss 中的 Loss 定义方式： <span class="math display">\[ L_{det} = -\alpha(1-p_k)^\gamma\mathrm{log}(p_k) \tag{6.2}\]</span></p></li>
<li><p>目标框回归<br>
传统的 L2 Loss 用于目标框的直接回归有两个问题：</p>
<ol type="1">
<li>目标框参数只是作独立的优化；</li>
<li>较大的目标框有较大的 Loss；</li>
</ol></li>
</ul>
<p>这里采用 UnitBox 中提出的 IoU Loss<a href="#13" id="13ref"><sup>[13]</sup></a>： <span class="math display">\[ L_{box} = -\mathrm{ln}(IoU_k) \tag{6.3} \]</span></p>
<ul>
<li>参考点中心化监督<br>
不像 CornerNet 之流，这里的参考点全作为正样本，并没有向负样本方向的权重衰减，所以为了参考点趋向于中心点，作者提出了 Center-ness，其真值监督项为： <span class="math display">\[ centerness^{gt} = \sqrt{\frac{\mathrm{min}(l^{gt},r^{gt})}{\mathrm{max}(l^{gt},r^{gt})} \times \frac{\mathrm{min}(t^{gt},b^{gt})}{\mathrm{max}(t^{gt},b^{gt})}} \tag{6.4} \]</span> 从而可用 L1 Loss 来计算该项的 Loss。</li>
</ul>
<h2 id="foveabox10">7. FoveaBox<a href="#10" id="10ref"><sup>[10]</sup></a></h2>
<p><img src="/Anchor-Free-Detection/Fovea-arch.png" width="60%" height="60%" title="图 7.1. FoveaBox 框架"> 　　如图所示，FoveaBox 完全继承了 RetinaNet 的主体网络，采用 FPN 形式。多尺度检测中的真值分配方式基本与 FCOS 一致，这里不做展开。 <img src="/Anchor-Free-Detection/Fovea-assign.png" width="60%" height="60%" title="图 7.2. FoveaBox 正负样本区域"> 　　正负样本的分配上，作者提出了 Fovea 区域，如图 7.2 所示，目标框收缩一定比例后的区域定义为正样本，收缩一定比例后的区域外定义为负样本。<br>
　　目标框的回归上，作者提出了另一种回归量，在 \((x,y)\) 像素点上，回归量定义为： <span class="math display">\[\left\{\begin{array}{l}
t_{x_1^{gt}} = \mathrm{log}\frac{2^l(x+0.5)-x_1^{gt}}{\sqrt{S_l}} \\
t_{y_1^{gt}} = \mathrm{log}\frac{2^l(y+0.5)-y_1^{gt}}{\sqrt{S_l}} \\
t_{x_2^{gt}} = \mathrm{log}\frac{x_2^{gt}-2^l(x+0.5)}{\sqrt{S_l}} \\
t_{y_2^{gt}} = \mathrm{log}\frac{y_1^{gt}-2^l(y+0.5)}{\sqrt{S_l}} \\
\end{array}\tag{7.1}\right.\]</span> 其中 \(S_l\) 为第 \(l\) 特征层设计的最大检测像素长度的平方。</p>
<h2 id="fsaf11">8. FSAF<a href="#11" id="11ref"><sup>[11]</sup></a></h2>
<p>　　网络结构及多尺度检测设置上与 FCOS，FoveaBox 并无新意。FSAF 新的东西是提出了多尺度特征层自动选择对应大小的真值目标框，用作本特征层的训练，具体选择的过程就是看每层特征层对该目标框输出的 Loss 大小，思想与 OHEM 或是 Focal Loss 差不多。该模块可与 Anchor-Based 方法一起嵌入到网络中。</p>
<h2 id="参考文献">9.参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.<br>
<a id="2" href="#2ref">[2]</a> Huang, Lichao, et al. "Densebox: Unifying landmark localization with end to end object detection." arXiv preprint arXiv:1509.04874 (2015).<br>
<a id="3" href="#3ref">[3]</a> Lin, Tsung-Yi, et al. "Focal loss for dense object detection." Proceedings of the IEEE international conference on computer vision. 2017.<br>
<a id="4" href="#4ref">[4]</a> Law, Hei, and Jia Deng. "Cornernet: Detecting objects as paired keypoints." Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br>
<a id="5" href="#5ref">[5]</a> Law, Hei, et al. "CornerNet-Lite: Efficient Keypoint Based Object Detection." arXiv preprint arXiv:1904.08900 (2019).<br>
<a id="6" href="#6ref">[6]</a> Duan, Kaiwen, et al. "Centernet: Keypoint triplets for object detection." Proceedings of the IEEE International Conference on Computer Vision. 2019.<br>
<a id="7" href="#7ref">[7]</a> Zhou, Xingyi, Jiacheng Zhuo, and Philipp Krahenbuhl. "Bottom-up object detection by grouping extreme and center points." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="8" href="#8ref">[8]</a> Zhou, X., Wang, D., &amp; Krähenbühl, P. (2019). Objects as Points arXiv preprint arXiv:1904.07850<br>
<a id="9" href="#9ref">[9]</a> Tian, Zhi, et al. "FCOS: Fully Convolutional One-Stage Object Detection." arXiv preprint arXiv:1904.01355 (2019).<br>
<a id="10" href="#10ref">[10]</a> Kong, Tao, et al. "FoveaBox: Beyond Anchor-based Object Detector." arXiv preprint arXiv:1904.03797 (2019).<br>
<a id="11" href="#11ref">[11]</a> Zhu, Chenchen, Yihui He, and Marios Savvides. "Feature selective anchor-free module for single-shot object detection." arXiv preprint arXiv:1903.00621 (2019).<br>
<a id="12" href="#12ref">[12]</a> Yang, Ze, et al. "RepPoints: Point Set Representation for Object Detection." arXiv preprint arXiv:1904.11490 (2019).<br>
<a id="13" href="#13ref">[13]</a> Yu, Jiahui, et al. "Unitbox: An advanced object detection network." Proceedings of the 24th ACM international conference on Multimedia. ACM, 2016.<br>
<a id="14" href="#14ref">[14]</a> Newell, Alejandro, Kaiyu Yang, and Jia Deng. "Stacked hourglass networks for human pose estimation." European conference on computer vision. Springer, Cham, 2016.<br>
<a id="15" href="#15ref">[15]</a> Lin, Tsung-Yi, et al. "Feature pyramid networks for object detection." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</p>
]]></content>
      <categories>
        <category>2D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>paper reading</tag>
        <tag>2D Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;FlowNet3D&quot;</title>
    <url>/paperreading-FlowNet3D/</url>
    <content><![CDATA[<p>　　本来以为这篇文章是 FlowNet<a href="#1" id="1ref"><sup>[1]</sup></a>，FlowNet2.0<a href="#2" id="2ref"><sup>[2]</sup></a> 的续作，其实不是，大概只是借鉴了其网络框架。从网络细节上来说，应该算是 PointNet<a href="#3" id="3ref"><sup>[3]</sup></a>，PointNet++<a href="#4" id="4ref"><sup>[4]</sup></a> 系列的续作，本文<a href="#5" id="5ref"><sup>[5]</sup></a>二作也是 PointNet 系列的作者。<br>
　　光流(Optical Flow)是指图像坐标系下像素点的运动(详细可见 <a href="/KLT/" title="KLT">KLT</a>)，而 Scene Flow 是三维坐标下，物理点的运动。Scene Flow 是较底层的一种信息，可进一步提取高层的语义信息，如运动分割等。</p>
<h2 id="背景">1. 背景</h2>
<h3 id="flownet-系列">1.1. FlowNet 系列</h3>
<p><img src="/paperreading-FlowNet3D/flownet.png" width="90%" height="90%" title="图 1. FlowNet"> <img src="/paperreading-FlowNet3D/refine.png" width="80%" height="80%" title="图 2. FlowNet Refinement"> 　　如图 1. 与 2. 所示，FlowNet 在特征提取编码阶段提出了两种网络结构：FlowNetSimple 以及 FlowNetCorr。FlowNetSimple 是将前后帧图像按通道维拼接作为输入，FlowNetCorr 则设计了互相关层，描述前后帧特征的相关性，从而得到像素级偏置。refinement 解码阶段则采用 FPN 形式进行上采样，这样每一层反卷积层在细化时，不仅可以获得深层的抽象信息，同时还能获得浅层的具体信息。 <img src="/paperreading-FlowNet3D/flownet2.png" width="90%" height="90%" title="图 3. FlowNet2.0"> 　　FlowNet 虽然验证了用深度学习预测光流的可行性，但是性能比不上传统方法。FlowNet2.0 在此基础上进行了三大改进：</p>
<ul>
<li><strong>增加训练数据，改进训练策略</strong>；<br>
在数据足够的情况下，证明了 FlowNetCorr 比 FlowNetSimple 较好。</li>
<li><strong>利用堆叠结构使性能得到多级提升</strong>；<br>
如图 3. 所示，采用 FlowNet2-CSS 形式堆叠一个 FlowNetCorr 以及两个 FlowNetSimple 模块，FlowNetSimple 的输入为前一模块预测的光流，原图像经过光流变换后的图像，以及与另一图像的误差，这样可以使得该模块专注去学习前序模块未预测准确的误差项。训练时，由前往后单独训练每个模块。</li>
<li><strong>针对小位移的情况引入特定的子网络进行处理</strong>；<br>
如图 3. 所示，FlowNet2-SD 网络卷积核均改为 3x3 形式，以增加对小位移的分辨率。最后再利用一个小网络将 FlowNet2-CSS 与 FlowNet2-SD 的结果进行融合。</li>
</ul>
<h3 id="pointnet-系列">1.2. PointNet 系列</h3>
<p>　　这部分详见 <a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a>。<br>
　　这里介绍下 PointNet++ 中点云采样的过程。点云采样有集中形式：</p>
<ul>
<li>格点采样<br>
空间栅格化，然后按照栅格进行点云采样；</li>
<li>随机采样<br>
</li>
<li>几何采样<br>
根据点云所在平面的曲率，将点云分成不同集合，在每一集合里面进行均匀采样，获得曲率大的地方采样点多的效果，即获得更多“细节”；</li>
<li>均匀采样</li>
</ul>
<p>PointNet++ 中采用的 Farthest Point Sample 属于均匀采样，其可以采样出特定个数的点，且比较均匀。大致过程为：</p>
<ol type="1">
<li>点云总集合为 \(\mathcal{C}\)，随机取一点，形成采样目标集合 \(\mathcal{S}\)；</li>
<li>在剩余点集 \(\mathcal{C}-\mathcal{S}\) 中取与集合 \(\mathcal{S}\) 距离最远的一点，加入目标集合 \(\mathcal{S}\)；</li>
<li>如果目标集合 \(\mathcal{S}\) 个数达到预定值，则终止，否则重复步骤 2.；</li>
</ol>
<h2 id="flownet3d-网络结构">2. FlowNet3D 网络结构</h2>
<p><img src="/paperreading-FlowNet3D/flownet3d.png" width="90%" height="90%" title="图 4. FlowNet3D"> 　　如图 4. 所示，FlowNet3D 整体思路与 FlowNetCorr 非常像，其 set conv，flow embedding，set upconv 三个层相当于 FlowNetCorr 中的 conv，correlation，upconv 层。网络结构的连接方式也比较相像，上采样的过程都有接入前面浅层的具体特征。下面重点分析下这三个层的细节。 <img src="/paperreading-FlowNet3D/flownet3d-layers.png" width="90%" height="90%" title="图 5. FlowNet3D Layers"> 　　假设两个连续帧的两堆点：\(\mathcal{P} = \{x_i\vert i = 1,...,n_1\}\) 以及 \(\mathcal{Q} = \{y_j\vert j = 1,...,n_2\}\)，其中 \(x_i, y_j \in \mathbb{R}^3\) 是每个点的物理空间坐标。Scene Flow 的目标是求解 \(\mathcal{D}=\{x_i'-x_i \vert i = 1,...,n_1\} = \{d_i\vert i=1,...,n_1\}\)，其中 \(x_i'\) 是 \(x_i\) 在下一帧的位置。图 5. 较清晰地阐述了这三个层对点云的作用：</p>
<h3 id="set-conv-layer">2.1. set conv layer</h3>
<p>　　set conv layer 就是 PointNet++ 中的 set abstraction layer，其作用相当于图像中的卷积操作，能提取环境上下文特征。假设输入 \(n\) 个点，每个点 \(p_i = \{x_i, f_i\}\)，其中 \(x_i\in \mathbb{R}^3\) 是物理坐标空间，\(f_i\in\mathbb{R}^c\) 是特征空间；输出 \(n'\) 个点，对应每个点为 \(p_j'=\{x_j',f_j'\}\)，其中 \(f_j'\in\mathbb{R}^{c'}\) 为特征空间。那么 set conv layer 可以描述为： <span class="math display">\[f_j&#39; = \max_{\left\{i\vert\Vert x_i-x_j&#39;\Vert \leq r\right\}}\left\{\mathbf{h}\left(\mathrm{concat}(f_i,x_i-x_j&#39;)\right)\right\}\]</span> 其中 \(x_j'\) 是输入的 \(n\) 个点经过 Farthest Point Sample 后的点集，感知机 \(\mathbf{h}\) 将空间 \(\mathbb{R}^{c+3}\) 映射到空间 \(\mathbb{R}^{c'}\)，然后进行 max 操作。</p>
<h3 id="flow-embedding-layer">2.2. flow embedding layer</h3>
<p>　　有了 PointNet 思想后，其实比较容易想到如何进行两个点云的特征融合提取(看论文之前，自己有想过，和论文一样⊙o⊙)。对于两个点集：\(\left\{p_i = \{x_i, f_i\}\right\}_{i=1}^{n_1}\) 以及 \(\left\{q_j = \{y_j, g_j\}\right\}_{j=1}^{n_2}\)，其中 \(x_i,y_j\in\mathbb{R}^3\)，特征量 \(f_i,g_j\in\mathbb{R}^c\)， 那么输出为：\(\left\{o_i=\{x_i,e_i\}\right\}_{i=1}^{n_1}\)，其中 \(e_i\in\mathbb{R}^{c'}\)。由此 flow embedding layer 可描述为： <span class="math display">\[e_i = \max_{\left\{j\vert\Vert y_j-x_i\Vert \leq r\right\}}\left\{\mathbf{h}\left(\mathrm{concat}(f_i,g_j,y_j-x_i)\right)\right\}\]</span> 可见，其数学形式与 set conv layer 基本一致，但是物理意义是完全不一样的， flow embedding layer 是以 \(x_i\) 为锚点，在另一堆点云中找到距离 \(r\) 范围内的点，从何提取特征，用来描述该点与另一堆点云的相关性。这里的感知机作用可以有其它形式，作者试验后发现这种方式最简单有效。</p>
<h3 id="set-upconv-layer">2.3. set upconv layer</h3>
<p>　　PointNet++ 中 refinement 过程是 3D 插值上采样与 unit pointnet 过程，这里作者参考图像中 conv2D 与 upconv2D 的关系，提出了 set upconv layer。图像中 upconv2D 可以认为是特征扩大+填0+conv的结合(插值上采样则等价于扩大+插值的过程)，那么类似的，set upconv layer 就是点云扩大后，再对每个目标点进行 set conv layer 的操作。<br>
　　作者称这种方法比纯插值上采样好(这当然了)，也有可能是称比插值上采样+unit pointnet 好？但是这种方法本质上还是插值上采样+pointnet。</p>
<h2 id="其它细节">3. 其它细节</h2>
<h3 id="training-loss">3.1. Training Loss</h3>
<p>　　输入两堆点云： \(\mathcal{P}=\{x_i\}_{i=1}^{n_1}\), \(\mathcal{Q}=\{y_j\}_{j=1}^{n_2}\)，网络预测的 Scene Flow 为 \(\mathcal{D}=F(\mathcal{P,Q;\theta})=\{d_i\}_{i=1}^{n_1}\)， 真值为 \(\mathcal{D}^*=\{d_i^*\}_{i=1}^{n_1}\)。经过 Scene Flow 变换后的点云为：\(\mathcal{P'}=\{x_i+d_i\}_{i=1}^{n_1}\)，那么经过网络预测的反向的 Scene Flow 为 \(\{d_i'\}_{i=1}^{n_1}=F(\mathcal{P',P;\theta})\)，由此定义 cycle-consistency 项 \(\Vert d_i'+d_i\Vert\)，最终的 Loss 函数为： <span class="math display">\[L(\mathcal{P,Q,D^*,\theta})=\frac{1}{n_1}\sum_{i=1}^{n_1}\left(\Vert d_i-d_i^*\vert+\lambda\Vert d_i&#39;+d_i\Vert\right)\]</span></p>
<h3 id="three-meta-architectures">3.2. Three Meta-architectures</h3>
<p><img src="/paperreading-FlowNet3D/mixture.png" width="60%" height="60%" title="图 6. 三种特征融合方式对比"> 　　如图 6. 所示，两个点云集合特征融合方式有三种，作者的 baseline 模型也是基于这三种，flow embedding layer 属于 Deep Mixture 类型。</p>
<h3 id="runtime">3.3.  Runtime</h3>
<p><img src="/paperreading-FlowNet3D/runtime.png" width="70%" height="70%" title="图 7. NIVIDA 1080 GPU with TensorFlow"> 　　速度嘛，还是比较慢的，要应用得做优化。</p>
<h3 id="applications-scan-registration-motion-segmentation">3.4.  Applications: Scan Registration &amp; Motion Segmentation</h3>
<p>　　待补充。</p>
<h2 id="实验结果">4. 实验结果</h2>
<p><img src="/paperreading-FlowNet3D/ablation.png" width="60%" height="60%" title="图 8. ablation study"> 　　如图 8. 所示，可得结论：</p>
<ul>
<li>PointNet 中 max 操作比 avg 操作效果好；</li>
<li>上采样中 upconv 比 interpolation 效果好；</li>
<li>cycle-consistency loss 项有助于提升性能；</li>
</ul>
<h2 id="参考文献">5. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Dosovitskiy, Alexey, et al. "Flownet: Learning optical flow with convolutional networks." Proceedings of the IEEE international conference on computer vision. 2015.<br>
<a id="2" href="#2ref">[2]</a> Ilg, Eddy, et al. "Flownet 2.0: Evolution of optical flow estimation with deep networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.<br>
<a id="3" href="#3ref">[3]</a> Qi, Charles R., et al. "Pointnet: Deep learning on point sets for 3d classification and segmentation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br>
<a id="4" href="#4ref">[4]</a> Qi, Charles Ruizhongtai, et al. "Pointnet++: Deep hierarchical feature learning on point sets in a metric space." Advances in neural information processing systems. 2017.<br>
<a id="5" href="#5ref">[5]</a> Liu, Xingyu, Charles R. Qi, and Leonidas J. Guibas. "Flownet3d: Learning scene flow in 3d point clouds." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
]]></content>
      <categories>
        <category>Scene Flow</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>Scene Flow</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds&quot;</title>
    <url>/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/</url>
    <content><![CDATA[<p>　　在多视角融合 3D 检测上，研究比较多的是俯视图下的激光点云以及前视图下的图像做多传感器融合，而融合点云俯视图(Bird's Eye View)与前视图(Perspective View)的特征则比较少，新鲜出炉的本文<a href="#1" id="1ref"><sup>[1]</sup></a>提供了一种较好的点云前视图与俯视图特征前融合(early fusion)方法。</p>
<h2 id="为什么要融合点云前视图特征">1. 为什么要融合点云前视图特征</h2>
<p>　　目前主流的点云检测算法，都是将点云在俯视图下以一定分辨率体素化(Voxelization)，然后用网络提取特征做 3D 检测。单纯在俯视图下提取特征虽然比单纯在前视图下做有优势，但还是存在几个问题：</p>
<ol type="1">
<li>激光点云在远处，会变得很稀疏，从而空像素会比较多；</li>
<li>行人等狭长型小目标特征所占像素会很小；</li>
</ol>
<p>将点云投影到前视图，这两个问题则能有效减弱，所以本文提出融合点云前视图特征。</p>
<h2 id="贡献点">2. 贡献点</h2>
<p>　　本文是在 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 基础上做的工作，PointPillars 主要由三个模块构成：</p>
<ul>
<li>Voxelization；</li>
<li>Point Feature Encoding；</li>
<li>CNN Backbone；</li>
</ul>
<p>本文改进了前两个模块，但是本质思想还是 PointNet 形式。其余包括 Loss 形式等与 PointPillars 一致。<br>
　　针对这两个模块，本文有两个贡献点，Dynamic Voxelization 以及 Point-level Feature Fusion，接下来作详细介绍。</p>
<h3 id="动态体素化dynamic-voxelization">2.1. 动态体素化(Dynamic Voxelization)</h3>
<p><img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/voxelization.png" width="90%" height="90%" title="图 1. 体素化过程对比"> 　　如图 1. 所示，PointPillars (包括之前的 VoxelNet 等工作)体素化的过程都是 Hard Voxelization，即 Voxel 数目要采样，每个 Voxel 里面的点数也会采样，比如 PointPillars 将每个 Voxel 的点数定义为 100 个，少于 100 个点，则作补零处理。这样会存在问题：</p>
<ul>
<li>内存消耗大，很多稀疏的区域导致体素中要补零的内存很多；</li>
<li>采样导致信息丢失；</li>
<li>采样导致检测输出有一定的不一致性；</li>
<li>不能作点级别的特征融合；</li>
</ul>
<p>　　由此提出动态体素化(Dynamic Voxelization)，取消所有的采样过程，为什么可以这么做呢？其实这么做也比较自然，PointPillars 中 PointNet 网络将 \((P, N, D)\) 特征映射为 \((P, N, C)\)，这里就是多层感知机将输入的 channel 维度从 \(D\) 变换到 \(C\)，与其它两个维度没有关系，而接下来做的 max-pooling 操作则将 \(N\) 维(N 个点)压缩到 1，PointPillars 中每个柱子的 N 是采样成一样的。但是可以不一样！这就是本文的动态体素化思想了。</p>
<h3 id="点级别特征融合point-level-feature-fusion">2.2. 点级别特征融合(Point-level Feature Fusion)</h3>
<p>　　<a href="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/" title="MMF">MMF</a> 以 Voxel-level 将前视图的图像特征融合到俯视图的点云特征中，并以 ROI-level 融合图像前视图特征及点云俯视图特征做检测分类，本文则提出了更加前序的特征融合-Point-level 融合。 <img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/MVF.png" width="90%" height="90%" title="图 2. 点级别特征融合框架"> 　　如图 2. 所示，首先将每个点的特征(x,y,z,intensity...)映射到高维度，然后经过 FC+Maxpool(PointNet 形式) 得到标准卷积网络需要的输入数据形式，再经过 Convolution Tower 模块进行环境上下文特征提取，最终每个体素的特征作为体素内每个点的特征，由此拼接成总的点特征。<br>
<img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/encoding.png" width="40%" height="40%" title="图 3. Convolution Tower"> 　　其中 Convolution Tower 网络结构如图 3. 所示，输入输出的尺寸保持不变，类似于 FPN 结构。<br>
　　最终每个点的特征由三部分构成：</p>
<ul>
<li>自身特征维度映射；</li>
<li>俯视图下抽取的 Voxel 级别特征，有一定的感受野；</li>
<li>前视图下抽取的 Voxel 级别特征，有一定的感受野；</li>
</ul>
<p>　　俯视图下点云特征提取过程我们比较熟悉了，这里再详细介绍下点云在前视图下提取特征的过程。可直接看 <a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a> 中 2. Cylindrical View 的描述。</p>
<h2 id="实验结果">3. 实验结果</h2>
<p><img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/eval.png" width="90%" height="90%" title="图 4. 实验结果"> 　　网络参数配置可详见论文，图 4. 是在 Waymo 公开数据集上的实验结果。可知：</p>
<ol type="1">
<li>动态体素化在全距离范围内对检测都有一定的提升；</li>
<li>融合前视图特征能有效提升提升检测性能，尤其是远距离情况，距离越远，提升越明显；</li>
<li>融合前视图特征对小目标提升更加明显，如行人；</li>
</ol>
<h2 id="参考文献">4. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Zhou, Yin, et al. "End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds." arXiv preprint arXiv:1910.06528 (2019).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>Apply IPM in Lane Detection from BEV</title>
    <url>/lane-det-from-BEV/</url>
    <content><![CDATA[<p>　　车道线检测(Lane Detection)是 ADAS 系统中重要的功能模块，而对于 L4 自动驾驶系统来说，在不完全依赖高精度地图的情况下，车道线检测结果也是车辆运动规划的重要输入信息。由于俯视图(BEV, Bird's Eye View)下做车道线检测相比于前视图，有天然的优势，所以本文根据几篇论文(就看了两三篇)及项目经验，探讨总结俯视图下做车道线检测的流程方案，并主要介绍 IPM 逆透视变换原理，<a href="#0" id="0ref">[0]</a>为车道线检测资源集。</p>
<h2 id="流程框架">1. 流程框架</h2>
<p>　　由于激光点云的稀疏性，目前车道线检测主要还是依靠图像，激光点云数据当然可作为辅助输入。由此归纳一种可能的粒度较粗的俯视图下车道线检测的流程：</p>
<ol type="1">
<li>IPM 逆透视变换，将图像前视图变换为俯视图；</li>
<li>网络，提取特征，进行像素级别的分类或回归；</li>
<li>后处理，根据网络输出作相应后处理，网络输出可能是像素级别预测；</li>
</ol>
<p>网络相对比较成熟，后处理则在不同网络方法下复杂度差异很大，这里不做讨论。接下来主要讨论如何进行逆透视变换。</p>
<h2 id="ipm-逆透视变换">2. IPM 逆透视变换</h2>
<p>　　设变换前后图像坐标为 \((u,v)\), \((u',v')\), 对于仿射变换(Affine transformation)，变换前后保持了线的平行性，其变换矩阵 A： <span class="math display">\[\begin{bmatrix}
u&#39; \\
v&#39; \\
\end{bmatrix} = A
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix} =
\begin{bmatrix}
a_{11} &amp;a_{12} &amp;a_{13} \\
a_{21} &amp;a_{22} &amp;a_{23}
\end{bmatrix}
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix} \tag{1}\]</span> 对透视变换，可表示为： <span class="math display">\[\begin{bmatrix}
u&#39; \\
v&#39; \\
1 \\
\end{bmatrix} = s\cdot P
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix} = \frac{1}{p_{31}u+p_{32}v+p_{33}}
\begin{bmatrix}
p_{11} &amp;p_{12} &amp;p_{13} \\
p_{21} &amp;p_{22} &amp;p_{23} \\
p_{31} &amp;p_{32} &amp;p_{33} \\
\end{bmatrix}
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix} \tag{2}\]</span></p>
<p>用于前视图到俯视图的 IPM 逆透视变换本质上还是透视变换，变换矩阵 \(P\in \mathbb{R}^{3\times3}\) 有 8 个自由度。</p>
<h3 id="ipminverse-perspective-mapping">2.1. IPM(Inverse Perspective Mapping)</h3>
<p><img src="/lane-det-from-BEV/coords.png" width="60%" height="60%" title="图 1. 坐标关系"> 　　世界(road)坐标系与相机坐标系如图 1. 所示，设 \((u',v')\) 表示图像像素坐标系下的点，\((X_w,Y_w,0)\) 表示世界坐标系下地面上的点坐标，\((u,v)\)表示俯视图像素坐标点，<strong>IPM 假设地面是平坦的</strong>。那么根据相机透视变换原理，可得： <span class="math display">\[\begin{align}
\begin{bmatrix}
u&#39; \\
v&#39; \\
1
\end{bmatrix} &amp;= K_{cam}\frac{1}{Z_{cam}}T_{world}^{cam}
\begin{bmatrix}
X \\
Y \\
Z \\
1
\end{bmatrix}_{world}\\ &amp;=
\begin{bmatrix}
f_x &amp;0 &amp;u_0 \\
0 &amp;f_y &amp;v_0\\
0 &amp;0 &amp;1
\end{bmatrix}
\frac{1}{r_{31}X+r_{32}Y+t_z}
\begin{bmatrix}
R &amp;t\\
0 &amp;1
\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
0 \\
1
\end{bmatrix}_{world}\\
&amp;= \frac{1}{r_{31}X+r_{32}Y+t_z}
\begin{bmatrix}
m_{11} &amp;m_{12} &amp;m_{13}\\
m_{21} &amp;m_{22} &amp;m_{23}\\
r_{31} &amp;r_{32} &amp;t_{z}\\
\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
1 
\end{bmatrix}_{world} = \frac{Res}{Z_{cam}} P 
\begin{bmatrix}
u \\
v \\
1 
\end{bmatrix} \tag{3}
\end{align}\]</span></p>
<p>式 (3) 与式 (2) 形式一致，其中 \(P\) 为相机内参及外参，\(Res\) 为俯视图像素对物理空间尺寸的分辨率，单位为\((meter/pixel)\)。<strong>IPM 需要预先标定相机的内外参</strong>，尤其是外参 \(R\)，表示与地面平行的世界坐标系与相机成像平面的相机坐标系之间的旋转关系，一般情况下不考虑相机的横滚角以及偏航角，只考虑俯仰角。</p>
<h3 id="俯视图求解过程">2.2. 俯视图求解过程</h3>
<p>　　已知前视图，相机内外参，求解俯视图有两种思路。一种是在世界坐标系下划定感兴趣区域，另一种是在前视图图像上划定感兴趣区域。</p>
<h4 id="世界坐标系下划定感兴趣区域">2.2.1 世界坐标系下划定感兴趣区域</h4>
<p>　　这种方式很直接，假设世界坐标系下感兴趣区域是 \(x\in [X_{min},X_{max}],y\in [Y_{min},Y_{max}], z\in [Z_{min},Z_{max}]\)，设定 \(Res\)，即可生成俯视图要生成的像素图，然后通过公式 (2) 投影到前视图的亚像素上，用双线性插值获得采样值填入俯视图中即可。</p>
<h4 id="前视图图像上划定感兴趣区域">2.2.2 前视图图像上划定感兴趣区域</h4>
<p>　　基于(3)，可以求出世界坐标系下两条平行 \(z\) 轴的平行直线在图像坐标系下的交点，即<strong>消失点(Vanishing Point)</strong>。假设世界坐标系下平行 \(z\) 轴的直线表示为，点 \((x_a,x_b,x_c)\) 及方向向量 \(k(0,0,1)\)，那么可得该直线上任意一点投影到图像坐标系下表示，当 \(k\) 趋向于无穷大时，即可得到消失点坐标： <span class="math display">\[\begin{bmatrix}
u \\
v \\
1
\end{bmatrix} = \frac{1}{Z_{cam}} M 
\begin{bmatrix}
x_a \\
x_b \\
x_c + k
\end{bmatrix}_{world} = 
\begin{bmatrix}
\frac{m_{11}x_a+m_{12}x_b+m_{13}(x_c+k)}{m_{31}x_a+m_{32}x_b+m_{33}(x_c+k)} \\
\frac{m_{21}x_a+m_{22}x_b+m_{23}(x_c+k)}{m_{31}x_a+m_{32}x_b+m_{33}(x_c+k)} \\
1
\end{bmatrix} \overset{k\to\infty}{\simeq}
\begin{bmatrix}
\frac{m_{13}}{m_{33}} \\
\frac{m_{23}}{m_{33}} \\
1
\end{bmatrix}
\tag{3}
\]</span> 有了图像坐标系下的消失点坐标以后，我们就可以选定需要作透视变换的 ROI 梯形区域(逆透视变换到俯视图后，梯形变矩形)。选定梯形四个角点后，根据像素距离关系，定义俯视图下其对应的矩形框四个像素坐标点，这样能得到四组(2)方程组，足可求解自由度 8 的透视矩阵 \(P\)。OpenCV 有较成熟的函数，更详细的代码原理可见<a href="#1" id="1ref">[1]</a>。</p>
<h2 id="其它思考">3. 其它思考</h2>
<p>　　如果在俯视图下作车道线检测，IPM 是必不可少的。以上 IPM 的缺陷是有一个较强的假设：路面是平坦的。并且时间一长标定参数，尤其是外参会失效，而且距离越远，路面的不平坦导致的逆透视变换误差也会增大。但对于 ADAS 系统来说，车道偏离预警(LDW，Lane Departure Warnings) 中车道线的检测距离在 50m 已经能满足要求。如果要消除更远距离下路面不平坦所带来的影响，也是有方法可以消除的，留到日后再讨论。<br>
　　按照之前的项目经验，LDW 系统完成度可以很高，基本思路就是 IPM，parsing(segmentation)，clustering，hough，optimization 等几个步骤(这里就不能说得太细了)，更多的精力可能在指标设计及 cornercase 优化上。唯一对用户不太友好的地方就是安装时要进行相机外参(尤其是 pitch 角)的标定，当然标定方法比较简单，我们假设相机坐标系与路面平行，所以透视变换矩阵是固定的，用户只要看路面经过逆透射后，两条 \(z\) 方向的直线是否平行即可。相对于 Mobileye 这种标定巨麻烦的产品，这种标定方式算是非常友好了。此外还可以用自动外参标定方法，脑洞也可以开出很多，效果嘛看具体环境了，需要作谨慎的收敛判断。</p>
<h2 id="参考文献">4. 参考文献</h2>
<p><a id="0" href="#0ref">[0]</a> <a href="https://github.com/amusi/awesome-lane-detection" target="_blank" rel="noopener">awesome-lane-detection</a><br>
<a id="1" href="#1ref">[1]</a> <a href="https://blog.csdn.net/qq_32864683/article/details/85471800" target="_blank" rel="noopener">LDW 原理及代码</a></p>
]]></content>
      <categories>
        <category>Lane Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Lane Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Multi-Task Learning Using Uncertainty to Weigh Losses&quot;</title>
    <url>/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/</url>
    <content><![CDATA[<p>　　深度学习网络中的不确定性(Uncertainty)是一个比较重要的问题，本文<a href="#1" id="1ref"><sup>[1]</sup></a>讨论了其中一种不确定性在多任务训练中的应用。目前关于深度学习不确定性的研究基本出自本文作者及其团队，后续我会较系统得整理其研究成果，这篇文章先只讨论一个较为实用的应用。</p>
<h2 id="不确定性概述">1. 不确定性概述</h2>
<p>　　在贝叶斯模型中，可以建模两类不确定性<a href="#2" id="2ref"><sup>[2]</sup></a>：</p>
<ul>
<li><p><strong>认知不确定性(Epistemic Uncertainty)</strong>，描述模型因为缺少训练数据而存在的未知，可通过增加训练数据解决；</p></li>
<li><p><strong>偶然不确定性(Aleatoric Uncertainty)</strong>，描述了数据不能解释的信息，可通过提高数据的精度来消除；</p>
<ul>
<li><strong>数据依赖地或异方差不确定性(Data-dependent or Heteroscedastic Uncertainty)</strong>，与模型输入数据有关，可作为模型预测输出；</li>
<li><strong>任务依赖地或同方差不确定性(Task-dependent or Homoscedastic Uncertainty)</strong>，与模型输入数据无关，且不是模型的预测输出，不同任务有不同的值；</li>
</ul></li>
</ul>
<p>本文讨论同方差不确定性，其描述了不同任务间的相关置信度，所以可用同方差不确定性来设计不同任务的 \(Loss\) 权重项。</p>
<h2 id="为什么需要设计不同任务的-loss-权重项">2. 为什么需要设计不同任务的 \(Loss\) 权重项</h2>
<p><img src="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/mt_weight.png" width="90%" height="90%" title="图 1. Multi-task loss weightings"> 　　如图 1. 所示，多任务学习能提高单任务的性能，但是要充分发挥多任务的性能，那么得精心调节各任务的 \(Loss\) 权重。当任务多的时候，人工搜索最优的权重项则显得费时费力，依靠模型的同方差不确定性，我们可以自动学习权重项。</p>
<h2 id="多任务似然建模">3. 多任务似然建模</h2>
<p>　　下面推倒基于同方差不确定性的最大化高斯似然过程。设模型权重 \(\mathbf{W}\)，输入 \(\mathbf{x}\)，输出为 \(\mathbf{f^W(x)}\)。对于回归任务，定义模型输出为高斯似然形式： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \mathcal{N}\left(\mathbf{f^W(x)}, \sigma ^2\right) \tag{1}\]</span> 其中 \(\sigma\) 为观测噪声方差，描述了模型输出中含有多大的噪声。对于分类任务，玻尔兹曼分布下的模型输出概率分布为： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)},\sigma\right) = \mathrm{Softmax}\left(\frac{1}{\sigma ^2}\mathbf{f^W(x)}\right) \tag{2}\]</span> 由此对于多任务，模型输出的联合概率分布为： <span class="math display">\[p\left(\mathbf{y}_1,\dots,\mathbf{y}_K\vert\mathbf{f^W(x)}\right) = p\left(\mathbf{y}_1\vert\mathbf{f^W(x)}\right) \dots p\left(\mathbf{y}_K\vert\mathbf{f^W(x)}\right) \tag{3}\]</span></p>
<p>　　对于回归任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) \propto -\frac{1}{2\sigma ^2} \Vert \mathbf{y-f^W(x)} \Vert ^2 - \mathrm{log}\sigma \tag{4}\]</span> 对于分类任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}=c\vert\mathbf{f^W(x)}, \sigma\right) = \frac{1}{2\sigma ^2}f_c^{\mathbf{W}}(\mathbf{x})- \mathrm{log}\sum_{c&#39;} \mathrm{exp}\left(\frac{1}{\sigma^2}f^{\mathbf{W}}_{c&#39;}(\mathbf{x}) \right) \tag{5}\]</span></p>
<p>　　现同时考虑回归与分类任务，则多任务的联合 \(Loss\)： <span class="math display">\[\begin{align}
\mathcal{L}(\mathbf{W}, \sigma _1, \sigma _2) &amp;= -\mathrm{log}p\left(\mathrm{y_1,y_2}=c\vert\mathbf{f^W(x)} \right) \\
&amp;= -\mathrm{log}\mathcal{N}\left(\mathbf{y_1};\mathbf{f^W(x)}, \sigma_1^2\right) \cdot \mathrm{Softmax}\left(\mathbf{y_2}=c;\mathbf{f^W(x)},\sigma_2\right) \\
&amp;= \frac{1}{2\sigma_1^2}\Vert \mathbf{y}_1-\mathbf{f^W(x)}\Vert ^2 + \mathrm{log}\sigma_1 - \mathrm{log}p\left(\mathbf{y}_2=c\vert\mathbf{f^W(x)},\sigma_2\right) \\
&amp;= \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\frac{\sum_{c&#39;}\mathrm{exp}\left(\frac{1}{\sigma_2^2}f_{c&#39;}^{\mathbf{W}}(x)\right)}{\left(\sum_{c&#39;}\mathrm{exp}\left(f_{c&#39;}^{\mathbf{W}}(x) \right) \right)^{\frac{1}{\sigma_2^2}}} \\
&amp;\approx \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\sigma_2 \tag{6}
\end{align}\]</span></p>
<p>由此得到两个权重项，任务噪声 \(\sigma\) 越大，则该任务的误差权重越小。实际应用中，为了数值稳定，令 \(s:=\mathrm{log}\sigma^2\): <span class="math display">\[\mathcal{L}(\mathbf{W}, s_1, s_2) = \frac{1}{2}\mathrm{exp}(-s_1)\mathcal{L}_1(\mathbf{W}) + \mathrm{exp}(-s_2)\mathcal{L}_2(\mathbf{W}) + \mathrm{exp}(\frac{1}{2}s_1) + \mathrm{exp}(\frac{1}{2}s_2) \tag{7}\]</span> 对于更多任务的模型，根据任务类型也很容易扩展，网络自动学习权重项 \((s_1,s_2,...,s_n)\)。</p>
<h2 id="实验结果">4. 实验结果</h2>
<p><img src="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/mt.png" width="90%" height="90%" title="图 2. Multi-task"> <img src="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/ablation.png" width="90%" height="90%" title="图 3. 实验结果"> 　　如图 2. 所示，作者设计了同时作语义分割、实例分割、深度估计的网络，由图 3. 可知，用任务的不确定性来加权任务的 \(Loss\)，效果显著。</p>
<h2 id="参考文献">5. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Kendall, Alex, Yarin Gal, and Roberto Cipolla. "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
<a id="2" href="#2ref">[2]</a> Kendall, Alex, and Yarin Gal. "What uncertainties do we need in bayesian deep learning for computer vision?." Advances in neural information processing systems. 2017.</p>
]]></content>
      <categories>
        <category>Uncertainty</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>paper reading</tag>
        <tag>Uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Multi-Task Multi-Sensor Fusion for 3D Object Detection&quot;&quot;</title>
    <url>/paperreading-MT-MS-Fusion-for-3D-Object-Detection/</url>
    <content><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种 3D 检测的多任务多传感器融合方法。输入数据为图像以及点云，输出为地面估计，2D/3D检测，稠密深度图。为了让其它任务来帮助提升 3D 检测效果，作者设计了很多方法，工作还是比较细致且系统。<br>
<img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/算法框架.png" width="90%" height="90%" title="图 1. 算法框架"> 　　整个算法框架如图 1. 所示。点云数据还是在俯视图(BEV)下进行栅格化处理，高度切割是在地面估计归一化后的基础上来做，因为要 3D 定位的目标都是在地面上的；另一方面，图像与投影到前视图的点云数据进行合并，作为网络的输入数据。 网络结构上作者提出了两种俯视图与前视图特征融合策略：1. Point-wise feature fusion; 2. ROI-wise feature fusion. 这也是文章比较重要的一个贡献点。<br>
　　文章所提的 3D 检测方法大多数细节技巧并无新意，这里主要讨论分析文章中与传统方法不太一样的两大贡献点： 1. 俯视图与前视图特征融合策略； 2. 其它两个任务对检测任务提升的作用。</p>
<h2 id="俯视图与前视图特征融合策略">1. 俯视图与前视图特征融合策略</h2>
<p>　　由于网络输入有俯视图与前视图两个数据流，所以如何将这两个数据流进行特征级别的融合就显得尤为重要，文章提出了两种方式，backbone 网络级别的 point-wise feature fusion 以及第二阶段 ROI-wise feature fusion。</p>
<h3 id="point-wise-feature-fusion">1.1. Point-wise Feature Fusion</h3>
<p>　　3D 检测主体还是在俯视图下来做的，相比前视图对 3D 检测的处理，俯视图 3D 检测有天然的优势。因此，如何有效地将前视图的特征融合到俯视图的特征中，就显得尤为重要（俯视图特征融合到前视图相对比较简单）。<br>
<img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/point-wise.png" width="55%" height="55%" title="图 2. Point-wise Feature Fusion"> 　　如图 2. 所示，像素点级别的特征融合方式有两个模块，Multi-scale Fusion 以及 Continuous Fusion。Multi-scale Fusion 我们比较熟悉，可以采用类似 FPN 的结构实现。这里主要讨论 Continuous Fusion 模块。<br>
<img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/算法框架2.png" width="90%" height="90%" title="图 3. Deep Continuous Fusion 检测框架"> 　　Continuous Fusion 源自作者的另一篇文章<a href="#2" id="2ref"><sup>[2]</sup></a>。如图 3. 所示，该文检测框架基本就是本文的主干，其中 Fusion Layers 就是 Continuous Fusion 模块。而 continuous fusion 前身是作者团队提出的 Deep Parametric Continuous Convolution<a href="#3" id="3ref"><sup>[3]</sup></a>。</p>
<ul>
<li><p><strong>Deep Parametric Continuous Convolution</strong><br>
传统的卷积只能作用于网格结构(gird-structured)的数据上，为了能处理点云这种非网格结构的数据，<a href="#3" id="3ref">[3]</a>提出了带参数的卷积(Parametric Continuous Convolution)。对于第 \(i\) 个需要计算的特征位置，其特征值 \(\mathrm{h}_i \in \mathbb{R}^N\) 数学形式为： <span class="math display">\[ \mathrm{h}_i=\sum_j \mathbf{MLP}(x_i-x_j)\cdot \mathrm{f}_j \]</span> 其中 \(j\) 表示第 \(i\) 个点周围的点，\(\mathrm{f}_j \in \mathbb{R}^N\) 为输入特征，\(x_j\in \mathbb{R}^3\) 是点的坐标值。多层感知机 \(\mathbf{MLP}\) 则起到了参数核函数的作用，将 \(\mathbb{R}^{J\times 3}\) 映射为 \(\mathbb{R}^{J\times N}\) 空间，用作特征计算的权重值。</p></li>
<li><p><strong>Continuous Fusion Layer</strong><br>
Continuous Fusion 则没有显示得计算卷积权重的过程，这样使得特征提取能力更强，而且计算效率更高，不用存储权重值。其数学描述为： <span class="math display">\[ \mathrm{h}_i=\sum_j \mathbf{MLP}(\mathrm{concat}[\mathrm{f}_j,x_i-x_j]) \]</span> 多层感知机 \(\mathbf{MLP}\) 直接将 \(\mathbb{R}^{J\times (N+3)}\) 映射到 \(\mathbb{R}^{J\times M}\) 空间，最后再做一个 element-wise 的相加即得空间为 \(\mathbb{R}^{M}\) 的特征输出(<strong>这个和 PointNet 几乎一模一样，本质就是将每个点的特征空间升维，然后用对称函数(pooling, sum)消除无序点的影响, 只是这里输入的点的特征空间 \(N\) 可能已经很大了</strong>)。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/continuous_fusion.png" width="70%" height="70%" title="图 4. Continuous Fusion"> 具体步骤如图 4. 所示：</p>
<ol type="1">
<li>将点云投影到图像坐标系，在图像特征图上用双线性插值求取每个点对应的图像特征向量；</li>
<li>俯视图下对于每个需要求取特征的像素点，采样邻近的 \(K\) 个物理点，然后应用 Continuous Fusion，得到该像素点的特征向量；</li>
</ol></li>
</ul>
<h3 id="roi-wise-feature-fusion">1.2. ROI-wise Feature Fusion</h3>
<p>　　在俯视图上获得 3D 检测框后(见图 1.)，将其分别投影到图像特征图以及点云特征图上，图像特征图上用 ROIAlign 提取出目标框内的图像特征；点云特征图上用类似方法提取出带方向的目标框内的点云特征，两种特征合并到一起，再用网络进行 2D/3D 目标框的优化回归。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/roi-wise.png" width="60%" height="60%" title="图 5. ROI-wise Fusion"> 　　如图 5. 所示，点云特征图上的目标框是带有一定方向的，准确提取特征时会有一些问题。由于旋转框有周期性，所以将目标框分成两种情况来考虑，这样提取的特征就没有奇异性了，如图 5.2 所示。此外 3D 优化回归是在目标框旋转后的坐标系下进行的。</p>
<h2 id="多任务对检测任务的提升作用">2. 多任务对检测任务的提升作用</h2>
<p><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ablation.png" width="100%" height="100%" title="图 6. Ablation on Kitti"> <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ablation2.png" width="50%" height="50%" title="图 7. Ablation on TOR4D"></p>
<h3 id="地面估计">2.1. 地面估计</h3>
<p>　　俯视图下点云进行栅格化手工提取特征之前，作者作了一个地面归一化的操作。地面估计是在栅格分辨率下进行的，所以自然能对点云的每个栅格进行地面归一化。作者认为自动驾驶 3D 检测的目标都是在地面上的，所以地面的先验知识应该有助于 3D 定位，与 HDNET<a href="#4" id="4ref"><sup>[4]</sup></a> 思想类似。而在线地面估计(地面估计是建图的其中一个任务)不依赖离线地图，能提高系统鲁棒性。<br>
<img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ground_est.png" width="70%" height="70%" title="图 8. 目标定位误差"> 　　如图 6.,8 所示，地面估计的加入，确实使得 3D 检测性能有所提升。</p>
<h3 id="深度估计">2.2. 深度估计</h3>
<p>　　由于前视图输入的是图像以及点云的投影图，所以可进一步通过网络预测稠密的前视深度图。作者对点云的投影图作了精心的设计，这里不做展开，有可能直接投影的 \((x,y,z)\) 3 通道的投影图也够用。<br>
　　获得了前视稠密深度图后，可将其反投影到点云俯视图下，这样稀疏的点云会变得更加稠密，更有利于图像到点云的 Point-wise Feature Fusion。这里作者只在邻近取不到点云的时候用这反投影的伪雷达点(pseudo LiDARP)。如图 7. 所示，在该数据集上效果提升还是比较明显，而 Kitti 上不太明显，因为两者的相机与雷达配置不太一样。在 TOR4D 数据集上，远距离的车上点云数量更小，所以该技术效果较好。</p>
<h2 id="其它细节">3. 其它细节</h2>
<p>　　Loss 设计为： <span class="math display">\[ Loss = L_{cls} + \lambda(L_{box}+L_{r2d}+L_{r3d}) + \gamma L_{depth} \]</span> 其中 \(\lambda\) 与 \(\gamma\) 为权重项，\(L_{box}\) 为俯视图下预测的 3D 框，\(L_{r2d},L_{r3d}\) 为优化回归的 2D/3D 框。每一项的 Loss 计算方式与传统无异。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/eval.png" width="90%" height="90%" title="图 9. 算法对比"> 　　本文方法与其它方法对比如图 9. 所示。</p>
<h2 id="参考文献">4. 参考文献</h2>
<p><a id="1" href="#1ref">[1]</a> Liang, Ming, et al. "Multi-Task Multi-Sensor Fusion for 3D Object Detection." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
<a id="2" href="#2ref">[2]</a> Liang, Ming, et al. "Deep continuous fusion for multi-sensor 3d object detection." Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br>
<a id="3" href="#3ref">[3]</a> Wang, Shenlong, et al. "Deep parametric continuous convolutional neural networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
<a id="4" href="#4ref">[4]</a> Yang, Bin, Ming Liang, and Raquel Urtasun. "Hdnet: Exploiting hd maps for 3d object detection." Conference on Robot Learning. 2018.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>Traveling-in-Serbia-Montenegro-Bosnia</title>
    <url>/traveling-in-Serbia-Montenegro-Bosnia/</url>
    <content><![CDATA[<div id="dplayer0" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"theme":"#FADFA3","loop":true,"video":{"url":"https://leijie.oss-cn-shenzhen.aliyuncs.com/travel/Serbia-Montenegro-Bosnia.mp4","pic":"https://leijie.oss-cn-shenzhen.aliyuncs.com/travel/Serbia-Montenegro-Bosnia.mp4"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script>
]]></content>
      <categories>
        <category>Travel</category>
      </categories>
      <tags>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Fast and Furious&quot;</title>
    <url>/paperreading-Fast-and-Furious/</url>
    <content><![CDATA[<p>　　动态目标状态估计传统的做法是将其分解为目标检测，目标跟踪，目标运动预测三个子问题进行链式求解，这回导致上游模块的误差在下游模块中会传递并放大。考虑到跟踪与预测能帮助提升检测的性能，比如对于遮挡或远距离目标，跟踪与预测能减少检测的漏检(FN)；而误检(FP)则可通过时域相关信息消除，由此本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种联合 3D 检测，跟踪，运动预测的多任务网络。</p>
<h2 id="model-architecture">1. Model Architecture</h2>
<h3 id="data-representation">1.1. Data Representation</h3>
<p>　　雷达坐标系下，每帧点云限定范围为\((x_{min}, x_{max}, y_{min}, y_{max}, z_{min}, z_{max})\)，那么在分辨率 \(r = (dx,dy,dz)\) 下进行栅格化，可得到体素 \((C, H, W) = (\frac{z_{max}-z_{min}}{dz}, \frac{y_{max}-y_{min}}{dy}, \frac{x_{max}-x_{min}}{dx})\), 如果体素中有点云那么该体素值置为1，否则置为0，这样就得到了俯视图下的伪图像。<br>
　　此外将历史 \(T-1\) 帧点云先转换到当前本体坐标系(需要 ego motion 信息)，然后串成一起，就获得 \((T, C, H, W) \) 维的模型数据输入表示。</p>
<h3 id="model-formulation">1.2. Model Formulation</h3>
<p>　　实际输入网络的应该是 \((N, T, C, H, W) \) 维的数据，首先需要经过一个 fusion 层将数据映射到 \((N, C', H', W') \) 维，然后用一个类似与 SSD 结构的 backbone+head 网络即可。</p>
<h4 id="fusion">1.2.1. Fusion</h4>
<p><img src="/paperreading-Fast-and-Furious/fusion.png" width="80%" height="80%" title="图 1. Fusion 结构"> 　　本文提出了两种融合方式：</p>
<ul>
<li>Early Fusion<br>
如图 1. 所示，直接在 T 维度上进行一维卷积，卷积 \(kernel_ size = T\)，由此得到 \((N, C, H, W) \) 维的特征。</li>
<li>Late Fusion<br>
如图 1. 所示，通过两次 3D 卷积将 \(T=5\) 变换到 \(T=1\)，\(kernel size = (3, 3, 3)\),由此也得到 \((N, C, H, W) \) 维的特征。</li>
</ul>
<p>相比 Early Fusion，Late fusion 有更深的特征提取。</p>
<h4 id="backbonehead">1.2.2. Backbone+Head</h4>
<p><img src="/paperreading-Fast-and-Furious/head.png" width="80%" height="80%" title="图 2. Fusion 结构"> 　　backbone 采用 VGG16 结构，图 1. 可见。<br>
　　head 采用类似 SSD 检测头的形式。anchor 也是有不同比例不同尺寸的矩形组成(另一种方法是，由于俯视图下同种类别的尺寸相似性，所以针对不同类别采用同一尺寸的 anchor 即够用)，角度回归则采用 \(cos, sin\) 形式。<br>
　　如图 2. 所示，检测头有两个分之分支，第一个输出预测的分类 score map(n 个预测的 score map 是共享的)，第二个输出 n 个预测的 3D 框编码信息。</p>
<h3 id="decoding-tracklets">1.3. Decoding Tracklets</h3>
<p>　　由于有检测及预测的信息，所以可用简单的方法解析出跟踪 ID。历史的预测框信息可认为是当前的跟踪框，所以就自然得在 MOT 问题里进行求解。这里可直接计算跟踪框(历史预测框)与当前检测框的 overlap 误差项，然后将重合度高的目标框标记为同一 ID 即可。</p>
<h3 id="loss-function">1.4. Loss Function</h3>
<p>　　总的误差由分类误差与回归误差构成： <span class="math display">\[\xi = \sum\left(\alpha \cdot \xi_{cla} + \sum_{i=t,t+1,...,t+n}\xi_{reg}^t\right)\]</span> 这两项误差具体计算与传统的并无很大差别，此外作者还用了 OHEM 的策略，来平衡正负样本量巨大的差异。</p>
<h2 id="experimental-evaluation">2. Experimental Evaluation</h2>
<p><img src="/paperreading-Fast-and-Furious/test.png" width="80%" height="80%" title="图 3. ablation study"> 　　作者用了比 kitti 大的数据集，图 3. 所示，late fusion 比 early fusion 效果好一点，但是 late fusion 需要 3D 卷积。其它实验结果可参见文章。</p>
<p><a id="1" href="#1ref">[1]</a> Luo, Wenjie, Bin Yang, and Raquel Urtasun. "Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net." Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2018.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-PointPillars</title>
    <url>/paperreading-PointPillars/</url>
    <content><![CDATA[<h2 id="voxelnet-second-pointpillars">1. VoxelNet-&gt;SECOND-&gt;PointPillars</h2>
<p>　　相比于图像，激光点云数据是 3D 的，且有稀疏性，所以对点云的前期编码预处理尤其重要，目前大多数算法都是在鸟瞰图下进行点云物体检测，由此对点云的编码预处理主要有两大类方法：</p>
<ol type="1">
<li>以一定的分辨率将点云体素化，每个垂直列中的体素集合被编码成一个固定长度，手工制作的特征，最终形成一个三维的伪图像，以此为代表的方法有 MV3D，AVOD，PIXOR，Complex YOLO；</li>
<li>PointNet 无序点云处理方式，以此为代表的方法 Frustum PointNet<a href="#1" id="1ref"><sup>[1]</sup></a>, VoxelNet<a href="#2" id="2ref"><sup>[2]</sup></a>，SECOND<a href="#3" id="3ref"><sup>[3]</sup></a>，后两者是在鸟瞰图下进行编码的，需要 3D 卷积运算；</li>
</ol>
<p>　　本文提出的 PointPillar<a href="#4" id="4ref"><sup>[4]</sup></a> 是延续 VoxelNet，SECOND 的工作，VoxelNet 将 PointNet(<a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a>) 思想引入体素化后的体素特征编码中，然后采用 3D 卷积做特征提取，再用传统的 2D 卷积进行目标检测；SECOND 则考虑到点云特征的稀疏性，用 2D 稀疏卷积代替传统卷积，速度得到了很大的提示。而 PointPillar 则在体素的垂直列上不做分割，从而移除了 3D 卷积的操作，其优点有：</p>
<ul>
<li>无手工编码的过程，利用了点云的所有信息，且无需要调节的参数；</li>
<li>运算均为 2D 卷积，高效；</li>
<li>可迁移至其它点云数据；</li>
</ul>
<p>　　这三篇工作框架结构基本一致，由三部分组成：</p>
<ol type="1">
<li>特征编码网络(Encoder，作特征编码)，在鸟瞰图下，将点云编码为稀疏的伪图像；</li>
<li>卷积中间网络(Middle，作特征提取)，将伪图像用 backbone 网络进行特征提取；</li>
<li>区域生成网络(RPN)，也可以是 SSD FPN 等检测头的改进，用于分类和回归 3D 框，与图像检测不一样的地方是，点云鸟瞰图下的最后一层特征层不能很小；</li>
</ol>
<p><img src="/paperreading-PointPillars/PointPillar.png" width="100%" height="100%" title="图 3. PointPillar 网络框架"> 　　如图 1. 所示，本文 Pointpillar 主要的工作集中在特征编码网络，所以以下主要介绍其特征编码网络方式，以及实现细节。</p>
<h2 id="特征编码">2. 特征编码</h2>
<p>　　Pointpillar 只对 \(x-y\) 平面作 \(H\times W\) 栅格化，栅格化后形成 \(H\times W=P\) 个柱子(Pillar)，每个柱子采样出 \(N\) 个点，每个点编码为 \(D=9\) 维的向量：\(\{x,y,z,r,x_c,y_c,z_c,x_p,y_p \}\)，其中 \(\{x_c,y_c,z_c\}\) 为该点与柱子内所有点的均值点的距离，\(x_p,y_p \) 为该点与柱子中心的距离。综上最后形成\((D,P,N )\) 维的张量，然后用 PointNet 网络输出 \((C,P,N )\) 维的张量，最后用 \(MAX\) 操作输出 \((C,P) = (C,H,W)\) 的伪图像。</p>
<h2 id="实现细节">3. 实现细节</h2>
<ol type="1">
<li><p>特征编码</p>
<ul>
<li>只取有点的柱子，所以 \(P &lt; H\times W\)</li>
<li>计算量较大，需要并行加速，我复现的时候是将柱子信息离线存下来的</li>
<li>pointpillar 方式可能只比高度体素采样方式效果高一点</li>
</ul></li>
<li><p>训练</p>
<ul>
<li>针对不同的类别设定唯一尺寸的 anchor，角度上旋转 90 度，所以每个点上每个类别是有两个 anchor</li>
<li>正负样本严重不均衡，所以需要 OHEM 或者 focalloss 技术</li>
</ul></li>
</ol>
<p><a id="1" href="#1ref">[1]</a> Qi, Charles R., et al. "Frustum pointnets for 3d object detection from rgb-d data." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
<a id="2" href="#2ref">[2]</a> Zhou, Yin, and Oncel Tuzel. "Voxelnet: End-to-end learning for point cloud based 3d object detection." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
<a id="3" href="#3ref">[3]</a> Yan, Yan, Yuxing Mao, and Bo Li. "Second: Sparsely embedded convolutional detection." Sensors 18.10 (2018): 3337.<br>
<a id="4" href="#4ref">[4]</a> Lang, Alex H., et al. "PointPillars: Fast encoders for object detection from point clouds." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>PointNet 系列论文详读</title>
    <url>/PointNet-%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%AF%A6%E8%AF%BB/</url>
    <content><![CDATA[<p>　　写了半天，发现还是网上一篇文章整理的好，直接上传递门吧：<a href="https://zhuanlan.zhihu.com/p/44809266" target="_blank" rel="noopener">PointNet 系列论文解读</a></p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>Point Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title>卡尔曼滤波器在三维目标状态估计中的应用</title>
    <url>/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%9C%A8%E4%B8%89%E7%BB%B4%E7%9B%AE%E6%A0%87%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>　　目前主流的三维目标的状态估计方法（也可称为 MOT 问题）主要包括三部分：<strong>1. 检测</strong>，出单帧三维目标信息；<strong>2. 跟踪</strong>，前后帧数据关联出 ID 信息；<strong>3. 滤波</strong>，平滑估计状态信息。这里的“跟踪”只是狭义地指出 ID 的过程，“滤波”也就是综述 <a href="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/" title="Multiple-Object-Tracking-A-Literature-Review">Multiple-Object-Tracking-A-Literature-Review</a> 中提到的 Inference 过程。Inference 过程还可以是基于优化的方法，本文我们主要讨论在自动驾驶中估计动态障碍物状态的扩展卡尔曼滤波方法。</p>
<h2 id="扩展卡尔曼滤波">1. 扩展卡尔曼滤波</h2>
<p>　　文章<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>中已经详细推导了卡尔曼滤波相关原理，这里摘抄如下。非线性系统： <span class="math display">\[\left\{\begin{array}{l}
运动方程:\quad x_k=f(x_{k-1},u_k)+w_k \\
测量方程:\quad z_k=h(x_k)+v_k
\end{array}\tag{1}\right.\]</span> 滤波的两个步骤：</p>
<ol type="1">
<li><strong>预测（Predict）</strong><br>
计算先验： <span class="math display">\[\begin{align}
\bar{x} _k&amp;=f(\hat{x} _{k-1},u _k) \tag{2}\\
\bar{P} _k&amp;=F\hat{P} _kF^T+Q _k \tag{3}
\end{align}\]</span></li>
<li><strong>更新（Update）</strong><br>
先计算卡尔曼增益： <span class="math display">\[K_k=\bar{P}_kH_k^T(H_k\bar{P}_kH_k^T+R_k)^{-1} \tag{4}\]</span> 再计算后验概率分布： <span class="math display">\[\begin{align}
\hat{x}_k &amp;=\bar{x}_k+K(z_k-h(\bar{x})) \tag{5}\\
\hat{P}_k &amp;=(I-KH_k)\bar{P}_k \tag{6}
\end{align}\]</span></li>
</ol>
<h2 id="非线性系统构建">2. 非线性系统构建</h2>
<p>　　要构建三维目标状态估计系统，我们得分析状态量 \(x_k\)，测量量 \(z_k\)，输入量 \(u_k\)，状态转移函数（运动学方程）\(f(\cdot)\)，观测函数 \(h(\cdot)\)，以及雅克比矩阵 \(F\)，\(H\) 各是什么。 <img src="/卡尔曼滤波器在三维目标状态估计中的应用/状态量.png" width="25%" height="25%" title="图 1. 目标状态"> 　　如图1所示，<strong>我们严格限定要构建的非线性系统场景：动态目标的状态估计</strong>。对于自动驾驶中的动态目标状态估计，我们关心的状态量有水平面上目标物理位置，朝向，速度，转向速度，加速度，记为： <span class="math display">\[x_k=\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\dot{\psi}\\
a\end{bmatrix}\tag{7}\]</span> 目前主流的 3D 检测方法，能出位置，尺寸，朝向。所以测量量： <span class="math display">\[z_k=\begin{bmatrix}
x\\
y\\
\psi\\
\end{bmatrix}\tag{8}\]</span> 以上针对的是目标三维状态估计，如果是本车的状态估计，那么测量量可能可以加上本车的速度等（故限定场景）。文章 <a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="[paper_reading]-" stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving"">[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"</a> 的 3.2.1 章节中提到过一种车辆运动学模型，<strong>该模型定义了输入量：方向盘比率以及加速度（实际使用中均设为0，类似下述质点模型中的 CTRV 模型）；并增加了状态量：方向盘转角</strong>。虽然那篇文章处理的场景与本篇讨论的一致，但是那篇文章采用的是优化方法，方向盘转角可作为优化参数进行求解；而本篇讨论的滤波迭代方法，则很难确定方向盘转角。所以该模型虽然能更好的描述目标，但是可能并不是更有效的（实际中可做实验对比），这里引出几种质点模型。<br>
　　<a href="#1" id="1ref">[1]</a>中介绍了几种非线性车辆质点模型：CHCV(Constant Heading and Constant Velocity)，CTRV(Constant Turn Rate and Velocity)，CTRA(Constant Turn Rate and Acceleration)，此外应该还有 CHA(Constant Heading and Acceleration)。这些模型均没有考虑输入量，即： <span class="math display">\[u_k=\mathbb{0}\tag{9}\]</span> 这里我们依次介绍各模型（为了完整性，重写文章 <a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="[paper_reading]-" stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving"">[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"</a> 中的前转向车运动学模型），并导出状态转移函数（运动学方程）\(f(\cdot)\)，观测函数 \(h(\cdot)\)，以及雅克比矩阵 \(F\)，\(H\)。</p>
<h3 id="chcvconstant-heading-and-constant-velocity">2.1. CHCV(Constant Heading and Constant Velocity)</h3>
<p>　　该模型下，目标车辆的朝向及速度不变，即 \(\dot{\psi}=0\)。则容易写出，在时间 \(\Delta T\) 内，运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{align}
&amp; \begin{bmatrix}
x\\
y\\
\psi\\
v\\
\dot{\psi}\end{bmatrix} _k=
\begin{bmatrix}
x+v\,cos(\psi)\Delta T\\
y+v\,sin(\psi)\Delta T\\
\psi\\
v\\
0\end{bmatrix} _{k-1} \\
\iff &amp;\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\end{bmatrix} _k=
\begin{bmatrix}
x+v\,cos(\psi)\Delta T\\
y+v\,sin(\psi)\Delta T\\
\psi\\
v\\
\end{bmatrix} _{k-1} \tag{10}
\end{align}\]</span> 观测方程\(h(\cdot)\)也可得到： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
\end{bmatrix} _k=
\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\end{bmatrix} _{k} \tag{11} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=
\begin{bmatrix}
1 &amp;0 &amp;-v\,sin(\psi)\Delta T &amp;cos(\psi)\Delta T\\
0 &amp;1 &amp;v\,cos(\psi)\Delta T &amp;sin(\psi)\Delta T\\
0 &amp;0 &amp;1 &amp;0\\
0 &amp;0 &amp;0 &amp;1
\end{bmatrix} \tag{12} \\
H&amp;=\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0\\
\end{bmatrix} \tag{13}
\end{align}\]</span></p>
<h3 id="ctrvconstant-turn-rate-and-velocity">2.2. CTRV(Constant Turn Rate and Velocity)</h3>
<p>　　该模型下，目标车辆的(朝向)转向速度及线速度不变，即 \(a=0\)。则分别在 \(x,y\) 方向上，位移积分为： <span class="math display">\[\left\{\begin{array}{l}
x=\int_0^{\Delta T} v\,cos(\dot{\psi}t+\psi)dt=\frac{v}{\dot{\psi}}sin(\dot{\psi}t+\psi)\vert_0^{\Delta T}&amp;= \frac{v}{\dot{\psi}}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi)) \\
y=\int_0^{\Delta T} v\,sin(\dot{\psi}t+\psi)dt=-\frac{v}{\dot{\psi}}cos(\dot{\psi}t+\psi)\vert_0^{\Delta T}&amp;=\frac{v}{\dot{\psi}}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) \\
\end{array}\tag{14}\right.\]</span> 由此得到运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\dot{\psi}\end{bmatrix} _k=
\begin{bmatrix}
x+\frac{v}{\dot{\psi}}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi))\\
y+\frac{v}{\dot{\psi}}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) \\
\psi+\dot{\psi}\Delta T\\
v\\
\dot{\psi}\end{bmatrix} _{k-1}  \tag{15}
\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
\end{bmatrix} _k=
\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\dot{\psi}
\end{bmatrix} _{k} \tag{16} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=
\begin{bmatrix}
1 &amp;0 &amp;\frac{v}{\dot{\psi}}(cos(\dot{\psi}+\psi)-cos(\psi)) &amp;\frac{1}{\dot{\psi}}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi)) &amp;\frac{v\Delta T}{\dot{\psi}}cos(\dot{\psi}\Delta T+\psi)-\frac{v}{\dot{\psi}^2}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi)) \\
0 &amp;1 &amp;\frac{v}{\dot{\psi}}(sin(\dot{\psi}+\psi)-sin(\psi)) &amp;\frac{1}{\dot{\psi}}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) &amp;\frac{v\Delta T}{\dot{\psi}}sin(\dot{\psi}\Delta T+\psi)-\frac{v}{\dot{\psi}^2}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) \\
0 &amp;0 &amp;1 &amp;0 &amp;\Delta T\\
0 &amp;0 &amp;0 &amp;1 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;1\\
\end{bmatrix} \tag{17} \\
H&amp;=\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
\end{bmatrix} \tag{18}
\end{align}\]</span></p>
<h3 id="ctraconstant-turn-rate-and-acceleration">2.3. CTRA(Constant Turn Rate and Acceleration)</h3>
<p>　　该模型下，目标车辆的(朝向)转向速度及线加速度不变。\(x,y\) 方向上的位移积分为： <span class="math display">\[\left\{\begin{array}{l}
x&amp;=&amp;\int_0^{\Delta T} (v+at)\,cos(\dot{\psi}t+\psi)dt= \frac{a}{\dot{\psi}^2}cos(\dot{\psi}t+\psi)+\frac{v+at}{\dot{\psi}}sin(\dot{\psi}t+\psi)\vert_0^{\Delta T}\\
 &amp;=&amp; \frac{a}{\dot{\psi}^2}cos(\dot{\psi}\Delta T+\psi)+\frac{v+a\Delta T}{\dot{\psi}}sin(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}cos(\psi)-\frac{v}{\dot{\psi}}sin(\psi)\\
y&amp;=&amp;\int_0^{\Delta T} (v+at)\,sin(\dot{\psi}t+\psi)dt= \frac{a}{\dot{\psi}^2}sin(\dot{\psi}t+\psi)-\frac{v+at}{\dot{\psi}}cos(\dot{\psi}t+\psi)\vert_0^{\Delta T}\\
 &amp;=&amp; \frac{a}{\dot{\psi}^2}sin(\dot{\psi}\Delta T+\psi)-\frac{v+a\Delta T}{\dot{\psi}}cos(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}sin(\psi)+\frac{v}{\dot{\psi}}cos(\psi)\\
\end{array}\tag{19}\right.\]</span> 由此得到运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\dot{\psi}\\
a\end{bmatrix} _k=
\begin{bmatrix}
x+\frac{a}{\dot{\psi}^2}cos(\dot{\psi}\Delta T+\psi)+\frac{v+a\Delta T}{\dot{\psi}}sin(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}cos(\psi)-\frac{v}{\dot{\psi}}sin(\psi)\\
y+\frac{a}{\dot{\psi}^2}sin(\dot{\psi}\Delta T+\psi)-\frac{v+a\Delta T}{\dot{\psi}}cos(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}sin(\psi)+\frac{v}{\dot{\psi}}cos(\psi) \\
\psi+\dot{\psi}\Delta T\\
v+a\Delta T\\
\dot{\psi}\\
a\end{bmatrix} _{k-1}  \tag{20}
\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
\end{bmatrix} _k=
\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\dot{\psi}\\
a\end{bmatrix} _{k} \tag{21} \]</span> 同理可得到雅克比矩阵，由于页面限制，这里不再写出。</p>
<h3 id="chaconstant-heading-and-acceleration">2.4. CHA(Constant Heading and Acceleration)</h3>
<p>　　该模型下，此时目标车辆的朝向及线加速度不变，即 \(\dot{\psi}=0\)。\(x,y\) 方向上的位移积分为： <span class="math display">\[\left\{\begin{array}{l}
x=\int_0^{\Delta T} (v+at)\,cos(\psi)dt= \left(vt+\frac{1}{2}at^2\right)cos(\psi)\vert_0^{\Delta T}&amp;= \left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)cos(\psi) \\
y=\int_0^{\Delta T} (v+at)\,sin(\psi)dt= \left(vt+\frac{1}{2}at^2\right)sin(\psi)\vert_0^{\Delta T}&amp;= \left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)sin(\psi)  \\
\end{array}\tag{22}\right.\]</span> 由此得到运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
v\\
a\end{bmatrix} _k=
\begin{bmatrix}
x+\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)cos(\psi)\\
y+\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)sin(\psi) \\
\psi\\
v+a\Delta T\\
a\end{bmatrix} _{k-1}  \tag{23}
\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
\end{bmatrix} _k=
\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
\psi\\
v\\
a\end{bmatrix} _{k} \tag{24} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=
\begin{bmatrix}
1 &amp;0 &amp;-\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)sin(\psi) &amp;\Delta Tcos(\psi) &amp;\frac{1}{2}\Delta T^2cos(\psi) \\
0 &amp;1 &amp;\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)cos(\psi) &amp;\Delta Tsin(\psi) &amp;\frac{1}{2}\Delta T^2sin(\psi)\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;1 &amp;\Delta T \\
0 &amp;0 &amp;0 &amp;0 &amp;1
\end{bmatrix} \tag{25} \\
H&amp;=\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
\end{bmatrix} \tag{26}
\end{align}\]</span></p>
<h3 id="前转向车模型">2.5. 前转向车模型</h3>
<p>　　这里给出文章 <a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="[paper_reading]-" stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving"">[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"</a> 中的前转向车运动学模型，推导过程可见文章。这里令方向盘角度比率 \(\gamma\) 以及加速度 \(a\) 为 0，所以本质上也是个 CTRV 模型。<br>
　　引入状态变量方向盘/车轮角度 \(\delta\)（与朝向转速 \(\psi\) 类似），运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\delta\end{bmatrix} _k=
\begin{bmatrix}
x+cos(\psi)v\Delta T\\
y+sin(\psi)v\Delta T \\
\psi+\frac{tan(\delta)}{L}v\Delta T\\
v\\
\delta\end{bmatrix} _{k-1}  \tag{27}
\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}
x\\
y\\
\psi\\
\end{bmatrix} _k=
\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
\psi\\
v\\
\delta\end{bmatrix} _{k} \tag{28} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=
\begin{bmatrix}
1 &amp;0 &amp;-sin(\psi)v\Delta T &amp;cos(\psi)\Delta T &amp;0 \\
0 &amp;1 &amp;cos(\psi)v\Delta T  &amp;sin(\psi)\Delta T &amp;0  \\
0 &amp;0 &amp;1 &amp;\frac{tan(\delta)}{L}\Delta T &amp;\frac{v}{Lcos^2(\delta)}\Delta T\\
0 &amp;0 &amp;0 &amp;1 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;1\\
\end{bmatrix} \tag{29} \\
H&amp;=\begin{bmatrix}
1 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;1 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;1 &amp;0 &amp;0\\
\end{bmatrix} \tag{30}
\end{align}\]</span></p>
<h2 id="状态及参数初始化">3. 状态及参数初始化</h2>
<p>　　以上介绍了四个质点模型以及一个前转向模型，当然还有更复杂的模型，但是对于目标车辆的状态估计，由于观测量有限，而且也不能知道输入量（如果 V2X 能够实现，那就知道目标车辆的更多状态信息了），所以这些模型也基本够用。<br>
　　模型构建好之后，为了迭代，还需初始化各个状态量及协方差参数矩阵。初始化值不对，会导致迭代发散，这里初始化就会有几个问题：</p>
<ul>
<li>无法观测的状态量较难初始化，如转向速度，线加速度等；</li>
<li>观测不稳定的状态量较难初始化，如目标有截断的情况下；</li>
<li>协方差矩阵较难初始化，如状态量的协方差矩阵；</li>
</ul>
<p>　　前两个问题需要在工程实践中优化；最后一个问题（<strong>非常重要</strong>）可以让检测网络同时出预测值的不确定性(Uncertainty)，这也是深度学习中一个较为系统性的工作，后面文章再做介绍。</p>
<p><a id="1" href="#1ref">[1]</a> https://github.com/balzer82/Kalman</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>3D Detection</tag>
        <tag>autonomous driving</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>卡尔曼滤波详解</title>
    <url>/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>　　状态估计问题是指，基于初始状态信息，一系列观测量，一系列输入量，以及系统的运动模型和观测模型，来计算系统在某时刻的真实状态的估计值。卡尔曼滤波及其相关卡尔曼滤波算法是状态估计的重要方法。本文介绍卡尔曼滤波(Kalman Filter)，扩展卡尔曼滤波(Extended Kalman Filter)。</p>
<h2 id="卡尔曼滤波">1. 卡尔曼滤波</h2>
<h3 id="线性高斯系统">1.1. 线性高斯系统</h3>
<p>　　卡尔曼滤波是线性高斯系统的最优无偏估计，定义离散线性高斯系统： <span class="math display">\[\left\{\begin{array}{l}
运动方程:\quad x_k=A_kx_{k-1}+B_ku_k+w_k\\
测量方程:\quad z_k=C_kx_k+v_k
\end{array}\tag{1}\right.\]</span> 其中矩阵 \(A_k\) 为转移矩阵（transition matrix），设矩阵 \(B_k=I\) 为控制矩阵，矩阵 \(C_k\) 为观测矩阵(observation matrix)。并且所有状态和噪声均满足高斯分布： <span class="math display">\[\begin{align}
过程噪声: \quad &amp; w_k \sim N(0,Q_k)\\
测量噪声: \quad &amp; v_k \sim N(0,R_k)
\end{align}\]</span> 卡尔曼滤波估计线性高斯系统的状态分为两个步骤：</p>
<ol type="1">
<li><strong>预测（Predict）</strong><br>
计算先验： <span class="math display">\[\begin{align}
\bar{x}_ k &amp;=A_ k\hat{x}_ {k-1}+u_ k \tag{2}\\
\bar{P}_ k &amp;=A_ k\hat{P}_ {k-1}A_ k^T+Q_ k \tag{3}
\end{align}\]</span></li>
<li><strong>更新（Update）</strong><br>
先计算卡尔曼增益： <span class="math display">\[K_k=\bar{P}_kC_k^T(C_k\bar{P}_kC_k^T+R_k)^{-1} \tag{4}\]</span> 再计算后验概率分布： <span class="math display">\[\begin{align}
\hat{x}_k &amp;=\bar{x}_k+K(z_k-C_k\bar{x}) \tag{5}\\
\hat{P}_k &amp;=(I-KC_k)\bar{P}_k \tag{6}
\end{align}\]</span></li>
</ol>
<p>以下通过三种方式来推导出卡尔曼滤波器。</p>
<h3 id="通过-map贝叶斯推断推导123">1.2. 通过 MAP/贝叶斯推断推导<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a><a href="#3" id="3ref"><sup>[3]</sup></a></h3>
<p>　　状态估计问题的概率解释就是用 \(0\) 到 \(k\) 的数据（包括初始状态，观测量，输入量）来估计当前时刻的状态分布：\(P(x_k\vert x_0,u_{1:k},z_{1:k})\)。根据贝叶斯法则： <span class="math display">\[P(x_k\vert x_0,u_{1:k},z_{1:k}) \propto P(z_k\vert x_k)P(x_k\vert x_0,u_{1:k},z_{1:k-1})\tag{7}\]</span> 这三项分别为后验概率，似然，先验概率。所以状态估计可转换为该后验概率最大化（Maximize a Posterior，MAP）问题。MAP 相当于最大化似然与先验的乘积。似然由测量方程给出，先验有运动方程给出。先验部分如果考虑历史所有信息，那么可以用非线性优化框架来解；如果只考虑一阶马尔科夫性，那么就是卡尔曼滤波方法，前述线性高斯系统就满足一阶马尔科夫性。<br>
　　该系统下，假设已知 \(k-1\) 时刻的后验状态估计 \(\hat{x}_ {k-1}\) 及其协方差 \(\hat{P}_ {k-1}\)，现在要根据 \(k\) 时刻的输入和观测数据，确定 \(x_k\) 的后验分布。这里以 \((\hat{\cdot})\) 表示后验分布，\((\bar{\cdot})\) 表示先验分布。<br>
　　卡尔曼滤波器首先通过(1)中的运动方程确定 \(x_k\) 的先验分布，即预测过程。将 \(k-1\) 时刻的分布通过运动方程传递，对于均值有： <span class="math display">\[\begin{align}
\bar{x}_k &amp;=E[x_k]=E[A_kx_{k-1}+u_k+w_k]\\
&amp;=A_kE[x_{k-1}]+u_k+E[w_k]\\
&amp;=A_k\hat{x}_{k-1}+u_k
\end{align}\]</span> 对于协方差有： <span class="math display">\[\begin{align}
\bar{P}_k &amp;=E\left[(x_k-E[x_k])(x_k-E[x_k])^T\right]\\
&amp;=E\left[(A_kx_{k-1}+u_k+w_k-A_k\hat{x}_{k-1}-u_k)\cdot(A_kx_{k-1}+u_k+w_k-A_k\hat{x}_{k-1}-u_k)^T\right]\\
&amp;=A_kE\left[(x _{k-1}-\hat{x} _{k-1})\cdot(x _{k-1}-\hat{x} _{k-1})^T\right]A_k^T+E[w_kw_k^T]\\
&amp;=A_k\hat{P} _{k-1}A _{k-1}^T+Q _k
\end{align}\]</span></p>
<p>由此可得<strong>预测过程</strong>： <span class="math display">\[\begin{align}
&amp;P(x_k\vert x_0,u_{1:k},z_{1:k-1})=N\left(A_k\hat{x}_{k-1}+u_k,A_k\hat{P}_{k-1}A_k^T+Q_k\right)\tag{8}\\
\iff &amp;公式 (2),(3)
\end{align}\]</span> 　　另一方面，通过(1)中的观测方程，可以得到在某个状态下观测数据应该为： <span class="math display">\[P(z_k\vert x_k)=N(C_kx_k,R)\tag{9}\]</span> 由公式(7)可知，状态的后验概率分布由预测量以及测量量融合得到，这个融合的过程是两个高斯状的概率分布进行相乘，即 \(x_k\) 的后验概率： <span class="math display">\[N(\hat{x}_k,\hat{P}_k)=N(C_kx_k,R)\cdot N(\bar{x}_k,\bar{P}_k)\tag{10}\]</span> 比较该式指数部分即可得到<strong>更新过程</strong>： <span class="math display">\[\begin{align}
&amp; (x_k-\hat{x}_k)^T\hat{P}_k^{-1}(x_k-\hat{x}_k)=(z_k-C_kx_k)^TR^{-1}(z_k-C_kx_k)+(x_k-\bar{x}_k)^T\bar{P}_k^{-1}(x_k-\bar{x}_k)\\
\iff &amp; 
\left\{\begin{array}{l}
二次项系数:\quad \hat{P}_k^{-1}=C_k^TR^{-1}C_k+\bar{P}_k^{-1}\\
一次项系数:\quad 2\hat{x}_k^T\hat{P}_k^{-1}x_k=2z_k^TR^{-1}C_kx_k+2\bar{x}_k^T\bar{P}_k^{-1}x_k
\end{array}\right. \tag{11} \\
\iff &amp; 
\left\{\begin{array}{l}
I=\hat{P}_kC_k^TR^{-1}C_k+\hat{P}_k\bar{P}_k^{-1}\\
\hat{x}_k=\hat{P}_kC_k^TR^{-1}z_k+\hat{P}_k\bar{P}_k^{-1}\bar{x}_k
\end{array}\right. 令 K=\hat{P}_kC_k^TR^{-1} \\
\iff &amp; 
\left\{\begin{array}{l}
I=KC_k+\hat{P}_k\bar{P}_k^{-1}\\
\hat{x}_k=Kz_k+(I-KC_k)\bar{x}_k
\end{array}\right. \\
\iff &amp; 式 (4),(5),(6)
\end{align}\]</span> 　　对于更新过程，<a href="#3" id="3ref">[3]</a>中提出了另一种更加形象的证明方法。如图1所示，容易得到小车模型的运动方程： <span class="math display">\[\begin{bmatrix}
x _k\\
\dot{x} _k\\
\end{bmatrix}=
\begin{bmatrix}
1 &amp; \Delta k\\
0 &amp; 1\\
\end{bmatrix}
\begin{bmatrix}
x _{k-1}\\
\dot{x} _{k-1}\\
\end{bmatrix}+
\begin{bmatrix}
\frac{(\Delta k)^2}{2}\\
\Delta k\\
\end{bmatrix}a_k
\]</span> 其中 \(a_k\) 为加速度输入量，对比式(1)也容易得到转移矩阵与控制矩阵。预测过程的证明方式与上述一致，下面简述其更新过程的证明，详见<a href="#3" id="3ref">[3]</a>。 <img src="/卡尔曼滤波详解/小车.png" width="100%" height="100%" title="图 1. 小车模型"> 　　如图1所示，红色区域代表预测量 \({x}_k\) 的概率分布高斯函数；蓝色代表测量量 \(z_k\) 概率分布的高斯函数，测量装置为左侧的 ToF 装置，单位为秒。绿色代表状态的后验概率分布 \(_k\)，由预测量的概率(先验)与测量量的概率(似然)相乘得到。由式(10)可知，两个高斯函数相乘还是高斯函数（但是是尺度变化的高斯函数，Scaled Gaussian<a href="#4" id="4ref"><sup>[4]</sup></a>），上面的证明过程直接比较二次项与一次项，这里是直接写出新的高斯分布均值方差与另两个高斯分布均值方差的关系，<strong>本质上都是比较自变量前面的系数</strong>，非系数是不相等的，还有 Scaled 项。由此可得到更新过程。要注意的是，高斯分布相乘时，要注意单位的转换(<strong>即需要满足式(10)的单位形式</strong>)，这里的观察矩阵就是基于测量装置的测量单位(秒)与状态单位(米，米/秒)的转换值。</p>
<h2 id="扩展卡尔曼滤波">2. 扩展卡尔曼滤波</h2>
<h3 id="非线性非高斯系统">2.1. 非线性非高斯系统</h3>
<p>　　通常系统（如 SLAM）的运动方程和观测方程是非线性函数，写成一般形式： <span class="math display">\[\left\{\begin{array}{l}
运动方程:\quad x_k=f(x_{k-1},u_k)+w_k\\
测量方程:\quad z_k=h(x_k)+v_k
\end{array}\tag{12}\right.\]</span> 扩展卡尔曼滤波估计非线性系统的状态与卡尔曼滤波类似，也分为两个步骤：</p>
<ol type="1">
<li><strong>预测（Predict）</strong> 计算先验： <span class="math display">\[\begin{align}
\bar{x} _k&amp;=f(\hat{x} _{k-1},u _k) \tag{13}\\
\bar{P} _k&amp;=F\hat{P} _kF^T+Q _k \tag{14}
\end{align}\]</span></li>
<li><strong>更新（Update）</strong> 先计算卡尔曼增益： <span class="math display">\[K_k=\bar{P}_kH_k^T(H_k\bar{P}_kH_k^T+R_k)^{-1} \tag{15}\]</span> 再计算后验概率分布： <span class="math display">\[\begin{align}
\hat{x}_k &amp;=\bar{x}_k+K(z_k-h(\bar{x})) \tag{16}\\
\hat{P}_k &amp;=(I-KH_k)\bar{P}_k \tag{17}
\end{align}\]</span></li>
</ol>
<h3 id="通过-map贝叶斯推断推导12">2.2. 通过 MAP/贝叶斯推断推导<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a></h3>
<p>　　在某个点附件考虑运动方程与观测方程的一阶泰勒展开，只保留一阶项，即线性部分，然后按照线性系统进行推导。在 \(k\) 时刻，将运动方程和观测方程在 \(\hat{x}_ {k-1},\hat{P}_ {k-1}\) 处进行线性化： <span class="math display">\[\left\{\begin{array}{l}
运动方程:\quad x_k\approx f(\hat{x}_{k-1},u_k)+F(x_{k-1}-\hat{x}_{k-1})+w_k\\
测量方程:\quad z_k\approx h(\bar{x}_k)+H(x_k-\bar{x}_k)+v_k
\end{array}\tag{18}\right.\]</span> 其中 \(F=\left.\frac{\partial f}{\partial x_{k-1}}\right\arrowvert_{\hat{x}_ {k-1}}\)， \(H=\left.\frac{\partial h}{\partial x_k}\right\arrowvert_{\bar{x}_ k}\)。<br>
　　由此可得<strong>预测过程</strong>: <span class="math display">\[\begin{align}
&amp;P(x_k\vert x_0,u_{1:k},z_{1:k-1})=N\left(f(\hat{x}_{k-1},u_k),F\hat{P}_{k-1}F^T+Q_k\right)\tag{19}\\
\iff &amp;公式 (13),(14)
\end{align}\]</span> 　　另一方面，通过(18)中的观测方程，可以得到在某个状态下观测数据应该为： <span class="math display">\[P(z_k\vert x_k)=N(h(\bar{x})+H(x_k-\bar{x}_k),R)\tag{20}\]</span> 由贝叶斯公式，可得 \(x_k\) 的后验概率： <span class="math display">\[N(\hat{x}_k,\hat{P}_k)=N(h(\bar{x})+H(x_k-\bar{x}_k),R))\cdot N(\bar{x}_k,\bar{P}_k)\tag{21}\]</span> 类似卡尔曼推导过程，由此可得到更新过程式(15)，(16)，(17)。</p>
<p><a id="1" href="#1ref">[1]</a> 高翔, 张涛, 颜沁睿, 刘毅, 视觉SLAM十四讲：从理论到实践, 电子工业出版社, 2017<br>
<a id="2" href="#2ref">[2]</a> T. D. Barfoot. State Estimation for Robotics. Cambridge University Press, 2017.<br>
<a id="3" href="#3ref">[3]</a> Faragher, Ramsey. "Understanding the basis of the Kalman filter via a simple and intuitive derivation." IEEE Signal processing magazine 29.5 (2012): 128-132.<br>
<a id="4" href="#4ref">[4]</a> Bromiley, Paul. "Products and convolutions of Gaussian probability density functions." Tina-Vision Memo 3.4 (2003): 1.</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>MOT</tag>
      </tags>
  </entry>
  <entry>
    <title>KLT 光流算法详解</title>
    <url>/KLT/</url>
    <content><![CDATA[<p>　　光流（Optical Flow）是物体在三维空间中的运动（运动场）在二维图像平面上的投影，由物体与相机的相对速度产生，反映了微小时间内物体对应的图像像素的运动方向和速度。<br>
　　KLT 是基于光流原理的一种特征点跟踪算法，本文首先介绍光流原理，然后介绍 KLT 及相关 KLT 变种算法。</p>
<h2 id="optical-flow">1. Optical Flow</h2>
<p>　　光流法假设：</p>
<ul>
<li>亮度恒定，图像中物体的像素亮度在连续帧之间不会发生变化；</li>
<li>短距离(短时)运动，相邻帧之间的时间足够短，物体运动较小；</li>
<li>空间一致性，相邻像素具有相似的运动；</li>
</ul>
<p>　　记 \(I(x,y,t)\) 为 \(t\) 时刻像素点 \((x,y)\) 的像素值，那么根据前两个假设，可得到： <span class="math display">\[I(x,y,t)=I(x+dx,y+dy,t+dt)\]</span> 一阶泰勒展开： <span class="math display">\[I(x+dx,y+dy,t+dt)=I(x,y,t)+\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy+\frac{\partial I}{\partial t}dt\]</span> 由此可得： <span class="math display">\[\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy+\frac{\partial I}{\partial t}dt=0 \iff \frac{\partial I}{\partial x}\frac{dx}{dt}+\frac{\partial I}{\partial y}\frac{dy}{dt}=-\frac{\partial I}{\partial t}\]</span> 记 \(\left(\frac{dx}{dt},\frac{dy}{dt}\right)=(u,v)\)，即为所要求解的像素光流；\(\left(\frac{\partial I}{\partial x},\frac{\partial I}{\partial y}\right)=(I_x,I_y)\) 为像素灰度空间微分；\(\frac{\partial I}{\partial t}=I_x\) 为像素坐标点的时间灰度微分。整理成矩阵形式： <span class="math display">\[\begin{bmatrix}
I_x &amp;I_y\\
\end{bmatrix}
\begin{bmatrix}
u\\
v\\
\end{bmatrix}=-I_t
\]</span> 该式表示相同坐标位置的时间灰度微分是空间灰度微分与这个位置上相对于观察者的速度的乘积。由空间一致性假设，对于周围多个点，有： <span class="math display">\[\begin{bmatrix}
I_{x1} &amp;I_{y1}\\
I_{x2} &amp;I_{y2}\\
I_{x3} &amp;I_{y3}\\
\vdots &amp;\vdots \\
\end{bmatrix}
\begin{bmatrix}
u\\
v\\
\end{bmatrix}=-
\begin{bmatrix}
I_{t1}\\
I_{t2}\\
\vdots\\
\end{bmatrix} \iff A\vec{u}=b
\]</span> 这是标准的线性方程组，可用最小二乘法求解 \(\vec{u}=\left(A^ TA\right)^ {-1}A^ Tb\)，也可以迭代求解。这种方式得到的光流，称为 Lucas-Kanade 算法。</p>
<h2 id="klt">2. KLT</h2>
<p>　　KLT 算法本质上也基于光流的三个假设，不同于前述直接比较像素点灰度值的作法，KLT 比较像素点周围的窗口像素，来寻找最相似的像素点。由光流假设，在很短时间 \(\tau\) 内，前后两帧图像满足： <span class="math display">\[J(A\mathrm{x}+d)=I(\mathrm{x}), 其中 A=1+D=1+\begin{bmatrix}
d_{xx} &amp; d_{xy}\\
d_{yx} &amp; d_{yy}\\
\end{bmatrix}\]</span> 像素位移(displacement)向量满足仿射运动模型(Affine Motion) \(=Dx+d\)，其中 \(D\) 称为变形矩阵(Deformation Matrix)，\(d\) 称为位移向量(Displacement Vector)。\(D\) 表示两个像素窗口块运动后的变形量，所以当窗口较小时，会比较难估计。通常 \(D\) 可以用来衡量两个像素窗口的相似度，即衡量特征点有没有漂移。而对于光流跟踪量，一般只考虑平移模型(Translation Model)： <span class="math display">\[J(\mathrm{x}+d)=I(\mathrm{x})\]</span> 　　为了普遍性，我们用仿射运动模型来推到 KLT 算法原理。在像素窗口下，构造误差函数： <span class="math display">\[\epsilon=\iint_W [J(A\mathrm{x}+d)-I(x)]^2 w(\mathrm{x})d\mathrm{x}\]</span> 其中 \(w(\mathrm{x})\) 是权重函数，可定义为高斯形式。上式分别对变量 \(D\) 和 \(d\) 求导： <span class="math display">\[\left\{\begin{array}{l}
\frac{\partial \epsilon}{\partial D}=2\iint_W[J(A\mathrm{x}+d)-I(\mathrm{x})]g\,\mathrm{x}^T\,w\,d\mathrm{x}&amp;=0\\
\frac{\partial \epsilon}{\partial d}=2\iint_W[J(A\mathrm{x}+d)-I(\mathrm{x})]g\,w\,d\mathrm{x}&amp;=0\\
\end{array}\right.\]</span> 其中 \(g=\left(\frac{\partial J}{\partial x},\frac{\partial J}{\partial y}\right)^ T\)。记光流 \(u=D\mathrm{x}+d\)，则对运动后的像素点进行泰勒展开： <span class="math display">\[J(A\mathrm{x}+d)=J(x)+g^T(u)\]</span> 仿射运动模型结果可见<a href="#1" id="1ref">[1]</a><a href="#5" id="5ref">[5]</a>，这里给出平移运动模型结果。令 \(D=0\)： <span class="math display">\[\begin{align}
&amp;\iint_W[J(A\mathrm{x}+d)-I(\mathrm{x})]g\,w\,d\mathrm{x}=0\\
\iff &amp;\iint_W[J(\mathrm{x})-I(\mathrm{x})]g\,w\,d\mathrm{x}=-\iint_Wg^T\,\mathrm{d}\,g\,w\,d\mathrm{x}=-\left[\iint_Wg\,g^T\,w\,d\mathrm{x}\right]\mathrm{d}\\
\iff &amp;Z\mathrm{d}=e
\end{align}\]</span> 其中 \(Z\) 是 \(2\times 2\) 矩阵，\(e\) 是 \(2\times 1\) 向量。这是线性方程组优化问题，当 \(Z\) 可逆时，这个方程可容易求解。因为推导过程用到了泰勒展开，所以只有当像素位移较小时，才成立。实际操作中，一般迭代式的来求解，每次用上次结果做初始化，进一步求解(In a Newton-Raphson Fasion)。</p>
<h2 id="pyramidal-iterative-klt">3. Pyramidal Iterative KLT</h2>
<p>　　以上标准的迭代式 KLT 计算过程只在位移较小时成立（泰勒展开），所以需要更优的金字塔式迭代求解。图像金字塔有多重定义方式，这里定义： <span class="math display">\[\begin{align}
I^L(x,y)&amp;=\frac{1}{4}I^{L-1}(2x,2y)\\
&amp;+\frac{1}{8}\left(I^{L-1}(2x-1,2y)+I^{L-1}(2x+1,2y)+I^{L-1}(2x,2y-1)+I^{L-1}(2x,2y+1)\right)\\
&amp;+\frac{1}{16}\left(I^{L-1}(2x-1,2y-1)+I^{L-1}(2x+1,2y+1)+I^{L-1}(2x-1,2y+1)+I^{L-1}(2x+1,2y-1)\right)
\end{align}\]</span> 　　特征点跟踪有两个关键指标：<strong>准确性(accuracy)</strong>，以及<strong>鲁棒性(robustness)</strong>。大的窗口，对大的运动量比较鲁棒，但是为了提高准确性，又不得不减小窗口。所以窗口的选择需要权衡跟踪准确性与鲁棒性。金字塔迭代 KLT 则能有效弱化窗口的局限性。这里介绍平移模型下金字塔迭代 KLT 算法，仿射模型算法过程可见<a href="#1" id="1ref">[1]</a><a href="#5" id="5ref">[5]</a>。<br>
　　定义金字塔迭代 KLT 算法的目标：图像 \(I\) 中某坐标点 \(\mathrm{x}\)，在图像 \(J\) 中找到其对应点 \(\mathrm{}\)。算法流程为：</p>
<blockquote>
<p>建立图像金字塔：\(\{I^ L\}_ {L=0,...,L_m}\)，\(\{J^ L\}_ {L=0,...,L_m}\)<br>
初始化光流在金字塔之间的传递值：\(g^ {L_m}=[g_x^ {L_m},g_y^ {L_m}]^ T=[0,0]^ T\)<br>
<strong>for \(L=L_m\) down to 0 with step of -1</strong></p>
<blockquote>
<p>计算图像 \(I^ L\) 中的 \(\mathrm{x}\) 坐标: \(\mathrm{x}^ L=[x,y]^ T=\mathrm{x}/2^ L\)<br>
计算空间梯度矩阵 \(Z\)<br>
初始化 KLT 迭代值：\(v^ 0=[0,0]^ T\)<br>
<strong>for \(k=1\) to \(K\) with step of 1</strong> or until \(\Vert\eta^ k\Vert\) &lt; accuracy threshold</p>
<blockquote>
<p>计算图像差矩阵 \(I^ L(\mathrm{x}^ L)-J^ L(\mathrm{x}^ L)=I^ L(x,y)-J^ L(x+g_x^ L+v_x^ {k-1},y+g_y^ L+v_y^ {k-1})\)<br>
计算图像差矩阵 \(e_k\)<br>
计算光流 \(\eta^ k=Z^ {-1}e_k\)<br>
更新下次迭代的初值 \(v^ k=v^ {k-1}+\eta^ k\)</p>
</blockquote>
<p><strong>end of for-loop on k</strong><br>
第 \(L\) 层金字塔下光流为：\(\mathrm{d}^ L=v^ K\)<br>
初始化第 \(L-1\) 层金字塔的光流： \(g^ {L-1}=[g_x^ {L-1}, g_y^ {L-1}]^ T=2(g^ L+\mathrm{d}^ L)\)</p>
</blockquote>
<p><strong>end of for-loop on L</strong> 最终的光流结果：\(\mathrm{d}=g^ 0+\mathrm{d}^ 0\)<br>
对应的 \(J\) 上的坐标点为：\(\hat{\mathrm{x}}=\mathrm{x}+\mathrm{d}\)</p>
</blockquote>
<h2 id="feature-selection">4. Feature Selection</h2>
<p>　　在特征点跟踪之前，特征点的选择也很重要，以上计算过程中，我们期望 \(Z\) 可逆，也就是其最小特征值要足够大。如果已经提取了角点，则可进一步做选择。因此特征点选择准则为：</p>
<ol type="1">
<li>计算图像每个像素(或已提取的角点)的 \(Z\) 矩阵，及其最小的特征值 \(\lambda_m\)</li>
<li>从所有 \(\lambda_m\) 中取最大值为 \(\lambda_{max}\)</li>
<li>保留 \(\lambda_m\) 大于一定百分比(10%) \(\lambda_{max}\) 的像素(角点)</li>
<li>在这些像素(角点)中，保留局部最大值</li>
<li>视计算能力，保留其中的子集</li>
</ol>
<p>以上特征点提取的过程类似于 <a href="https://blog.csdn.net/u010103202/article/details/73331440" target="_blank" rel="noopener">Harris 角点</a>。要注意的是选择特征计算 \(Z\) 时，\(3\times3\) 窗口足够，但是跟踪时，一般大于 \(3\times3\)。</p>
<h2 id="dissimilarity">5. Dissimilarity</h2>
<p>　　相似性度量决定该特征点是否已经漂移而不能使用了，即外点检测(Outlier Detection)，所以非常重要。相比于平移模型，仿射模型对特征点的相似性度量更有效果。在长距离跟踪下，相似性度量可能解决不了是否漂移的问题，但是好的相似性度量能从一开始就剔除漂移的特征点。此外，也可用其它更高层面的外点检测技术替代。</p>
<p><a id="1" href="#1ref">[1]</a> Shi, Jianbo, and Carlo Tomasi. Good features to track. Cornell University, 1993.<br>
<a id="2" href="#2ref">[2]</a> Birchfield, Stan. "Derivation of kanade-lucas-tomasi tracking equation." unpublished notes (1997).<br>
<a id="3" href="#3ref">[3]</a> Bouguet, J.-Y.. “Pyramidal implementation of the lucas kanade feature tracker.” (2000).<br>
<a id="4" href="#4ref">[4]</a> Suhr, Jae Kyu. "Kanade-lucas-tomasi (klt) feature tracker." Computer Vision (EEE6503) (2009): 9-18.<br>
<a id="5" href="#5ref">[5]</a> Bouguet, Jean-Yves. "Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm." Intel Corporation 5.1-10 (2001): 4.</p>
]]></content>
      <categories>
        <category>Scene Flow</category>
      </categories>
      <tags>
        <tag>autonomous driving</tag>
        <tag>tracking</tag>
        <tag>ADAS</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Visual Odometry Part I&amp;II&quot;</title>
    <url>/%5Bpaper_reading%5D-Visual_Odometry_Part_I_II/</url>
    <content><![CDATA[<h2 id="overview-of-vo">1. Overview of VO</h2>
<p>　　SFM(Structure from Motion) 是解决从一堆图片中将场景以及相机姿态进行 3-D 重建的问题，最后的场景以及相机姿态可以通过离线优化方法（bundle adjustment）来 refine。VO &amp; VSLAM 都属于 SFM 的特殊情况，SfM 处理的图像时间上可以是无序的，而 VO &amp; VSLAM 则要求图像时间上有序。VO 只关心轨迹的局部一致性，而 VSLAM 关心全局轨迹和地图的一致性。VO 可以作为 VSLAM 的一个模块，用于重建相机的增量运动，Bundle Adjustment 可以用来 refine 相机的轨迹。如果用户只对相机路径感兴趣，不需要环境地图，且需要较高的实时性，那么一般 VO 就能满足需求。<br>
　　视觉里程计（VO）最早应用于 NASA 火星地面探测器，相比于车轮里程计的优势：</p>
<ul>
<li>不受车轮打滑的影响；</li>
<li>不受拐弯影响，拐弯时左右轮速度不一样；</li>
<li>更加准确，相对位置误差大概在 0.1% 到 2%，可作为车轮里程计、GPS，IMU等其它测量装置的补充；</li>
<li>在某些领域是必须的，比如无法使用车轮里程计的无人机，GPS 失效的水下环境等；</li>
</ul>
<p>　　根据视觉传感器数量，VO 可分为 Stereo VO，与 Monocular VO。当场景距离远远大于双目基线时，Stereo VO 也需要退化成 Monocular VO 来处理。</p>
<h3 id="stereo-vo-monocular-vo">1.1. Stereo VO &amp; Monocular VO</h3>
<p>　　特征点匹配可以通过特征跟踪（Feature Tracking）或特征检测（Feature Detect）再匹配两种方式实现。特征跟踪计算量小，但是容易漂移；特征检测再匹配计算量大，需要用 RANSAC 去除无匹配点，但是特征点不容易漂移。<br>
　　Motion Estimation 可通过 3D-3D，3D-2D，2D-2D 三种方式实现。Stereo 系统可以获得每个点的深度信息，所以这三种方式都可以用来做相机的运动估计。实验表明，直接在原始的 2-D 点上进行相机运动的估计，更加准确（？存疑）。<br>
　　之所以研究单目 VO，是因为当场景距离相机很远的时候（相对于双目的基线），双目就退化为单目了。单目 VO 中绝对深度（尺度）是未知的，刚开始两帧相机移动的距离通常设定为 1，之后的相对位姿都基于此。相关方法可分为：</p>
<ul>
<li>Feature-based Methods，用每一帧的特征点来估计运动。</li>
<li>Appearance-based Methods，用图像中所有的像素点或是子区域中的像素点来估计运动。</li>
<li>Hybrid Methods，结合以上两种形式。</li>
</ul>
<p>第一种方法较好，运动估计用 five-point RANSAC 来求解。</p>
<h3 id="reducing-the-drift">1.2. Reducing the Drift</h3>
<p>　　由于 VO 是一步步计算相机的运动轨迹然后作累加的，那么误差就有累积性，使得估计的运动轨迹会漂移。这可以用 Sliding Window(Windowed) Bundle Adjustment 局部优化方法来解决。也可以用 GPS 或 laser 或 IMU 融合来解决。Windowed Bundle Adjustment，是通过 m 个窗口下的信息来优化求解这 m 个相机位姿。</p>
<h3 id="vo-versus-v-slam">1.3. VO Versus V-SLAM</h3>
<p>　　V-SLAM 两大方法：</p>
<ul>
<li><strong>Filtering Methods</strong> 概率法，以一定的概率分布融合所有图像信息；</li>
<li><strong>Keyframe Methods</strong> 关键帧法，使用全局 Bundle Adjustment 优化被选择的关键帧；</li>
</ul>
<p>　　VO 只关心相机轨迹的一致性，而 SLAM 关注轨迹与地图整体的一致性。SLAM 中两大问题是，检测 loop closure 的发生以及用这个约束来更好的优化当下的地图和轨迹。而 VO 只对历史中以往 n 个轨迹中的位姿进行优化（windowed bundle adjustment），这可以认为与 SLAM 中建立局部地图与轨迹是等价的。但是这两者的 philosophy 不同：</p>
<ul>
<li>VO 只关心局部轨迹的一致性，局部地图只是用来（在 bundle ajustment）更精确的估计局部轨迹；</li>
<li>SLAM 关心整个地图的一致性，当然也包括轨迹，轨迹的精确性能使地图更加精确；</li>
</ul>
<p>　　VO 可以是 SLAM 的一个模块（相机运动轨迹的重建），SLAM 还需要一个闭环检测，以及一个全局的地图优化策略。V-SLAM 重建相机运动轨迹理论上比 VO 更精确（加入了更多的约束），但是不一定更鲁棒，因为闭环检测中的奇异值对地图的一致性有较大影响。此外 SLAM 更加复杂以及耗计算资源。VO 牺牲了全局一致性，来达到实时运行的目的，因为不需要记录所有的地图信息。</p>
<h2 id="formulation-of-the-vo-problem">2. Formulation of the VO Problem</h2>
<p>　　在时间 \(k\) 下，相机拍摄的图像集记为：\(I_{0:n}=\{I_0,...,I_k\}\)。相机在时间 \(k-1\) 与 \(k\) 的位姿转换矩阵为 \(T_{k,k-1}\in \mathbb{R}^{4\times 4}\)。VO 所要求解的问题就是 \(T=T_{1,0}T_{2,1}...T_{k,k-1}\)。由此可知 VO 是计算相邻帧的相机位姿，然后对之前 m 个位姿做一个局部优化从而估计更准确的轨迹。 <img src="/[paper_reading]-Visual_Odometry_Part_I_II/VO流程.png" width="50%" height="50%" title="图 1. VO流程图"> 　　大多数 VO 算法是基于特征点来估计运动的，特征点法的流程如图 1. 所示：</p>
<ol type="1">
<li><strong>Feature Detection(Extraction) and Matching/Feature Tracking</strong><br>
特征提取并与上一帧的特征进行匹配，或者直接用上一帧的特征在这一帧进行跟踪；</li>
<li><strong>Motion Estimation</strong><br>
在 \(k,k-1\) 帧之间求解 \(T_{k,k-1}\) 的过程，根据匹配的特征点对是 2D 还是 3D，运动估计可分为 3D-3D，3D-2D，2D-2D 三种方式实现；</li>
<li><strong>Local Optimization</strong><br>
在 \(k,k-m\) 帧用 Bundle Adjustment 迭代优化求解最优的局部轨迹；</li>
</ol>
<p>本文会重点阐述 <strong><em>Camera Model</em></strong><a href="#1" id="1ref"><sup>[1]</sup></a>，<strong><em>Feature Detection and Matching</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>，<strong><em>Motion Estimation</em></strong><a href="#1" id="1ref"><sup>[1]</sup></a>，<strong><em>Robust Estimation</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>，<strong><em>Local Optimization</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>。</p>
<h2 id="camera-modeling-and-calibration">3. Camera Modeling and Calibration</h2>
<p>　　<a href>相机模型及标定</a>，另文详述。</p>
<h2 id="feature-detection-and-matchingfeature-tracking">4. Feature Detection and Matching/Feature Tracking</h2>
<p>　　生成前后帧特征点的匹配对，有两种方法：</p>
<ul>
<li>feature tracking<br>
用局部搜索的方法，较适用于相邻两帧视角变化不大的情况，会有漂移（drift）的现象；</li>
<li>feature detection and matching<br>
独立在每个图像上进行检测，然后用某种度量准则进行匹配。在视野变化较大的情况下，只能用这种方法；</li>
</ul>
<h3 id="feature-tracking">4.1. Feature Tracking</h3>
<p>　　主要采用 KLT（详见 <a href="/KLT/" title="KLT 算法详解">KLT 算法详解</a>）方法进行特征点跟踪。</p>
<h3 id="feature-detection-and-matching">4.2. Feature Detection and Matching</h3>
<p>　　特征点包含特征检测子与特征描述子。一个好的特征点应该有如下性质：</p>
<ul>
<li>可重复性(Repeatability)，不同图像下相同特征点可再次检测出；</li>
<li>可区别性(Distinctiveness)，不同特征点表达形式不一样，可以更好匹配；</li>
<li>高效率(Efficiency)，计算高效；</li>
<li>本地性(Locality)，特征仅与一小片图像区域有关；</li>
<li>定位准确(Localization Accuracy)，不同尺度下定位都要准确；</li>
<li>鲁棒性(Robustness)，对噪声，模糊，压缩有较好的鲁棒；</li>
<li>不变性(Invariance)，对光照(photometric)，旋转，尺度，投影畸变(geometric)有不变性；</li>
</ul>
<h4 id="feature-detector">4.2.1. Feature Detector</h4>
<p>　　特征检测子（feature detector）的计算过程包含两步，首先将图像进行一个特征响应函数的变换，比如 Harris 中的 角点响应函数，SIFT 中的 DoG 变换；然后应用非极大值抑制，提取最小或最大值。<br>
　　特征检测子可分为两类：</p>
<ul>
<li>角点(corners)<br>
角点检测子被定义为至少两个边缘相交的地方；角点计算快，定位精度高，但是区分度低，大尺度下定位精度低；</li>
<li>斑点(blobs)<br>
斑点检测子被定义为一种与周围区域在亮度、颜色、纹理下不同的模式；区分度较高，但是速度较慢；</li>
</ul>
<p><img src="/[paper_reading]-Visual_Odometry_Part_I_II/detectors.png" width="80%" height="80%" title="图 2. 检测子比较"> 　　如图2. 所示，常用的角点检测子有 ORB 特征中的 FAST 关键点，Harris 角点等；常用的斑点检测子有 SIFT，SURF，CENSURE 等。</p>
<h4 id="feature-descriptor">4.2.2. Feature Descriptor</h4>
<p>　　有了特征检测子后，为了特征点匹配，还需要描述这个检测子，描述量称为特征描述子。描述子可分为以下几类：</p>
<ol type="1">
<li>Appearance，检测子周围的像素信息
<ul>
<li>SSD 匹配，sum of squared difference，计算检测子周围像素亮度与其的误差和；</li>
<li>NCC 匹配，normalized cross correlation，相比 SSD，有一定的光照不变性；</li>
<li>Census Transform，将检测子周围的 patch 像素与其进行对比，合成 0,1 向量；</li>
</ul></li>
<li>Histogram of Local Gradient Orientations
<ul>
<li>SIFT，光照，旋转，尺度，均具有不变性；不适用于角点，适用于斑点；</li>
</ul></li>
<li>Much Faster
<ul>
<li>BRIEF，二进制描述子，用于 ORB；对于旋转和尺度有较强的区分性，并且提取以及比较速度都很快；</li>
</ul></li>
</ol>
<p>　　目前常用的 ORB 特征，采用的是 Oriented FAST 角点，以及 BRIEF 描述子。</p>
<h4 id="feature-matching">4.2.3. Feature Matching</h4>
<p>　　通过比较特征点中的描述子部分，来完成特征点的匹配。如果是 appearance 描述子，那么一般通过 SSD/NNC 来计算描述子之间的相似度，其它二进制描述子，可通过欧氏距离或汉明距离来度量。<br>
　　基于相似性度量的特征匹配，最简单的就是暴力匹配，两组特征点挨个计算相似度。暴力匹配时间复杂度较高，通常我们采用<strong>快速近似最近邻算法（FLANN）</strong>，也可以加入运动估计模型（通过 IMU 等装置获得的大致运动位姿）来缩小搜索范围。特殊的如果是双目系统，因为左右目图像都是矫正过的，所以左右目的特征点匹配可通过行矩阵搜索解决。<br>
　　匹配结束后，我们还得进一步验证匹配的正确性，去除误匹配的情况。比如相互一致性验证，每个特征点只能匹配一个特征点。<br>
　　实验表明特征点的分布也很影响匹配效果，特征应尽量均匀分布，可以将图像栅格化，然后对不同的栅格用不同的特征检测阈值即可，保证栅格之间特征数量相等。</p>
<h2 id="motion-estimation">5. Motion Estimation</h2>
<h3 id="d-2d">5.1. 2D-2D</h3>
<p>　　这种情况下特征点 \(f_{k-1},f_k\) 分别是在 2D 图像 \(I_{k-1},I_k\) 坐标系上。<br>
　　<a href>对极约束推倒过程可详见这里</a>。根据对极约束，可推导出同一 3D 点投影到两个相机视角图像下后，其坐标之间的关系： <span class="math display">\[p_2^TK^{-T}t^{\wedge} RK^{-1}p_1=0\]</span> 记<strong>本质矩阵(Essential Matrix)</strong>\(E=t^{\land} R\)，记<strong>基础矩阵(Fundamental Matrix)</strong>\(F=K^ {-T}EK^ {-1}\)。基础矩阵描述的是两幅图像对应点的像素坐标的关系；本质矩阵描述的是世界中的某点分别在两个相机坐标系下坐标的相对关系。<br>
　　一般相机内参是已知的，所以我们求解本质矩阵。可采用五点法或者八点法来求解，五点法只能处理已知相机标定参数的情况，所以我们一般采用八点法来求解本质矩阵 \(E\)，大于八点即可用最小二乘求解线性方程。然后对本质矩阵进行奇异值分解，即可求出相机的位姿 \(R,t\)。<br>
　　当选取的点共面时，基础矩阵的自由度下降，即出现退化的现象，这个时候需要同时求解单应矩阵\(H\)，选择重投影误差较小的那个作为最终的运动估计矩阵。<br>
　　此外，还需计算当前运动的相对尺度，可由 3D 点的位置信息求解相对尺度。绝对尺度的求解需要三角化求解。<br>
　　总结过程如下：</p>
<ol type="1">
<li>得到新的当前帧 \(I_K\);</li>
<li>提取当前帧的特征点，并与上一帧的特征进行匹配；</li>
<li>根据匹配的特征点对，计算本质矩阵\(E\)；</li>
<li>奇异值分解本质矩阵，得到相机运动 \(R_K,t_k\)；</li>
<li>该相邻帧的相机运动信息与之前相机运动信息进行累计；</li>
<li>重复 1.；</li>
</ol>
<h3 id="d-2d-1">5.2. 3D-2D</h3>
<p>　　这种情况下，特征点 \(f_{k-1}\) 是 3D 坐标点，\(f_k\) 是其投影到 2D 图像 \(I_K\) 上的匹配点。对于单目的情况，\(f_{k-1}\) 需要从相邻的前面帧中（比如 \(I_{k-2},I_{k-1}\)）三角化出 3D 坐标，然后与当前帧进行匹配，至少需要三帧的视角。3D-2D 比 3D-3D 更加精确，因为 3D-3D 直接优化相机运动，没有优化投影的过程。<br>
　　该问题也称为 <strong>PnP(Perspective from n Points)</strong>。PnP 问题有很多种求解方法：</p>
<ul>
<li>P3P 只是用 3 个点对进行求解，容易受误匹配的影响；</li>
<li>直接线性变换 需要 6 对匹配点才能求解，如果大于 6 对，则可用 SVD 等方法求线性方程的最小二乘解；</li>
<li>EPnP</li>
<li>UPnP</li>
<li>非线性优化(Bundle Adjustment)</li>
</ul>
<p>记 \(p_{k-1}^ i\) 为 \(k-1\) 时刻下第 \(i\) 个特征点在相机坐标系下的坐标，定义重投影的误差项： <span class="math display">\[\xi=\mathop{\arg\min}\limits_{T_{k,k-1}} \sum_i \left\Vert uv^i_k-K \, T_{k,k-1} \, p_{k-1}^i \right\Vert^2\]</span></p>
<p>　　总结过程如下：</p>
<ol type="1">
<li>初始化，在 \(I_{k-2},I_{k-1}\) 两张图里提取特征并匹配，三角花得到特征点的 3D 坐标；</li>
<li>在 \(I_k\) 图像中提取特征点，并与上一帧的特征进行匹配；</li>
<li>用 PnP 求解相机运动；</li>
<li>在 \(I_{k-1},I_{k}\) 中三角化所有特征点；</li>
<li>重复 2.；</li>
</ol>
<h3 id="d-3d">5.3. 3D-3D</h3>
<p>　　这种情况下特征点都是 3D 坐标点，都需要三角花得到，可以使用一个立体视觉系统。<br>
　　已知两组匹配好的 3D 点，可以用 <strong>ICP(Iterative Closest Point)</strong> 来求解位姿。ICP 有两种求解方式：</p>
<ul>
<li>线性求解</li>
<li>非线性优化(类似 Bundle Adjustment)</li>
</ul>
<p>定义重投影的误差项： <span class="math display">\[\xi=\mathop{\arg\min}\limits_{T_{k,k-1}} \sum_i \left\Vert p_{k}^i - T_{k,k-1} \, p_{k-1}^i \right\Vert^2\]</span></p>
<p>　　ICP 问题存在唯一解或无穷多解的情况，所以非线性优化时，只要找到极小值，那一定是全局最优解，这也意味着 ICP 非线性优化时可以任意选定初始值。<br>
　　在匹配已知的情况下，ICP 问题是有解析解的。不过如果有些特征点观察不到深度，那么可以混合着使用 PnP 和 ICP 优化：对于深度已知的特征点，建模 3D-3D 误差，对于深度未知的特征点，建模 3D-2D 的重投影误差。两个误差项，用非线性优化求解。</p>
<h3 id="triangulation-and-keyframe-selection">5.4. Triangulation and Keyframe Selection</h3>
<p>　　对于 stereo camera， 3D-2D 比 3D-3D 更准确；3D-2D 比 2D-2D 计算更快，前者是 P3P 问题，后者则至少需要 5 个点。当场景中物体相比基线很大时，那么立体视觉系统就失效了，这时候用单目视觉系统比较靠谱。<br>
　　对于 monocular camera，2D-2D 比 3D-2D 看样子更好，因为避免了三角测量；然后实际中，3D-2D 用得更多，因为数据关联更快。<br>
　　当两帧之间相隔很短时间时，可以认为基线非常小，这种情况，获得的深度信息不确定性很高，所以需要选择某些 keyframes 来计算。</p>
<h2 id="robust-estimationoutlier-rejection">6. Robust Estimation/Outlier Rejection</h2>
<p>　　匹配的特征点可能因为噪音、遮挡、模糊、视角变化、光照变化等原因成为外点（outliers），这时候该匹配对对运动估计来说就是个外点，估计的时候应该想办法去除掉。<br>
　　<strong>RANSAC</strong> 目前已是在含有噪声的数据中进行模型估计的标准方法。其思想是随机选取一些数据进行建模，涵盖数据最多的模型即被选择是最终模型。对于相机运动估计来说，模型就是相机的运动 \(R,t\)，数据就是特征匹配对。RANSAC 流程为：</p>
<ol type="1">
<li>初始化，记 A 为特征点对集；</li>
<li>从 A 中随机选取一些点对 s；</li>
<li>用 s 估计运动模型；</li>
<li>计算所有的点对与这个模型的距离误差，可使用 point-to-epipolar 距离或是 directional 误差(Sampson distance)；</li>
<li>统计距离误差小于一定阈值的点对的数量，并存储标记这些内点(inliers)；</li>
<li>重复 2.，直到达到最大迭代次数；</li>
<li>选取数量最多的内点点对集，用这些点估计最终模型；</li>
</ol>
<p><img src="/[paper_reading]-Visual_Odometry_Part_I_II/ransac.png" width="60%" height="60%" tit le="图 3. RANSAC 迭代次数比较"></p>
<p>　　为保证得到正确解，迭代次数要求： <span class="math display">\[N=\frac{log(1-p)}{log(1-(1-\epsilon)^s)}\]</span> 其中，\(p\) 表示得到正确解的概率，\(\epsilon\) 表示外点的百分比，\(s\) 表示每次模型估计取出的点数。如图 3. 所示，选出的点数越少，迭代次数就可以越少。这个角度来讲，五点法比八点法有优势，但是五点法的前提是相机都是标定过的。不过不考虑速度的话，还是选择更多的点，因为可以平滑噪声。</p>
<h2 id="local-optimization">7. Local Optimization</h2>
<p>　　每次估计的相机运动都有误差，随着运动的累计，误差也会累计。这就要求做局部最优化，消除轨迹的漂移。优化方式有 Pose-Graph Optimization（需要回环检测） 以及 Windowed Bundle Adjustment 两种，这里主要介绍 BA。定义误差函数： <span class="math display">\[\xi=\mathop{\arg\min}\limits_{X^i,C_k} \sum_{i,k} \left\Vert uv_{k}^i - g(X^i,C_k) \right\Vert^2\]</span> 其中 \(X^i\) 为世界坐标系下特征点的 3D 坐标，\(C_k = T_{1,0}...T_{k,k-1}\)，\(g(X^i,C_k)\)为特征点投影到图像的映射函数。该非线性问题可用 Newton-Gauss 或 LM 法解决。为了加速运算，如果 3D 特征点是准确的(如立体视觉获得的)，那么可以固定特征点的 3D 量，只优化相机的轨迹。</p>
<p><a id="1" href="#1ref">[1]</a> Scaramuzza, Davide, and Friedrich Fraundorfer. "Visual odometry [tutorial]." IEEE robotics &amp; automation magazine 18.4 (2011): 80-92.<br>
<a id="2" href="#2ref">[2]</a> Fraundorfer, Friedrich, and Davide Scaramuzza. "Visual odometry: Part ii: Matching, robustness, optimization, and applications." IEEE Robotics &amp; Automation Magazine 19.2 (2012): 78-90.</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>SLAM</tag>
        <tag>VO</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Stereo R-CNN based 3D Object Detection for Autonomous Driving&quot;</title>
    <url>/%5Bpaper_reading%5D-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/</url>
    <content><![CDATA[<p>　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（如 geometry 约束或专家系统）的验证，能有效抑制这个问题。本文就是一个比较好的 learning+geometry 想结合的方法。<br>
　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于图像语义及几何信息，通过 3D 目标的稀疏与密集约束，提出了一种准确的 3D 目标检测方法。根据输入数据的类型，作者将 3D 检测分为三大类：</p>
<ul>
<li>LiDAR-based，近期被研究的较多，基本是自动驾驶所必须的；</li>
<li>Monocular-based，低成本方案；</li>
<li>Stereo-based，相比 Monocular-based，有优势，但是研究较少；</li>
</ul>
<p>本文就是 Stereo-based 3D 检测方案。不同于一般的 rgb+depth 作为输入的方案，本文直接将左右目 rgb 作为输入，没有显示地 depth 生成过程。工程上来说，这也极大地缩短了 3D Detection 的时延(latency)。<br>
　　本文方法如图 1 所示，主要有三部分组成：</p>
<ol type="1">
<li> Network，又有三部分构成：
<ul>
<li>Stereo RPN Module，输出左右图的 RoI；</li>
<li>Classification and Regression branches，输出目标类别，朝向，尺寸；</li>
<li>Keypoint branch，输出左目目标的关键点；</li>
</ul></li>
<li> Sparse constraints，3D 框-2D 框的稀疏约束；</li>
<li> Dense constraints，准确定位的关键模块；</li>
</ol>
<p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/net_arch.png" width="100%" height="100%" title="图 1. 网络结构"></p>
<h2 id="stereo-r-cnn-network">1. Stereo R-CNN Network</h2>
<p>　　Stereo R-CNN 是在 Faster R-CNN 基础上，同时检测与关联左右目图像 2D 框的微小差异。</p>
<h3 id="stereo-rpn">1.1. Stereo RPN</h3>
<p>　　在传统 RPN 网络的基础上，本文先对左右图做 paramid features 提取，然后将不同尺度的特征 concatenate 一起，进入 RPN 网络。 <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/target.png" width="60%" height="60%" title="图 2. 真值框定义方式"> 　　关键的一点是 objectness classification与 stereo box regression 的真值框定义不一样。如图 2 所示，</p>
<ul>
<li>对于 objectness classification，真值框定义为左右目真值框的外接合并（union GT box），一个 anchor 在与真值框的交并比（Intersection-over-Union）大于 0.7 时标记为正样本，小于 0.3 时标记为负样本。分类任务的候选框包含了左右目真值框区域的信息。</li>
<li>对于 stereo box regression，真值框定义为左右目分别的真值框。待回归的参数定义为 \([u, w, u', w', v, h]\)，分别为左目的水平位置及宽，右目的水平位置及宽，垂直位置及高。因为输入为矫正过的左右目图像，所以可认为左右目的垂直方向上已经对齐。</li>
</ul>
<p>每个左右目的 proposal 都是通过同一个 anchor 产生的，自然左右目的 proposal 是关联的。通过 NMS 后，保留左右目都还存在的 proposal 关联对，取前 2000 个用于训练，测试时取前 300 个。</p>
<h3 id="stereo-r-cnn">1.2. Stereo R-CNN</h3>
<p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/viewpoint.png" width="50%" height="50%" title="图 3. 各角度关系"> 　　网络头包含两大部分：</p>
<ol type="1">
<li><p> <strong>Stereo Regression</strong><br>
左右目的 proposal 关联对，分别在左右目的 feature 上进行 RoI Align 的操作，然后 concatenate 输入到全链接层。左右目的 RoI 对与真值框的 IoU 均大于 0.5 时定位正样本，左右目的 RoI 对与真值框的 IoU 有一个小于 0.5 且大于 0.1，则定位负样本。用四个分支分别预测：</p>
<ul>
<li>object class；</li>
<li>stereo bounding boxes，与 stereo rpn 中一致，左右目的高度已对齐；</li>
<li>dimension，先统计平均的尺寸，然后预测相对量；</li>
<li>viewpoint angle，如图 3 所示，\(\theta\) 为相机坐标系下的朝向角，\(\beta\) 为相机中心点下的方位角(azimuth)，这三个目标在相机视野下是一样的，所以我们回归的量是视野角(viewpoint angle) \(\alpha=\theta+\beta\)，其中 \(\beta=arctan\left(-\frac{x}{z} \right) \)。并且为了连续性，回归量为 \([sin\,\alpha,cos\,\alpha]\)。</li>
</ul></li>
</ol>
<p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/keypoints.png" width="70%" height="70%" title="图 4. 语义关键点"></p>
<ol start="2" type="1">
<li> <strong>Keypoint Prediction</strong><br>
如图 4 所示，考虑 3D 框底部矩形的四个关键点，投影到图像平面后，最多只有一个关键点会在图像 2D 矩形框内。对左目图像进行关键点预测，类似 Mask R-CNN，在 6×28×28 的基础上，因为关键点只有图像坐标 u 方向才提供了额外的信息，所以对每列进行累加，最终输出 6×28 的向量。前 4 个通道代表每个关键点作为 perspective keypoint 投影到该 u 坐标下的概率；后 2 个通道代表该 u 坐标是左右边缘关键点(boundary keypoints)的概率。为了找出 perspective keypoint，softmax 应用于 4×28 的输出上；为了找出左右边缘关键点，softmax 分别应用于后两个 1×28 的输出上。训练的时候，4×28 中只有一个被赋予 perspective keypoint，忽略没有 perspective keypoint 的情况（遮挡等），然后最小化 cross-entropy loss；对于边缘关键点，则分别最小化 1×28 维度上的 cross-entropy loss，前景中也会被赋予边缘关键点。</li>
</ol>
<h2 id="d-box-estimation">2. 3D Box Estimation</h2>
<p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/projection.png" width="70%" height="70%" title="图 5. 关键点投影关系"> 　　已知关键点，2D 框，尺寸，朝向角，我们可以求解出 3D 框 \(\{x,y,z,\theta\}\)。求解目标是最小化 3D 框投影到 2D 框以及关键点的误差。如图 5 所示，已知 7 个观测量 \(z = \{u_l,v_t,u_r,v_b,u_l',u_r',u_p\}\)，分别代表左目 2D 框的左上坐标，右下坐标，右目 2D 框的左右 u 方向坐标，以及 perspective keypoint 的 u 方向坐标。在图 5 的情况下（其它视角下，注意符号变化），左上点投影关系如下： <span class="math display">\[\require{cancel}
\begin{bmatrix}
u_l\\
v_t\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{cam}^{tl}\\
y_{cam}^{tl}\\
z_{cam}^{tl}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{obj}^{tl}\\
y_{obj}^{tl}\\
z_{obj}^{tl}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
-\frac{w}{2}\\
-\frac{h}{2}\\
-\frac{l}{2}\\
\end{bmatrix}\]</span> 其中 \(K\) 为相机内参，\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)_{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，右下点为： <span class="math display">\[\require{cancel}
\begin{bmatrix}
u_r\\
v_b\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{cam}^{tl}\\
y_{cam}^{tl}\\
z_{cam}^{tl}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{obj}^{tl}\\
y_{obj}^{tl}\\
z_{obj}^{tl}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
\frac{w}{2}\\
\frac{h}{2}\\
-\frac{l}{2}\\
\end{bmatrix}\]</span> 右目两个边缘点以及 perspective keypoint 点也可同样得到，由此可整理出 7 个方程组（论文中第一个公式符号有错）： <span class="math display">\[\left\{\begin{array}{l}
u_l=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\
v_t=(y- \frac{h}{2}) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\
u_r=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\
v_b=(y+ \frac{h}{2}) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\
u&#39;_l=(x-b- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\
u&#39;_r=(x-b+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\
u_p=(x+ \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\
\end{array}\right.\]</span> 其中 \(b\) 为双目的基线长(baseline)。以上方程组可用 Gauss-Newton 法求解。</p>
<h2 id="dense-3d-box-alignment">3. Dense 3D Box Alignment</h2>
<p>　　以上得到的目标 3D 位置是 object-level 求解得到的，利用像素信息，还可以进行优化精确求解。首先在图像 2D 目标框内扣取一块 RoI，要使 RoI 能较为确定的在目标上，扣取方式定义为：</p>
<ul>
<li>目标一半以下区域；</li>
<li>perspective keypoint 与边缘关键点包围区域；</li>
</ul>
<p>关键点预测的时候只预测了 u 方向的坐标，边缘关键点无 v 方向的信息，看起来会使某些背景像素被划入为目标像素，更好的方法是加入 instance segmentation 信息。定义误差函数为： <span class="math display">\[E=\sum_{i=0}^N e_i=\sum_{i=0}^N \left\| I_l(u_i,v_i)-I_r(u_i-\frac{b}{z+\Delta z_i},v_i)\right\|\]</span> 可由三角测量关系 \(z=\) 推出。上式中，\(z_i=z_i-z\) 表示某个像素点 \(i\) 所对应的 3D 点与目标中心点之间的距离。最小化总误差即可求得最优的中心点距离 \(z\)。优化过程可以用 coarse-to-fine 的策略，先以 0.5m 的精度找 50 步，再以 0.05m 的精度找 20 次。<br>
　　这个 dense alignment 模块是独立的，可以应用到任意的左右目 3D 检测的后处理中。因为目标 RoI 是物理约束，所以这个方法避免了深度估计中不连续、病态的问题，且对光照是鲁棒的，因为每个像素都会对估计起作用。这里，本文只做了中心点的 align，尺寸，甚至朝向角是否能加入优化?</p>
<h2 id="other-details">4. Other Details</h2>
<p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r1.png" width="110%" height="110%"> <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r2.png" width="70%" height="70%"> <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r3.png" width="100%" height="100%"> <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r4.png" width="90%" height="90%"></p>
<p><a id="1" href="#1ref">[1]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. "Stereo R-CNN based 3D Object Detection for Autonomous Driving." arXiv preprint arXiv:1902.09738 (2019).</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>[paper_reading]-&quot;Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving&quot;</title>
    <url>/%5Bpaper_reading%5D-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/</url>
    <content><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，对工程应用也有较强的指导意义。 <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/arch.png" width="100%" height="100%" title="图 1. 语义跟踪系统框架"> 　　如图 1. 所示，整个系统框架由三部分组成：</p>
<ul>
<li>2D object detection and viewpoint classification，目标位姿通过 2D-3D 约束求解出来；</li>
<li>feature extraction and matching，双目及前后帧的特征提取与匹配；</li>
<li>ego-motion and object tracking，将语义信息及特征量加入到优化中，并且加入车辆动力学约束以获得平滑的运动估计。</li>
</ul>
<h2 id="viewpoint-classification-and-3d-box-inference">1. Viewpoint Classification and 3D Box Inference</h2>
<h3 id="viewpoint-classification">1.1. Viewpoint Classification</h3>
<p>　　选用 Faster R-CNN 作为 2D 检测框架，在此基础上，加入车辆视野（viewpoint）分类分支。由图 2. 所示，水平视野分为八类，垂直视野分为两类，总共 16 类。 <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/view.png" width="90%" height="90%" title="图 2. 车辆视野分类"></p>
<h3 id="d-box-inference-based-on-viewpoint">1.2. 3D Box Inference Based on Viewpoint</h3>
<p>　　网络输出图像 2D 框以及目标车辆的视野类别（viewpoint），此时我们假设：</p>
<ul>
<li>2D 框准确；</li>
<li>每种车辆的尺寸相同；</li>
<li>2D 框能紧密包围 3D 框；</li>
</ul>
<p>在以上假设条件下，我们可以求得 3D 框，该 3D 框作为后续优化的初始值。约束方程的表示在论文中比较晦涩，在这里我做细致的推倒。 3D 框可表示为 \(\{x,y,z,\theta,w,h,l\}\)，其中 \(\{w,h,l\}\) 分别对应 \({x,y,z}\) 维度。如图 2.(b) 所示，这个视角下，四个 3D 框的顶点，可得四个约束方程。推倒过程为： <span class="math display">\[\require{cancel}
\begin{bmatrix}
u_{min}\\
v_1\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{1}^{cam}\\
y_{1}^{cam}\\
z_{1}^{cam}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{1}^{obj}\\
y_{1}^{obj}\\
z_{1}^{obj}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
\frac{w}{2}\\
\frac{h}{2}\\
\frac{l}{2}\\
\end{bmatrix}\]</span> 其中 \(K\) 为相机内参，做归一化处理消去；\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)^{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，②，③，④ 点都可以由此获得： <span class="math display">\[\left\{\begin{array}{l}
\require{cancel}
\begin{bmatrix}
u_{min}\\
v_1\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{1}^{cam}\\
y_{1}^{cam}\\
z_{1}^{cam}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{1}^{obj}\\
y_{1}^{obj}\\
z_{1}^{obj}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
\frac{w}{2}\\
\frac{h}{2}\\
\frac{l}{2}\\
\end{bmatrix}\\
\begin{bmatrix}
u_{max}\\
v_2\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{2}^{cam}\\
y_{2}^{cam}\\
z_{2}^{cam}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{2}^{obj}\\
y_{2}^{obj}\\
z_{2}^{obj}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
-\frac{w}{2}\\
\frac{h}{2}\\
-\frac{l}{2}\\
\end{bmatrix}\\
\begin{bmatrix}
u_3\\
v_{min}\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{3}^{cam}\\
y_{3}^{cam}\\
z_{3}^{cam}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{3}^{obj}\\
y_{3}^{obj}\\
z_{3}^{obj}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
\frac{w}{2}\\
-\frac{h}{2}\\
-\frac{l}{2}\\
\end{bmatrix}\\
\begin{bmatrix}
u_4\\
v_{max}\\
1\\
\end{bmatrix}=K\cdot
\begin{bmatrix}
x_{4}^{cam}\\
y_{4}^{cam}\\
z_{4}^{cam}\\
\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot 
\begin{bmatrix}
x_{4}^{obj}\\
y_{4}^{obj}\\
z_{4}^{obj}\\
\end{bmatrix}=\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}+
\begin{bmatrix}
cos\theta &amp; 0 &amp;sin\theta\\
0 &amp; 1 &amp; 0\\
-sin\theta &amp; 0 &amp; cos\theta\\
\end{bmatrix} \cdot
\begin{bmatrix}
-\frac{w}{2}\\
\frac{h}{2}\\
\frac{l}{2}\\
\end{bmatrix}
\end{array}\right.\]</span></p>
<p>将 \(z\) 方向归一化后，进一步得到最终的四个约束式子： <span class="math display">\[\left\{\begin{array}{l}
u_{min}=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\
u_{max}=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\
v_{min}=(y- \frac{h}{2}) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\
v_{max}=(y+ \frac{h}{2}) / (z+ \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)
\end{array}\right.\]</span> 以上四个方程可以闭式求解 3D 框 \({x,y,z,}\)。该方法将 3D 框的回归求解分解成了 2D 框回归，视野角分类以及解方程组的过程，强依赖于前面的三点假设，实际情况 3D 框与 2D 框不会贴的很紧。这个 3D 框结果只用来作后续的特征提取区域及最大后验概率估计的初始化。</p>
<h2 id="feature-extraction-and-matching">2. Feature Extraction and Matching</h2>
<p>　　这一部分做的是左右目及前后帧特征提取及匹配。选用 ORB 特征，目标区域由投影到图像的 3D 框确定。</p>
<ul>
<li><strong>目标区域内左右目的立体匹配</strong> 由于已知目标的距离及尺寸，所以只需要在一定小范围内进行特征点的行搜索匹配。</li>
<li><strong>目标及背景区域下前后帧的时序匹配</strong> 首先进行 2D 框的关联，2D 框经过相机旋转补偿后，最小化关联框的中心点距离及框形状相似度值。然后在关联上的目标框区域以及背景区域里，分别作 ORB 特征的匹配，异常值在 RANSAC 下通过基础矩阵测试去除。</li>
</ul>
<h2 id="ego-motion-and-object-tracking">3. Ego-motion and Object Tracking</h2>
<p>　　首先进行本车运动状态估计，可在传统 SLAM 框架下做，不同的是将动态障碍物中的特征点去除。有了本车的位姿后，再估计动态障碍物的运动状态。文中符号定义较为复杂，这里不做赘述。</p>
<h3 id="ego-motion-tracking">3.1. Ego-motion Tracking</h3>
<p>　　给定左目前后帧背景区域特征点的观测，本车状态估计可以通过极大似然估计（Maximum Likelihood Estimation）得到。MLE 可以转化为非线性最小二乘问题，也就是 Bundle Adjustment 过程，这是典型的 SLAM 问题。文中给出的误差方程： <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/3.png" width="50%" height="50%"> 需要求解的是本车位姿以及背景特征点坐标，这是后验概率，可转为似然函数求解，然后转化为非线性优化问题。可参考《视觉 SLAM 十四讲》(107-108)来理解。</p>
<h3 id="semantic-object-tracking">3.2. Semantic Object Tracking</h3>
<p>　　得到本车相机的位姿后，运动目标的状态估计可以通过最大后验概率估计（Maximum-a-posterior, MAP）得到。类似的，可转为非线性优化问题进行求解，联合优化每个车辆的<strong>位姿</strong>，<strong>尺寸</strong>，<strong>速度</strong>，<strong>方向盘转角</strong>，<strong>所有特征点 3D 位置</strong>。有四个 loss 项： <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/6.png" width="80%" height="80%"> <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/10.png" width="80%" height="80%"> \(r_Z,r_P,r_M,r_S\) 分别代表：</p>
<ul>
<li><strong>Sparse Feature Observation</strong><br>
目标上的特征点重投影到左右目图像的误差，注意有左右目两个误差项；</li>
<li><strong>Semantic 3D Object Measurement</strong><br>
3D 框投影到图像上与 2D 框的尺寸约束投影误差，即 1.2 节中的形式，区别在车辆尺寸与位姿作为了优化项；</li>
<li><strong>Vehicle Motion Model</strong><br>
对于车辆，前后时刻的状态要有连续性，即误差最小；</li>
<li><strong>Point Cloud Alignment</strong><br>
为了减少 3D 框的整体偏移，引入特征点到 3D 观察面的最小距离误差；</li>
</ul>
<p>这里只对车辆运动模型进行分析，其它几项基本在前文已经有描述或者比较常识化，就不展开，具体公式可参见论文。<br>
　　由实验可知 Sparse Feature Observation 与 Point Cloud Alignment 对性能提升较明显，Motion Model 对困难情景性能才有提升。</p>
<h4 id="vehicle-motion-model">3.2.1. Vehicle Motion Model</h4>
<p>　　<a href="#2" id="2ref">[2]</a> 中介绍了前转向车的两种模型：运动学模型(Kinematic Bicycle Model)，以及更复杂的动力学模型(Dynamic Bicycle Model)。运动学模型假设车辆不存在滑动，这在大多数情况下都是满足的，所以我们只介绍运动学模型。 <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/kinematic.png" width="30%" height="30%" title="图 3. 车辆运动学模型"> 　　如图 3. 所示，前后轮无滑动的约束下，可得方程组： <span class="math display">\[\left\{\begin{array}{rl}
\dot{x}_fsin(\theta+\delta)-\dot{y}_fcos(\theta+\delta)=&amp;0\\
\dot{x}sin(\theta)-\dot{y}cos(\theta)=&amp;0\\
x+Lcos(\theta)=&amp;x_f  \quad\Rightarrow \quad \dot{x}-\dot{\theta}Lsin(\theta)=\dot{x}_f\\
y+Lsin(\theta)=&amp;y_f \quad\Rightarrow \quad \dot{y}+\dot{\theta}Lcos(\theta)=\dot{y}_f
\end{array}\right.\]</span> 由此可得到: <span class="math display">\[\dot{x}sin(\theta+\delta)-\dot{y}cos(\theta+\delta)-\dot{\theta}Lcos(\delta)=0\]</span> 用 \(\left(v \cdot cos(\theta),v\cdot sin(\theta)\right)\) 代替 \((\dot{x},\dot{y})\) 可得： <span class="math display">\[\dot{\theta}=\frac{tan(\delta)}{L}\cdot v\]</span> 最终可整理成矩阵形式： <span class="math display">\[
\begin{bmatrix}
\dot{x}\\
\dot{y}\\
\dot{\theta}\\
\dot{\delta}\\
\dot{v}\\
\end{bmatrix}=
\begin{bmatrix}
0 &amp;0 &amp;0 &amp;0 &amp;cos(\theta)\\
0 &amp;0 &amp;0 &amp;0 &amp;sin(\theta)\\
0 &amp;0 &amp;0 &amp;0 &amp;\frac{tan(\delta)}{L}\\
0 &amp;0 &amp;0 &amp;0 &amp;0\\
0 &amp;0 &amp;0 &amp;0 &amp;0\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
\theta\\
\delta\\
v\\
\end{bmatrix}+
\begin{bmatrix}
0 &amp;0\\
0 &amp;0\\
0 &amp;0\\
1 &amp;0\\
0 &amp;1\\
\end{bmatrix}
\begin{bmatrix}
\gamma\\
\alpha\\
\end{bmatrix}
\]</span> 其中 \(L\) 为车辆参数。观测量有：</p>
<ul>
<li>\((x,y,\theta)\) 为车辆的位置及朝向角；</li>
<li>\(\delta\) 为方向盘/车轮转角；</li>
<li>\(v\) 为车辆速度；</li>
</ul>
<p>控制量有：</p>
<ul>
<li>\(\gamma\) 为方向盘角度比率；</li>
<li>\(\alpha\) 为加速度；</li>
</ul>
<p>本文的目的是要约束车辆时序上运动(速度及朝向)的平滑一致性，令控制量 \(\gamma,\alpha\) 为 0，然后可得状态量在相邻时刻的关系应满足： <span class="math display">\[\left\{\begin{array}{l}
\hat{x}^t=x^{t-1}+cos(\theta^{t-1})v^{t-1}\Delta t\\
\hat{y}^t=y^{t-1}+sin(\theta^{t-1})v^{t-1}\Delta t\\
\hat{\theta}^t=\theta^{t-1}+\frac{tan(\delta^{t-1})}{L}v^{t-1}\Delta t\\
\hat{\delta}^t=\delta^{t-1}\\
\hat{v}^t=v^{t-1}
\end{array}\right.\]</span> 由此可整理成论文中矩阵的形式及误差项： <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/15.png" width="80%" height="80%"></p>
<p><a id="1" href="#1ref">[1]</a> Li, Peiliang, and Tong Qin. "Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving." Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br>
<a id="2" href="#2ref">[2]</a> Gu, Tianyu. Improved trajectory planning for on-road self-driving vehicles via combined graph search, optimization &amp; topology analysis. Diss. Carnegie Mellon University, 2017.</p>
]]></content>
      <categories>
        <category>3D Detection</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>MOT Metrics in Academia and Industry</title>
    <url>/MOT-Metrics-in-Academia-and-Industry/</url>
    <content><![CDATA[<p>　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介绍，<a href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/#more">the CLEAR MOT Metrics</a>）。而工业界则尚无统一的标准，实际的指标需求情况也比学术界复杂。<br>
　　指标的计算过程可由三部分组成，真值过滤(Filter)，匹配构建(Establishing Correspondences)与指标计算(Calculating Metrics)。其中真值过滤，更多的是工程细节，学术界没有文章对这一部分进行讨论研究。本文首先介绍学术界各评价指标详情，然后讨论工业界需要的评价指标又是怎样的。</p>
<h2 id="metrics-in-academia">1. Metrics in Academia</h2>
<p>　　在学术界，因为数据集质量较高，噪声相对较小，匹配构建中距离的度量偏向于严格且简单的方式。对于区域(框)跟踪器，采用重叠区域来度量；对于点跟踪器，采用中心点的欧式距离来度量。指标汇总如下：</p>
<p>A. <strong>检测指标</strong><br>
　\(\lozenge\)　准确性(Accuracy)</p>
<ul>
<li><strong>Recall</strong> = \(\frac{TP}{GT}\)；</li>
<li><strong>Precision</strong> = \(\frac{TP}{TP+FP}\)；</li>
<li><strong>FAF/FPPI</strong><a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> ，Average False Alarms per Frame；False Positive Per Image;</li>
<li><strong>MODA</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multipe Object Detection Precision，整合了 FN 与 FP，设 \(c_m, c_f\) 分别为 FN，FP 的权重： <span class="math display">\[MODA=1-\frac{\sum_{t=1}^{N_frames}(c_m(fn_t)+c_f(fp_t))}{\sum_{t=1}^{N_frames}gt_t}\]</span></li>
</ul>
<p>　\(\lozenge\)　精确性(Precision)</p>
<ul>
<li><strong>MODP</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multiple Object Detection Accuracy， <span class="math display">\[MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; dist}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}\]</span> 其中 \(N_{mapped}^{(t)}\) 为第 \(t\) 帧匹配的目标数；\(dist\) 为距离度量方法，如框的交并比度量法： <span class="math display">\[Mapped Overlap Ratio = \frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|}\]</span></li>
</ul>
<p>B. <strong>跟踪指标</strong><br>
　\(\lozenge\)　准确性(Accuracy)</p>
<ul>
<li><strong>IDS</strong><a href="#4" id="4ref"><sup>[4]</sup></a>，ID switch，a tracked target changes its ID with another target(预测关联真值)；</li>
<li><strong>MOTA</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Accuracy，整合了 FN，FP，ID-Switch： <span class="math display">\[MOTA=1-\frac{\sum_{t=1}^{N_{frames}} \;\; (c_m(fn_t)+c_f(fp_t)+c_s(ID-SWITCHES_t))}{\sum_{t=1}^{N_{frames}} \;\; gt_t}\]</span> 其中权重方程一般可设为：\(c_m=c_f=1, \quad c_s=log_{10}\)；</li>
</ul>
<p>　\(\lozenge\)　精确性(Precision)</p>
<ul>
<li><strong>MOTP</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Precision， <span class="math display">\[MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; \left(\frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|} \right)}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}\]</span></li>
<li><strong>TDE</strong><a href="#6" id="6ref"><sup>[6]</sup></a>，Distance between the ground-truth annotation and the tracking result；像素级别的误差计算，适用于人群跟踪；</li>
<li><strong>OSPA</strong><a href="#7" id="7ref"><sup>[7]</sup></a><a href="#8" id="8ref"><sup>[8]</sup></a>，Optimal Subpattern assignment，由定位 (localization) 误差及基数 (cardinality) 误差构成，对于第 \(t\) 帧： <span class="math display">\[e^t=\left[\frac{1}{n^t}\left( \mathop{\min}_{\pi\in\Pi_n} \sum_{i=1}^{m^t} d^{(c)}(x_i^t,y_{\pi(i)}^t)^p + (n^t-m^t)\cdot c^p \right) \right]^{1/p}\]</span> 其中，\(n^t\) 为目标真值与算法输出中数量较大者。\(\Pi_n\) 为从 \(n^t\) 中取出的 \(m\) 个目标。\(p\) 为距离指数范数。其中定位截断误差为： <span class="math display">\[d^{(c)}(x_i^t,y_{\pi(i)}^t) = \mathop{\min}\left(c,d(x_i^t,y_{\pi(i)}^t)\right)\]</span> \(c\) 为截断参数。定位误差又由距离误差和标签误差组成： <span class="math display">\[d(x_i^t,y_{\pi(i)}^t=\parallel x_i^t-y_{\pi(i)}^t\parallel + \alpha \; \bar{\delta}(l_x, l_y)\]</span> 其中 \(\alpha\in[0,c]\)，为标签误差的权重系数。如果 \(l_x=l_y\)，\(\bar{\delta}(l_x, l_y)=0\)，否则 \(\bar{\delta}(l_x, l_y)=1\).</li>
</ul>
<p>　\(\lozenge\)　完整性(Completeness)</p>
<ul>
<li><strong>MT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Tracked，真值轨迹长度被跟踪大于80%的比例；</li>
<li><strong>ML</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Lost，真值轨迹长度被跟踪小于20%的比例；</li>
<li><strong>PT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Partially Tracked，\(1-MT-ML\);</li>
<li><strong>FM</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Fragments，ID of a target changed along a GT trajectory, or no ID(真值关联预测)；</li>
</ul>
<p>　\(\lozenge\)　鲁棒性(Robustne)</p>
<ul>
<li><strong>RS</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from short term occlusion;</li>
<li><strong>RL</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from long term occlusion;</li>
</ul>
<h2 id="metrics-in-industry">2. Metrics in Industry</h2>
<p>　　工业界的数据噪声较大，传感器配置也比较多样，不同的产品（传感器+算法），对 MOT 性能维度要求也不一样。更重要的是，评价指标应该从功能层面进行定义，在模块层面 (MOT) 进行调整及细化。可以说，工业界是以学术界为基础来设计 MOT 指标的，不同的产品没有统一的标准，但有比较通用的设计准则。<br>
　　这里以自动驾驶/辅助驾驶中动态目标状态估计模块为例，模块详细分析<a href>日后再写</a>。该模块的基本输入为：</p>
<ul>
<li><strong>传感器数据</strong>，可以是图像，激光等；</li>
<li><strong><em>自定位系统</em></strong>，可以是基于视觉的 VO，基于视觉-IMU 的 VINS等；</li>
</ul>
<p>其中自定位系统能使目标状态估计在世界坐标系（惯性系）下优化，否则只能在本体（ego）非惯性系下优化，会减少一些约束量。该功能的基本输出为：</p>
<ul>
<li><strong>位置</strong>，本体坐标系下目标的三维位置，\(x,y,z\)；</li>
<li><strong>尺寸</strong>，目标的物理尺寸大小，包括立方体的长宽高；或者图像坐标系下的像素大小；或者图像/点云下目标的 mask，即分割后的目标；</li>
<li><strong><em>朝向</em></strong>，一般只考虑目标的航向角；</li>
<li><strong>速度</strong>，本体坐标系或世界坐标系下的三维速度，一般只考虑航向平面的速度；</li>
</ul>
<p>其中朝向是非必须项，有了朝向后，能更有效地进行状态优化。该模块的子模块有（注意，MOT 只包含前三者）：</p>
<ul>
<li><strong>检测(Detection)</strong>，进行多目标检测；</li>
<li><strong>跟踪(Tracking)</strong>，根据上一帧结果，进行多目标跟踪；</li>
<li><strong>数据关联(Association)</strong>，检测结果与跟踪结果的融合，出目标的 tracklets，生成 ID；</li>
<li><strong>状态估计(State Estimation)</strong>，不同的方法包括不同的部分；</li>
</ul>
<p>　　工业界设计产品时，基本遵循自顶向下的策略：产品需求-功能需求-模块需求，层层推倒。所以我们设计评价准则时，一般会问几个问题：</p>
<ul>
<li>该模块服务的产品功能，其需求及对应的指标是什么？</li>
<li>要达到功能指标，本模块的输出需要哪些指标来评测？</li>
<li>各个子模块对模块的影响是怎样的，对应需要增加哪些指标？</li>
</ul>
<p>这里提到了功能指标，模块指标，子模块指标三层概念。功能指标及部分模块指标是可以写入产品手册的，所以需要突出重点，易于理解；部分模块及子模块指标则主要是为了产品上工程优化迭代，这就要求这部分指标要相当细致，将模块的不足尽可能解耦，且完全暴露出来。以下通过两个例子来分析设计过程。</p>
<h3 id="adas-中的-fcw-功能">2.1. ADAS 中的 FCW 功能</h3>
<p>　　FCW 基本功能要求为：</p>
<ul>
<li>不允许误报，尽可能不漏报；</li>
<li>在 V km/h 下，以一定的刹车加速度 a，能避免与静止的前车相碰撞；</li>
</ul>
<p>　　由以上两个功能需求，可确定必须的功能指标：</p>
<ul>
<li>（百公里）误报率；</li>
<li>（百公里）漏报率；</li>
<li>观测距离，可由第二项功能要求推到出（人反应时间已知）；</li>
</ul>
<p>　　相应的 MOT +状态估计模块输出的指标为<strong>各距离维度各类别维度</strong>下的：</p>
<ul>
<li>误检率；</li>
<li>漏检率；</li>
<li>ID Switch；</li>
<li>定位精度；</li>
<li>速度估计精度；</li>
</ul>
<p>　　其中 MOT 主要涉及误检率，漏检率，ID Switch（直接影响状态估计模块）。这些指标的计算方式可以在学术界定义的基础上做进一步改进，比如漏检率，就需要体现出百公里漏报率的性能，所以可以考虑将连续 N 帧漏检的目标才归为漏检，分母可以定义为每多少帧。此外，要在各距离维度各类别维度下进行计算，这就涉及到过滤（filter）策略。对于 FCW 而言，首要关注的是本车前方近距离位置，距离维度上的功能重要程度要突显出来，类别维度也要区别对待，以便算法模块可以重点优化。</p>
<h3 id="自动驾驶中的动态障碍物检测功能">2.2. 自动驾驶中的动态障碍物检测功能</h3>
<p>　　自动驾驶中动态障碍物检测的要求就高了，子模块也较为复杂，指标除了评估功能模块的性能，还需要指导迭代各子模块算法，包括本子模块的迭代比较，以及上下游模块相关指标的对比。<br>
　　功能需求，我们简单列举几项：</p>
<ul>
<li>不允许漏检，尽可能不误检；</li>
<li>前向，后向，侧向观测距离分别要达到 x, y, z；</li>
</ul>
<p>　　相应的功能指标为：</p>
<ul>
<li>漏检率；</li>
<li>误检率；</li>
<li>观测距离；</li>
<li>观测精度；</li>
<li>观测时延(delay)；</li>
</ul>
<p>　　MOT +状态估计模块输出的指标依然在<strong>各距离维度各类别维度</strong>下：</p>
<ul>
<li>误检率；</li>
<li>漏检率；</li>
<li>ID Switch；</li>
<li>定位精度；</li>
<li>尺寸，朝向，速度估计精度；</li>
<li>状态估计收敛时间；</li>
<li>一系列描述时序稳定性的指标；</li>
</ul>
<p>　　与前述 FCW 功能类似，只是多了较多的指标。过滤操作也做的更加细致，我们还可以将目标做重要性等级划分，比如本车道前车多少米内，那指标基本都要达到 99%+；还可以将地面区域做重要性划分（比距离维度更加细致，可以认为是三维层面），周围几米内，那误检率肯定要非常低。除了过滤策略需要仔细设计外，匹配策略也需要进一步思考。如果传感器本身精度就有限，那么匹配策略就要相应放宽。还需注意的是引入过滤策略后，FP与FN计算的细微差别，比如有个过滤条件为去除目标像素面积小于一定阈值的目标集 A，观测值与真值匹配时，如果与 A 中的目标匹配上，那么不应该记为 FP，如果没匹配上 A 中的目标，那么 A 中地目标也不应该被记为 FN。这种类似的情况逻辑要思考清楚。</p>
<h2 id="summary">3. Summary</h2>
<p>　　以上设计的出发点是，我们要承认<strong>算法的不完美性</strong>以及<strong>传感器的局限性</strong>，在工程领域，一定要首先解决主要矛盾，再打磨细节。本文还对以下内容未作进一步分析（以后有机会再写文细究）：</p>
<ul>
<li>状态估计时序相关指标，描述估计的时序稳定性，也可以用于 MOT 的评估；</li>
<li>标注与过滤策略的关系，过滤策略往往依赖于标注策略；</li>
<li>各个指标的阈值确定，确定阈值也是产品中一件重要而又系统的事，有时候比指标设计更复杂； 　　</li>
</ul>
<p><a id="1" href="#1ref">[1]</a> Yang B, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;hl=zh-CN&amp;as_sdt=0,5&amp;as_vis=1" target="_blank" rel="noopener">Learning affinities and dependencies for multi-target tracking using a CRF model</a>[C]//CVPR 2011. IEEE, 2011: 1233-1240.<br>
<a id="2" href="#2ref">[2]</a> Choi W, Savarese S. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;btnG=" target="_blank" rel="noopener">Multiple target tracking in world coordinate with single, minimally calibrated camera</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 553-567.<br>
<a id="3" href="#3ref">[3]</a> Kasturi, Rangachar, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;btnG=" target="_blank" rel="noopener">Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol</a> IEEE transactions on Pattern Analysis and Machine intelligence 31.2 (2008): 319-336.<br>
<a id="4" href="#4ref">[4]</a> Yamaguchi K, Berg A C, Ortiz L E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=who+are+you+with+and+where+are+you+going&amp;btnG=" target="_blank" rel="noopener">Who are you with and where are you going?</a>[C]//CVPR 2011. IEEE, 2011: 1345-1352.<br>
<a id="5" href="#5ref">[5]</a> Bernardin K, Stiefelhagen R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;btnG=" target="_blank" rel="noopener">Evaluating multiple object tracking performance: the CLEAR MOT metrics</a>[J]. Journal on Image and Video Processing, 2008, 2008: 1.<br>
<a id="6" href="#6ref">[6]</a> Kratz L, Nishino K. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;btnG=" target="_blank" rel="noopener">Tracking with local spatio-temporal motion patterns in extremely crowded scenes</a>[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010: 693-700.<br>
<a id="7" href="#7ref">[7]</a> Ristic B, Vo B N, Clark D, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;btnG=" target="_blank" rel="noopener">A metric for performance evaluation of multi-target tracking algorithms</a>[J]. IEEE Transactions on Signal Processing, 2011, 59(7): 3452-3457.<br>
<a id="8" href="#8ref">[8]</a> Schuhmacher D, Vo B T, Vo B N. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+Consistent+Metric+for+Performance+Evaluation+of+Multi-Object+Filters&amp;btnG=" target="_blank" rel="noopener">A consistent metric for performance evaluation of multi-object filters</a>[J]. IEEE transactions on signal processing, 2008, 56(8): 3447-3457.<br>
<a id="9" href="#9ref">[9]</a> Li Y, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;btnG=" target="_blank" rel="noopener">Learning to associate: Hybridboosted multi-target tracker for crowded scene</a>[C]//2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009: 2953-2960.<br>
<a id="10" href="#10ref">[10]</a> Song B, Jeng T Y, Staudt E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;btnG=" target="_blank" rel="noopener">A stochastic graph evolution framework for robust multi-target tracking</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 605-619.</p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>autonomous driving</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>MOT 评价指标-&quot;Evaluating Multiple Object Tracking Performance, the CLEAR MOT Metrics&quot;</title>
    <url>/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/</url>
    <content><![CDATA[<p>　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。<br>
　　一个理想的 MOT 算法，我们期望每一帧：</p>
<ul>
<li>准确检测目标的数量；</li>
<li>准确估计每个目标的状态，如位置，朝向，速度等；</li>
<li>准确估计每个目标的轨迹，即目标的 ID 不变性；</li>
</ul>
<p>这就要求评价准则：</p>
<ul>
<li>能评估目标定位的精度；</li>
<li>能反映目标轨迹的追踪能力，即同一个目标产生唯一的 ID；</li>
</ul>
<p>此外，为了提高评价准则的实用性：</p>
<ul>
<li>参数尽可能少，阈值可调；</li>
<li>易于理解，表现方式符合人们的直觉；</li>
<li>有较强的通用性，能评估各种跟踪算法；</li>
<li>指标个数少，但是能足够反映算法不同维度的性能；</li>
</ul>
<p>假设第 \(t\) 帧，有目标集 \(\{o_1,...,o_n\}\)，跟踪算法的输出(hypotheses)：\(\{h_1,...h_m\}\)。根据上述设计准则，设计评价计算过程：</p>
<ol type="1">
<li> 构建 \(h_j\) 与 \(o_i\) 的最优匹配；</li>
<li> 对于每个匹配对，计算位置估计误差；</li>
<li> 累加所有匹配对的误差，包括：
<ol type="a">
<li> 计算漏检数(FN)；</li>
<li> 计算误检数(FP)；</li>
<li> 计算 ID swith 次数，包括两个邻近目标的 ID 交换，以及遮挡后，同一目标的 ID 跳变；</li>
</ol></li>
</ol>
<p>由此可得到两大指标：</p>
<ul>
<li>tracking precision，目标位置的估计精度；</li>
<li>tracking accuracy，包括 misses(FN), FP, mismatches(IDs), failures to recover；</li>
</ul>
<p>下面分两块做细节分析，匹配构建 (Establishing Correspondences) 与评价指标 (Metrics)。</p>
<h2 id="匹配构建">1. 匹配构建</h2>
<p>　　算法估计与目标真值的匹配，大致还是基于匹配最近 object-hypothesis 的思想，没匹配上的估计就是 FP，没匹配上的真值就是 FN。但是这中间需要进一步考虑一些问题。</p>
<h3 id="有效匹配">1.1. 有效匹配</h3>
<p>　　如果算法估计 \(h_j\) 与目标 \(o_i\) 的最近距离 \(dist_{i,j}\) 超过了一定的阈值 \(T\)，那么这个匹配也是不合理的，因为这个距离误差加入到定位误差中是不合理的，所以只能说这个跟踪的结果不是这个目标。关于距离的度量：</p>
<ul>
<li>区域（框）跟踪器，距离可用两者的重叠区域来度量，\(T\) 可以设为 0；</li>
<li>点跟踪器，距离可用两者中心点的欧氏距离来度量，\(T\) 可以根据目标的尺寸来设定；</li>
</ul>
<h3 id="跟踪一致性">1.2. 跟踪一致性</h3>
<p>　　统计目标与算法输出的匹配跳变的次数，也就是目标 ID 的跳变数。文章还提到，当目标有两个有效地匹配时，选择之前的匹配，即使那个匹配的距离大于另一个匹配，这点当存在两个很近的目标时，可能会有问题，需要全局来看。</p>
<h3 id="匹配过程">1.3. 匹配过程</h3>
<ol type="1">
<li> 对 \(t\) 帧，考虑 \(M_{t-1}\) 中所有匹配是否还依然有效，包括目标真值及算法输出是否还存在，如果都存在，那么距离是否超出阈值 \(T\)；</li>
<li> 对于剩下的没找到匹配的真值目标，在唯一匹配以及阈值约束下，可采用匹配算法或者贪心算法来求解，使得距离误差的总和最小（文章的意思是排除了从上一帧继承的已有匹配，当目标密集时，这部分也应该加入进来优化）。统计当前帧目标真值匹配的跳变数 \(mme_t\)，作为 mismatch errors；</li>
<li> 经过之前两步后，找到了所有的匹配，统计匹配个数为 \(c_t\)，计算匹配上的目标真值与算法输出的定位误差 \(d_t^i\)；</li>
<li> 统计没有匹配上的算法输出 (hypotheses) 为 \(fp_t\)，没有匹配上的目标真值为 \(m_t\)，目标真值个数为 \(g_t\)；</li>
<li> 每一帧重复步骤１，第一帧没有 mismatch；</li>
</ol>
<h2 id="评价指标">2. 评价指标</h2>
<p>　　基于以上的匹配策略，得出两个合理的指标：</p>
<ul>
<li><strong>MOTP</strong>(multiple object tracking precision)，跟踪定位精度指标：<span class="math display">\[MOTP=\frac{\sum_{i,t}d_t^i}{\sum_tc_t}\]</span></li>
<li><strong>MOTA</strong>(multiple object tracking accuracy)，综合了漏检率，误检率，以及 ID 跳变率：<span class="math display">\[MOTA=1-\frac{\sum_t(m_t+fp_t+mme_t)}{\sum_tg_t}\]</span></li>
</ul>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>MOT 综述-&#39;Multiple Object Tracking: A Literature Review&#39;</title>
    <url>/MOT-%E7%BB%BC%E8%BF%B0-Multiple-Object-Tracking-A-Literature-Review/</url>
    <content><![CDATA[<p>　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。<br>
　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recognition，behavior analysis，车辆的 state estimation。单目标跟踪(SOT)主要关注 appearance model 以及 motion model 的设计，解决尺度、旋转、光照等影响因素。而 MOT 包含两个任务：目标数量以及目标ID，这就要求 MOT 还需要解决其它问题：</p>
<ul>
<li>frequent occlusions</li>
<li>initialization and termination of tracks</li>
<li>similar appearance</li>
<li>interactions among multiple objects</li>
</ul>
<h2 id="问题描述">1. 问题描述</h2>
<p>　　多目标跟踪实际上是多参数估计问题。给定图像序列\(\{I_1,I_2,...,I_t,...\}\)，第\(t\)帧中目标个数为\(M_t\)，第\(t\)帧中所有目标的状态表示为\(S_t=\{s_t^ 1,s_t^ 2,...,s_t^ {M_t}\}\)，第\(i\)个目标的轨迹表示为\(s_{1:t}^ i=\{s_1^ i,s_2^ i,...,s_t^ i\}\)，所有图像中所有目标的状态序列为\(S_{1:t}=\{S_1,S_2,...,S_t\}\)。相应的，所有图像中所有目标观测到的状态序列为\(O_{1:t}=\{O_1,O_2,...,O_t\}\)。多目标跟踪的优化目标是求解最优的各目标状态，即求解一个后验概率问题，<span class="math display">\[ \widehat{S}_{1:t}=\mathop{\arg\max}_{S_{1:t}}P(S_{1:t}|O_{1:t})\]</span> 这种形式有两种实现方法：</p>
<ul>
<li><p><strong>probabilistic inference</strong><br>
适合用于 online tracking 任务，Dynamic Model 为 \(P(S_t|S_{t-1})\)，Observation Model 为 \(P(O_t|S_t)\)，两步求解过程：<br>
　\(\circ\)　Predict: \(P(S_t|O_{1:t-1})=\int P(S_t|S_{t-1})dS_{t-1}\)<br>
　\(\circ\)　Update: \(P(S_t|O_{1:t}) \propto P(O_t|S_t)P(S_t|O_{1:t-1})\)</p></li>
<li><p><strong>deterministic optimization</strong><br>
适合用于 offline tracking 任务，直接利用多帧信息进行最优化求解。</p></li>
</ul>
<h2 id="分类方法">2. 分类方法</h2>
<ul>
<li><p><strong>initialization method</strong><br>
初始化方式分为：<br>
　\(\circ\)　Detection-Based Tracking，优势明显，除了只能处理特定的目标类型；<br>
　\(\circ\)　Detection-Free Tracking，能处理任何目标类型；</p></li>
<li><p><strong>processing mode</strong><br>
根据是否使用未来的观测，处理方式可分为：<br>
　\(\circ\)　online tracking，适合在线任务，缺点是观测量会比较少；<br>
　\(\circ\)　offline tracking，输出结果存在时延，理论上能获得全局最优解；</p></li>
<li><p><strong>type of output</strong><br>
根据问题求解方式输出是否存在随机性：<br>
　\(\circ\)　probabilistic inference，概率性推断；<br>
　\(\circ\)　deterministic inference，求解最大后验概率；</p>
<p><strong>自动驾驶等在线任务主要关注 Detection-Based，online tracking。</strong></p></li>
</ul>
<h2 id="框架">3. 框架</h2>
<p>　　MOT 主要考虑两个问题：</p>
<ul>
<li>目标在不同帧之间的相似性度量，即对appearance, motion, interaction, exclusion, occlusion的建模；</li>
<li>恢复出目标的ID，即 inference 过程；</li>
</ul>
<h3 id="appearance-model">3.1. Appearance Model</h3>
<h4 id="visual-representation">3.1.1. Visual Representation</h4>
<p>　　视觉表达即目标的特征表示方式：</p>
<ol type="1">
<li><strong>local features</strong><br>
本质上是点特征，点特征由 corner+descriptor(角点+描述子) 组成。KLT(good features to track)在 SOT 中应用广泛，用它可以生成短轨迹，估计相机运动位姿，运动聚类等；Optical Flow也是一种局部特征，在数据关联之前也可用于将检测目标连接到短轨迹中去。</li>
<li><strong>region features</strong><br>
在一个块区域内提取特征，根据像素间作差的次数，可分为：
<ul>
<li>zero-order, color histogram &amp; raw pixel template</li>
<li>first-order, HOG &amp; level-set formulation(?)</li>
<li>up-to-second-order, Region covariance matrix</li>
</ul></li>
<li><strong>others</strong><br>
其它特征本质上也需要 local 或 region 的方式提取，只是原始信息并不是灰度或彩图。如 depth,probabilistic occupancy map, gait feature.</li>
</ol>
<p>　　Local features，比如颜色特征，在计算上比较高效，但是对遮挡，旋转比较敏感；Region features 里，HOG 对光照有一定的鲁棒性，但是对遮挡及形变效果较差；Region covariance matrix 更加鲁棒，但是需要更高的计算量；深度特征也比较有效，但是需要额外的获取深度信息的代价。</p>
<h4 id="statistical-measuring">3.1.2. Statistical Measuring</h4>
<p>　　有了目标的特征表示方式之后，就可以评价两个观察的目标的相似性。特征表示的线索(cue)可分为：</p>
<ol type="1">
<li><strong>single cue</strong><br>
因为只有一个线索，相似性(similarity)可以直接通过两个向量的距离转换得到。可以将距离指数化，高斯化。也可以将不相似度转为可能性，用协方差矩阵表示。</li>
<li><strong>multiple cues</strong><br>
多线索，即多种特征的融合，能极大提高鲁棒性，融合的策略有：
<ul>
<li>Boosting, 选取一系列的特征，用 boost 算法选取表达能力最强的特征；</li>
<li>Concatenation, 各个特征直接在空间维度上串起来，形成一个 cue 的表达方式；</li>
<li>Summation, 加权融合各个特征，形成一个 cue 的表达方式；</li>
<li>Product, 各个特征相乘的方式，比如目标 \(s_0\) 的某个潜在匹配 \(s_1\) 的颜色，形状特征为 \(color\), \(shape\) 的概率为 \(p(color|s_0)\), \(p(shape|s_0)\), 假设特征独立，那么， 　　　　　　　<span class="math display">\[p(s_1|s_0)=p(color, shape|s_0)=p(color|s_0)\cdot p(shape|s_0)\]</span></li>
<li>Cascading, coarse-to-fine 的方式，逐步精细化搜索；</li>
</ul></li>
</ol>
<h3 id="motion-model">3.2. Motion Model</h3>
<p>　　运动模型对关联两个 tracklets 比较管用，而 online tracking 任务，对输出的时延要求较高，所以其中一个 tracklet 可以任务就是当前帧与上一帧形成的轨迹，所以这里很难去计算两个 tracklets 的相似度。能看到的一个应用点就是，通过 motion model 模型，预测下一时刻目标的位置，作为一个线索项目。以下讨论的各模型主要是为了度量 tracklets 的相似性，从而做 tracklets 的匹配。</p>
<h4 id="linear">3.2.1. Linear</h4>
<ul>
<li>Velocity Smoothness. N 帧 M 个目标轨迹: \(C_{dyn}=\sum_{t=1}^ {N-2}\sum_{i=1}^ {M}\parallel v_i^ t-v_i^ {t+1}\parallel^ 2\)</li>
<li>Position Smoothness. \(G(p^ {tail}+v^ {tail}\Delta t-p^ {head}, \sum_p)\cdot G(p^ {head}-v^ {head}\Delta t-p^ {tail}, \sum_p)\)</li>
<li>Acceleration Smoothness.</li>
</ul>
<h4 id="non-linear">3.2.2. Non-linear</h4>
<p>　　运动模型假设是非线性的，相似度计算还是按照以上高斯形式。引为中提到，非线性运动模型并不作为目标的惩罚因子，因为目标并不需要满足该模型，但是只要有目标满足，就降低惩罚系数。</p>
<h3 id="interaction-model">3.3. Interaction Model</h3>
<h4 id="social-force-models">3.3.1. Social Force Models</h4>
<ol type="1">
<li><strong>Individual Force</strong>
<ul>
<li>fidelity, 目标不会改变它的目的地方向；</li>
<li>constancy, 目标不会突然改变速度和方向；</li>
</ul></li>
<li><strong>Group Force</strong>
<ul>
<li>attraction, 目标间应该尽量靠近；</li>
<li>repulsion, 目标间也得保留适当的距离；</li>
<li>coherence, 同一个 group 里面的目标速度应该差不多；</li>
</ul></li>
</ol>
<h4 id="crowd-motion-pattern-models">3.3.2. Crowd Motion Pattern Models</h4>
<p>　　当一个 group 比较密集的时候，单个目标的运动模型不太显著了，这时候群体的运动模型更加有效，可以用一些方法来构建群体运动模型。</p>
<h3 id="exclusion-model">3.4. Exclusion Model</h3>
<h4 id="detection-level">3.4.1. Detection-level</h4>
<p>　　同一帧两个检测量不能指向同一个目标。匹配 tracklets 时，可以将这一项作为惩罚项。不过目前的检测技术都做了 NMS，基本可以消除这种情况。</p>
<h4 id="trajectory-level">3.4.2. Trajectory-level</h4>
<p>　　两个轨迹不能非常靠近。对于 online tracking 来说，就是 tracking 结果的两个量不能挨在一起，如果挨在一起，就说明有问题，比如遮挡，或跟丢。</p>
<h3 id="occlusion-handling">3.5. Occlusion Handling</h3>
<ul>
<li>Part-to-whole, 将目标分成栅格来处理；</li>
<li>Hypothesize-and-test,</li>
<li>Buffer-and-recover, 在遮挡产生前，记录一定量的观测，遮挡后恢复；</li>
<li>Others</li>
</ul>
<h3 id="inference">3.6. Inference</h3>
<h4 id="probabilistic-inference">3.6.1. Probabilistic Inference</h4>
<p>　　概率法只需要用到当前时刻之前的信息，所以适合用于 online tracking 任务。首先，如果假设一阶马尔科夫，当前目标的状态之依赖于前一时刻目标的状态，即 <em>dynamic model</em>： <span class="math display">\[P(S_t|S_{1:t-1})=P(S_t|S_{t-1})\]</span> 其次，观测是独立的，即当前目标的观测只由当前目标的状态决定，<em>observation model</em>： <span class="math display">\[P(O_{1:t}|S_{1:t})=\prod_{i=1}^t P(O_t|S_t)\]</span> dynamic model 对应的就是跟踪算法策略，observation model 是状态观测手段，包括检测方法。目标状态估计的迭代过程为：</p>
<ul>
<li><strong>predict step</strong><br>
根据 dynamic model，由目标的上一状态预测当前状态的后验概率分布；</li>
<li><strong>update step</strong><br>
根据 observation model，更新当前目标状态的后验概率分布；</li>
</ul>
<p>　　状态估计的过程伴随着噪音等因素的影响，常用的概率推断模型有：</p>
<ul>
<li>Kalman filter</li>
<li>Extended Kalman filter</li>
<li>Particle filter</li>
</ul>
<h4 id="deterministic-optimization">3.6.2. Deterministic Optimization</h4>
<p>　确定性优化法需要至少一个时间窗口的观测量，所以适合 offline tracking 任务。优化方法有：</p>
<ul>
<li>Bipartite graph matching</li>
<li>Dynamic Programming</li>
<li>Min-cost max-flow network flow</li>
<li>Conditional random field</li>
<li>MWIS(Maximum-weight independent set)</li>
</ul>
<h2 id="评价方法">4. 评价方法</h2>
<p>　　评价方法是非常重要的，一方面对算法系统进行调参优化，另一方面比较各个不同算法的优劣。评价方法 (evaluation) 包括评价指标 (metrics) 以及数据集 (datasets)，多类别的数据集主要有：</p>
<ul>
<li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li>
<li><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">KITTI</a>　　</li>
</ul>
<p>评价指标可分为：</p>
<p>A. <strong>检测指标</strong><br>
　\(\lozenge\)　准确性(Accuracy)</p>
<ul>
<li>Recall &amp; Precision</li>
<li>False Alarme per Frame(FAF) rate, from <a href="https://www.google.com/search?q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;oq=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;aqs=chrome..69i57.1077j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li>
<li>False Positive Per Image(FPPI), from <a href="https://www.google.com/search?q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;oq=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;aqs=chrome..69i57j0.1134j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li>
<li>MODA(Multiple Object Detection Accuracy), 包含了 false positive &amp; miss dets. from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li>
</ul>
<p>　\(\lozenge\)　精确性(Precision)</p>
<ul>
<li>MODP(Multiple Object Detection Precision), 衡量检测框与真值框的位置对齐程度；from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li>
</ul>
<p>B. <strong>跟踪指标</strong><br>
　\(\lozenge\)　准确性(Accuracy)</p>
<ul>
<li>ID switches(IDs), from <a href="https://www.google.com/search?safe=strict&amp;ei=agXyXMaQEKyl_Qa4zJrQCg&amp;q=who+are+you+with+and+where+are+you+going&amp;oq=Who+are+you+with+and+where+are+you+going&amp;gs_l=psy-ab.1.0.0i203.53050.53050..55771...0.0..0.559.559.5-1......0....2j1..gws-wiz.nigYYAJc4jQ" target="_blank" rel="noopener">paper</a></li>
<li>MOTA(Multiple Object Tracking Accuracy), 包含了FP，FN，mismatch；from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li>
</ul>
<p>　\(\lozenge\)　精确性(Precision)</p>
<ul>
<li>MOTP(Multiple Object Tracking Precision), from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li>
<li>TDE(Tracking Distance Error), from <a href="https://www.google.com/search?safe=strict&amp;ei=fATyXNnwEvCH_QaG17fwDA&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;oq=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;gs_l=psy-ab.12..0i30.82181.82181..83291...0.0..0.292.292.2-1......0....2j1..gws-wiz.hs0Je90zzHU" target="_blank" rel="noopener">paper</a></li>
<li>OSPA(optimal subpattern assignment), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;oq=A_Metric_for_Performance_Evaluation_of_Multi-Targe&amp;gs_l=psy-ab.1.0.0i30.106502.106502..109413...0.0..0.303.303.3-1......0....2j1..gws-wiz.vrzc0MG18OM" target="_blank" rel="noopener">paper</a></li>
</ul>
<p>　\(\lozenge\)　完整性(Completeness)</p>
<ul>
<li>MT, the numbers of Mostly Tracked, from <a href="https://www.google.com/search?q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;oq=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;aqs=chrome..69i57.1261j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li>
<li>PT, the numbers of Partly Tracked</li>
<li>ML, the numbers of Mostly Lost</li>
<li>FM, the numbers of Fragmentation</li>
</ul>
<p>　\(\lozenge\)　鲁棒性(Robustness)</p>
<ul>
<li>RS(Recover from Short-term occlusion), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;oq=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;gs_l=psy-ab.12..0i30.453442.453442..454691...0.0..0.315.315.3-1......0....2j1..gws-wiz.OPYJ8mRFgYg" target="_blank" rel="noopener">paper</a></li>
<li>RL(Recover from Long-term occlusion)</li>
</ul>
<p>评价指标汇总： <img src="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/metrics.png" width="50%" height="50%"></p>
<h2 id="总结">5. 总结</h2>
<h3 id="还存在的问题">5.1. 还存在的问题</h3>
<p>　　MOT 算法模块较多，参数也较复杂，但是最依赖于检测模块的性能，所以算法间比较性能时，需要注意按模块进行变量控制。</p>
<h3 id="未来研究方向">5.2. 未来研究方向</h3>
<ul>
<li><strong>MOT with video adaptation</strong>，检测模块式预先训练的，需要在线更新学习；</li>
<li><strong>MOT under multiple camera</strong>: \(\circ\)　multiple views，不同视野相同场景信息的记录， \(\circ\)　non-overlapping multi-camera，不同视野不同场景的 reidentification；</li>
<li><strong>Multiple 3D object tracking</strong>，能更准确预测位置，大小，更有效处理遮挡；</li>
<li><strong>MOT with scene understanding</strong>，拥挤场景，用场景理解来有效跟踪；</li>
<li><strong>MOT with deep learning</strong></li>
<li><strong>MOT with other cv tasks</strong>，和其他任务融合，比如目标分割等；</li>
</ul>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title>3D Detection Paper List</title>
    <url>/3D-Detection-paper-list/</url>
    <content><![CDATA[<p>这篇文章从输入数据类别上进行 3D Detection paper 的归类。</p>
<h2 id="rgb">RGB</h2>
<h2 id="rgb-d双目单目点云">RGB-D(双目，单目+点云)</h2>
<h2 id="lidar">Lidar</h2>
]]></content>
      <categories>
        <category>Trash</category>
      </categories>
      <tags>
        <tag>3D Detection</tag>
        <tag>paper reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Study Topic List</title>
    <url>/study-topic-list/</url>
    <content><![CDATA[<p>　　本文罗列了相关领域知识的学习资料。</p>
<h2 id="detection">1. Detection</h2>
<h3 id="d-detection">1.1. 2D Detection</h3>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34142321" target="_blank" rel="noopener">入门</a></li>
<li><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">amusi</a></li>
<li><span class="citation" data-cites="handong">[Object Detection @handong]</span>(https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#yolov3)</li>
<li><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a></li>
<li><a href="https://paperswithcode.com/task/object-detection" target="_blank" rel="noopener">Paper with Code</a></li>
</ul>
<h3 id="d-detection-1">1.2. 3D Detection</h3>
<ul>
<li><a href="https://paperswithcode.com/task/3d-object-detection" target="_blank" rel="noopener">Paper with Code</a></li>
<li><a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" target="_blank" rel="noopener">KITTI Leaderboard</a></li>
</ul>
<hr>
<h2 id="tracking">2. Tracking</h2>
<h3 id="single-object-tracking">2.1. Single Object Tracking</h3>
<ul>
<li><a href="https://paperswithcode.com/task/visual-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li>
</ul>
<h3 id="multi-object-tracking">2.2. Multi Object Tracking</h3>
<ul>
<li><a href="https://paperswithcode.com/task/multiple-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/65177442" target="_blank" rel="noopener">Paper List</a></li>
<li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li>
<li><a href="https://arxiv.org/abs/1409.7618" target="_blank" rel="noopener">综述：Multiple Object Tracking: A Literature Review</a></li>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Wu_Online_Object_Tracking_2013_CVPR_paper.html" target="_blank" rel="noopener">综述：Online object tracking: A benchmark</a></li>
<li><a href="https://arxiv.org/abs/1504.01942" target="_blank" rel="noopener">综述：MOTChallenge 2015: Towards a benchmark for multi-target tracking</a></li>
</ul>
<hr>
<h2 id="computational-photography">3. Computational Photography</h2>
<ul>
<li><a href="http://graphics.cs.cmu.edu/courses/15-463/2017_fall/" target="_blank" rel="noopener">2017年秋季的计算摄影学课程15-463</a></li>
</ul>
<hr>
<h2 id="cnn-acc">4. CNN ACC</h2>
<hr>
<h2 id="slam">5. SLAM</h2>
<h3 id="理论知识">5.1. 理论知识</h3>
<ul>
<li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95.pdf" target="_blank" rel="noopener">计算机视觉中的数学方法</a></li>
<li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20%28Second%20Edition%29.pdf" target="_blank" rel="noopener">Multiple View Geometry in Computer Vision</a></li>
<li><a href="https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf" target="_blank" rel="noopener">Probabilistic Robotics</a>(有中文版)</li>
<li><a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf" target="_blank" rel="noopener">State Estimation for Robotics</a>(有中文版)</li>
<li><a href="https://github.com/gaoxiang12/slambook" target="_blank" rel="noopener">视觉SLAM十四讲</a></li>
</ul>
<h3 id="综述">5.2. 综述</h3>
<ul>
<li>[Visual Odometry Part I: Fundamentals]</li>
<li>[Visual Odometry Part II: Matching, Robustness, Optimization, Applications]</li>
<li><a href="https://link.springer.com/content/pdf/10.1186%2Fs40064-016-3573-7.pdf" target="_blank" rel="noopener">Review of Visual Odometry: Types, Approaches, Challenges, and Applications</a></li>
<li><a href="https://ipsjcva.springeropen.com/track/pdf/10.1186/s41074-017-0027-2" target="_blank" rel="noopener">Visual SLAM algorithms: a Survey from 2010 to 2016</a></li>
<li><a href="http://www.cvc.uab.es/~asappa/publications/C__IEEE_IV_2012_W3.pdf" target="_blank" rel="noopener">Visual SLAM for Driverless Cars: a Brief Survey</a></li>
<li><a href="https://link.springer.com/article/10.1007/s10462-012-9365-8" target="_blank" rel="noopener">Visual Simultaneous Locations and Mapping: a Survey</a></li>
</ul>
<h3 id="工具">5.3. 工具</h3>
<ul>
<li><a href="http://www.guyuehome.com/column/ros-explore" target="_blank" rel="noopener">ROS</a></li>
<li><a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html" target="_blank" rel="noopener">Opencv Camera Calibration</a></li>
<li><a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="noopener">Matlab Camera Calibration Toolbox</a></li>
<li><a href="http://wiki.ros.org/camera_calibration" target="_blank" rel="noopener">ROS Wiki Camera Calibration</a></li>
</ul>
<h3 id="算法">5.4. 算法</h3>
<ul>
<li><a href="https://openslam-org.github.io/" target="_blank" rel="noopener">OpenSLAM</a></li>
</ul>
<h3 id="其它资料">5.5. 其它资料</h3>
<ul>
<li><a href="https://www.zhihu.com/people/cheng-xu-yuan-10/posts" target="_blank" rel="noopener">计算机视觉life</a></li>
<li><a href="https://paperswithcode.com/task/visual-odometry" target="_blank" rel="noopener">Paper with Code</a></li>
</ul>
]]></content>
      <categories>
        <category>Trash</category>
      </categories>
  </entry>
</search>
